{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Farming and Countryside Programme (FCP) Development Guide","text":"<p>This is a repository of standards and supporting guidance for all software developers working within the Farming and Countryside Programme (FCP).</p> <p>The purpose of the standards are to ensure that delivery supports the Architecture Vision set out by the programme and better enable developer agility and mobility through consistent patterns and practices.</p> <p>The guide also provides help and support for common setup and problem solving.</p>"},{"location":"#architecture-vision","title":"Architecture Vision","text":"<p>The FCP Architecture vision is to deliver a modern, cloud native, event driven microservice ecosystem.</p> <p>Containerised Node.js with JavaScript microservices are the primary delivery mechanism for the programme.  These are deployed to a Defra cloud environment and are highly available, scalable and resilient.</p> <p>For scenarios where Node.js is not appropriate, Defra's secondary framework and language is .NET with C#.</p>"},{"location":"#defra-standards","title":"Defra standards","text":"<p>The standards and guidance contained here is intended to be an FCP context specific layer over Defra's wider development standards.</p> <p>This guide assumes that Developers already understand and follow these core standards.</p>"},{"location":"#gds-standards","title":"GDS standards","text":"<p>The Government Digital Service (GDS) also have a set of standards that all government services must adhere to.  These are also assumed to be understood and followed by developers working within FCP.</p> <ul> <li>Service Manual</li> <li>Technology Code of Practice</li> </ul>"},{"location":"create-a-new-service/ado/","title":"Setup an Azure DevOps CD pipeline","text":"<p>The following steps are only applicable for teams using the FCP Platform.  CD capability is automatically scaffolded for teams using CDP.</p> <p>Common Azure DevOps (ADO) pipeline definitions have been created and can be reused to deploy services to the various environments.</p> <p>These pipelines are responsible for not only deploying applications, but for provisioning and configuring the Azure environments the service run in.</p>"},{"location":"create-a-new-service/ado/#pipeline-categories","title":"Pipeline categories","text":"<p>Pipelines are split into different capabilities which must be run sequentially into an environment when changes are made.</p>"},{"location":"create-a-new-service/ado/#platform-core","title":"<code>platform-core</code>","text":"<p>Deploy the core platform services such as PaaS services and networking.  This pipeline rarely needs to be run and should only be done via the Platform team.</p>"},{"location":"create-a-new-service/ado/#platform-aks-configuration","title":"<code>platform-aks-configuration</code>","text":"<p>Deploy configuration for the AKS.  This pipeline should be run when a new namespace is added to support a new bounded context.</p>"},{"location":"create-a-new-service/ado/#platform-services","title":"<code>platform-services</code>","text":"<p>Deploy updates to the environment for a specific resource types including:</p> <ul> <li>Managed Identities</li> <li>Azure Service Bus</li> <li>Azure Storage</li> </ul> <p>To select the service definition, when running the pipeline select the <code>Service</code> and <code>Resource Type</code>.</p> <p>The pipeline is also capable deploying all databases and helm charts for a bounded context, but should not be used for that purpose.  Instead the Helm and database pipelines should be used as they allow deploying individual services by semantic version.</p>"},{"location":"create-a-new-service/ado/#platform-services-database","title":"<code>platform-services-database</code>","text":"<p>Deploy a database migration for a specific application version.  This pipeline should be run when a new database migration is required selecting <code>service</code>, <code>databaseRepo</code> and <code>version</code>.</p>"},{"location":"create-a-new-service/ado/#platform-services-helm","title":"<code>platform-services-helm</code>","text":"<p>Deploy a helm chart for a specific application version.  This pipeline should be run whenever an update to a service running in AKS is required selecting <code>service</code>, <code>helmChart</code> and <code>chartVersion</code>.</p>"},{"location":"create-a-new-service/ado/#creating-a-new-service-definition","title":"Creating a new service definition","text":"<p>The above pipelines are dependent on teams maintaining an ADO Git repository with their service definitions and configuration.</p> <p>This repository will need to be cloned locally and updated through a pull request.</p>"},{"location":"create-a-new-service/ado/#create-bounded-context-folder","title":"Create bounded context folder","text":"<p>Similar to Jenkins, each service should group it's service definitions in a folder matching the name of the bounded context under the <code>services</code> folder. </p> <p>For example, <code>ffc-demo</code>.</p>"},{"location":"create-a-new-service/ado/#create-definitions","title":"Create definitions","text":"<p>Definitions should be created sub-categorised by the type of resource they are deploying.</p> <p>Guidance for this activity can be found in the Platform services documentation.</p>"},{"location":"create-a-new-service/choose-a-technology/","title":"Choose a technology","text":""},{"location":"create-a-new-service/choose-a-technology/#development-language-and-framework","title":"Development language and Framework","text":"<p>The primary technology choice for new FCP services is Node.js with JavaScript deployed to CDP.</p> <p>Delivery capability in the programme has been optimised for this technology stack.</p> <p>For scenarios where Node.js is not appropriate, Defra's secondary framework and language is .NET and C#.</p> <p>To deviate from Node.js and JavaScript, you must have an evidenced exception approved at SDA.</p> <p>For full details of Defra's development framework standards, see the Defra Software Development Standards.</p>"},{"location":"create-a-new-service/choose-a-technology/#nodejs","title":"Node.js","text":"<p>For web based services, Hapi.js as the web framework.  Other web frameworks such as Express are not permitted.</p> <p>Code must be written in JavaScript linted with neostandard.</p>"},{"location":"create-a-new-service/choose-a-technology/#net","title":".NET","text":"<p>For scenarios where Node.js is not appropriate, .NET can be used with approval of SDA.</p> <p>CDP and the FCP Platform have been developed to support .NET, however there are not as many supporting tools and libraries given the lower adoption of the technology.</p>"},{"location":"create-a-new-service/choose-a-technology/#compute","title":"Compute","text":"<p>CDP provides an abstraction over AWS Fargate, this is the primary compute technology for FCP services.</p> <p>For services still hosted on FCP Platform, Azure Kubernetes Service (AKS) is the preferred compute technology.</p> <p>Kubernetes deployments must be managed using Helm.</p>"},{"location":"create-a-new-service/choose-a-technology/#databases","title":"Databases","text":"<p>MongoDB is the primary database technology supported by CDP.</p> <p>CDP also supports PostgreSQL for specific uses cases, however teams should use MongoDB in the first instance. </p> <p>Azure Database for PostgreSQL is the only database that is fully supported by the FCP Platform.</p>"},{"location":"create-a-new-service/choose-a-technology/#code-first-migrations","title":"Code first migrations","text":"<p>When working with PostgreSQL, Liquibase is the approved tool for managing database schema changes.  Both CDP and FCP Platform support Liquibase.</p> <p>Whilst technologies such as Sequelize and Entity Framework can be used, they should not be responsible for managing database schema changes.</p>"},{"location":"create-a-new-service/choose-a-technology/#messaging","title":"Messaging","text":"<p>SNS and SQS are the only supported messaging technologies when deploying to CDP.</p> <p>Azure Service Bus is the only supported messaging technology when deploying to the FCP Platform.</p>"},{"location":"create-a-new-service/choose-a-technology/#caching","title":"Caching","text":"<p>Redis is the approved caching technology.</p>"},{"location":"create-a-new-service/choose-a-technology/#storage","title":"Storage","text":"<p>S3 is the approved storage technology when deploying to CDP.</p> <p>Azure Blob Storage, Azure File Storage and Azure Table Storage are the supported storage technologies when deploying to the FCP Platform.</p> <p>Blob Storage should be favoured over File Storage where possible due to it's superior local development experience.</p>"},{"location":"create-a-new-service/choose-a-technology/#containers","title":"Containers","text":"<p>Docker is the approved container technology.</p>"},{"location":"create-a-new-service/conventions/","title":"Repository conventions","text":"<p>For teams using the FCP Platform, repositories should be setup with the following conventions to ensure consistency and to ensure the CI pipeline functions correctly.</p>"},{"location":"create-a-new-service/conventions/#structure","title":"Structure","text":"<p>The following structure should be adhered to for all repositories.  Note that not all contents will be applicable depending on the nature of the service.</p> <ul> <li><code>&lt;root&gt;</code></li> <li><code>changelog</code><ul> <li>Liquibase changesets</li> </ul> </li> <li><code>docs</code><ul> <li>Documentation such as AsyncAPI and OpenAPI specifications</li> </ul> </li> <li><code>helm/&lt;repository name&gt;</code><ul> <li>Helm chart for the service</li> </ul> </li> <li><code>scripts</code><ul> <li>Scripts to support local development and testing</li> </ul> </li> <li><code>.dockerignore</code></li> <li><code>.gitignore</code></li> <li><code>.snyk</code></li> <li><code>Dockerfile</code></li> <li><code>Jenkinsfile</code></li> <li><code>LICENCE</code></li> <li><code>README</code></li> <li><code>docker-compose.debug.yaml</code></li> <li><code>docker-compose.link.yaml</code></li> <li><code>docker-compose.migrate.yaml</code></li> <li><code>docker-compose.override.yaml</code></li> <li><code>docker-compose.test.debug.yaml</code></li> <li><code>docker-compose.test.watch.yaml</code></li> <li><code>docker-compose.test.yaml</code></li> <li><code>docker-compose.yaml</code></li> <li><code>provision.azure.yaml</code></li> <li><code>sonar-project.properties</code></li> </ul>"},{"location":"create-a-new-service/conventions/#nodejs","title":"Node.js","text":"<p>In addition to the above, Node.js services should include the following content.</p> <ul> <li><code>&lt;root&gt;</code></li> <li><code>app</code><ul> <li>All application code</li> </ul> </li> <li><code>test</code><ul> <li><code>unit</code></li> <li>Unit tests</li> <li><code>integration</code></li> <li><code>narrow</code><ul> <li>Narrow integration tests</li> </ul> </li> <li><code>local</code><ul> <li>Local integration tests</li> </ul> </li> <li><code>contract</code></li> <li>Contract tests</li> <li><code>acceptance</code></li> <li>UI and compatibility tests (for frontend applications)</li> </ul> </li> </ul>"},{"location":"create-a-new-service/conventions/#net","title":".NET","text":"<p>In addition to the above, .NET services should include the following content.</p> <ul> <li><code>&lt;root&gt;</code></li> <li><code>&lt;Project Name&gt;</code><ul> <li>All application code</li> </ul> </li> <li><code>&lt;Project Name&gt;.Tests</code><ul> <li>Application tests</li> </ul> </li> </ul>"},{"location":"create-a-new-service/deploy/","title":"Deploy a service","text":""},{"location":"create-a-new-service/deploy/#environment-control","title":"Environment control","text":""},{"location":"create-a-new-service/deploy/#non-production","title":"Non production","text":"<p>All team members are free to deploy to non-production environments at any time without the need for approvals external to the team.</p>"},{"location":"create-a-new-service/deploy/#production","title":"Production","text":"<p>Teams are encouraged to deploy to production as early and as often as possible to reduce risk and gain regular feedback from users.</p> <p>However, there are additional controls around production deployments.</p> <p>Team members are responsible for deploying to production as long as the following criteria are met.</p> <ul> <li>An approved change request raised in ServiceNow</li> <li>An agreed release time with Release Management Team</li> <li>A CCoE resource available to support the release (if required)</li> <li>A run book provided to Release Management Team in advance</li> </ul>"},{"location":"create-a-new-service/deploy/#change-requests","title":"Change requests","text":"<p>Change requests are raised in ServiceNow and take eight working days for approval.  Therefore, it is important to raise change requests as soon as possible.</p> <p>Once a service has had three successive successful releases to production, a standard change proposal can be submitted.  If approved, teams can deploy to production within the following rules:</p> <ul> <li>standard change raised in ServiceNow at implement stage specifying microservice and version to be released</li> <li>notify Release Management before release</li> <li>confirm to Release Management if CCoE resource is required</li> <li>provide run book to Release Management</li> <li>releases Monday to Friday within working hours only</li> </ul> <p>Teams are encouraged to achieve standard change status as soon as possible to enable more frequent releases to production.</p>"},{"location":"create-a-new-service/deploy/#workflow","title":"Workflow","text":"<p>Teams across the FCP typically follow the below process.</p> <p>Individual delivery teams are free to add their own processes on top of these core values to best fit their context.</p>"},{"location":"create-a-new-service/deploy/#route-to-live","title":"Route to live","text":"<p>This applies equally to a new feature or bug fix.</p> <ol> <li>developer creates a feature branch from main branch</li> <li>developer creates an empty commit with the title of the change and a link to Jira ticket</li> <li>developer opens draft PR for change</li> <li>developer write code in line with team standards, following TDD where appropriate</li> <li>as developer commits, CI builds and tests code</li> <li>CI runs tests to assure code quality</li> <li>when ready, code is reviewed using PR deployment and CI outputs to support</li> <li>when review passes code is merged to main branch</li> <li>CI packages, versions and prepares for deployment to higher environments</li> <li>deployment to non-production environments</li> <li>testing in the non-production environments</li> <li>developer raises change request within ServiceNow to deploy to Production environment</li> <li>Release Management notified of deployment time and resource need</li> <li>developer deploys to Production environment</li> <li>change request is closed once deployment is verified</li> </ol> <p></p>"},{"location":"create-a-new-service/github/","title":"Create a source code repository","text":"<p>As per Defra and GDS standards, source code should be open source and hosted on GitHub.  Other tools such as Azure DevOps, Bitbucket and GitLab are not permitted for application development.</p> <p>Scenarios where open source is not appropriate are rare and must be approved by a Principal Developer.</p> <p>All source code is hosted in the DEFRA GitHub Organisation</p> <p>Repositories should contain a single microservice/application.  Monorepos are not supported by either CDP or the FCP Platform.</p>"},{"location":"create-a-new-service/github/#naming-conventions","title":"Naming conventions","text":"<p>Repositories should be named in a consistent manner to aid discoverability and to ensure that they are easily identifiable as part of FCP.</p> <p>Below is the agreed naming convention for repositories.</p> <p><code>ffc-&lt;bounded context&gt;-&lt;service&gt;</code> or <code>fcp-&lt;bounded context&gt;-&lt;service&gt;</code></p> <p>Where bounded context is an identifier to a set of related microservices and services is an identifier for the specific service.</p> <p>For example, <code>ffc-pay-enrichment</code> would belong to FCP, be part of the Payment ecosystem and perform an enrichment function.</p> <p>Following this naming convention helps understand ownership of repositories, avoid collisions and is essential for some CI/CD pipelines to function correctly.</p> <p>The Farming and Countryside programme (FCP) was previously known as the Future Farming and Countryside programme (FFC), hence the original <code>ffc</code> convention.</p>"},{"location":"create-a-new-service/github/#creating-a-new-repository","title":"Creating a new repository","text":"<p>Teams using CDP should follow the CDP documentation to create a new repository.</p> <p>For teams using the FCP Platform, a new repository can be created directly within the DEFRA GitHub organisation.</p> <p>When creating a new repository intended for a Node.js application, the repository can be based on the ffc-template-node template repository.  </p> <p>The template includes everything needed to get a minimal Node.js application up and running with CI/CD pipelines and follows all FCP structure and naming conventions.  Once created follow the instructions in the <code>README.md</code> file to rename the assets contained within.</p> <p>For .NET there is no template repository, however, this demo repository can be used for guidance.</p> <p>Due to the GitHub permission model, only certain users can create new repositories.  If you are unable to create a new repository, please ask in Slack channel <code>#github-support</code>.</p>"},{"location":"create-a-new-service/github/#repository-setup","title":"Repository setup","text":""},{"location":"create-a-new-service/github/#update-access","title":"Update access","text":"<p>All repositories using the FCP Platform need the <code>ffcplatform</code> user account to be added with <code>Write</code> access to allow the FCP Platform CI pipeline to function correctly.</p>"},{"location":"create-a-new-service/github/#branch-policies","title":"Branch policies","text":"<p>Teams can extend branch policies to fit their needs, however, the following are the minimum requirements for the <code>main</code> branch on all repositories.</p> <ul> <li>protected by a branch policy</li> <li>requires at least one pull request review before merging</li> <li>stale reviews are automatically dismissed</li> <li>requires all build steps to complete (requires Jenkins setup before this can be enabled)</li> <li>must require signed commits</li> </ul> <p>For prototype and spike repositories, the branch policy can be relaxed to allow direct commits to the <code>main</code> branch.</p>"},{"location":"create-a-new-service/github/#licence","title":"Licence","text":"<p>The Open Government Licence (OGL) Version 3 should be added to the repository.  Example content is below.</p> <pre><code>The Open Government Licence (OGL) Version 3\n\nCopyright (c) 2026 Defra\n\nThis source code is licensed under the Open Government Licence v3.0. To view this\nlicence, visit www.nationalarchives.gov.uk/doc/open-government-licence/version/3\nor write to the Information Policy Team, The National Archives, Kew, Richmond,\nSurrey, TW9 4DU.\n</code></pre>"},{"location":"create-a-new-service/github/#jenkinsfile","title":"<code>Jenkinsfile</code>","text":"<p>In order to use the FCP Platform CI pipeline, a <code>Jenkinsfile</code> should be added to the repository.  This file references the version of the shared pipeline to use and sets the parameters for the build.</p> <p>Details on the content of the <code>Jenkinsfile</code> can be found in the FCP CI pipeline documentation</p>"},{"location":"create-a-new-service/github/#webhooks","title":"Webhooks","text":"<p>In order to support the FCP Platform CI/CD pipeline, the following webhooks should be added to the repository.</p> <ol> <li>navigate to <code>Settings -&gt; Webhooks -&gt; Add webhook</code></li> <li>set <code>Payload URL</code> to be <code>https://jenkins-ffc-api.azure.defra.cloud/github-webhook/</code></li> <li>set <code>Content type</code> to be <code>application/json</code></li> <li>set <code>Secret</code> to be the webhook secret value.  This can be retrieved from Azure Key Vault <code>github-webhook-secret</code> value in the FCP Shared Services Azure subscription.</li> <li>set the <code>Pull requests</code> and <code>Pushes</code> events to trigger the webhook</li> <li>select <code>Add webhook</code></li> </ol>"},{"location":"create-a-new-service/github/#sonarqube-cloud","title":"SonarQube Cloud","text":""},{"location":"create-a-new-service/github/#configure-sonarqube-cloud","title":"Configure SonarQube Cloud","text":"<p>This step should be performed before running a build or you may end up with duplicate projects.</p> <ol> <li>navigate to the Defra organisation within SonarQube Cloud</li> <li>select <code>Analyze new project</code></li> <li>select repository to analyse</li> <li>select <code>Set Up</code></li> <li>(FCP Platform only) select <code>Administration -&gt; Update key</code> and ensure key matches the name of the repository, removing the <code>DEFRA_</code> prefix, for example, <code>ffc-pay-enrichment</code></li> <li>select <code>Administration -&gt; Analyse Method</code> and disable Automatic Analysis</li> </ol> <p>The default Defra Quality gates and language rules should not be changed.  Some existing projects may be using the default \"Sonar Way\" gates.  The expectation is that teams should be working towards changing these to the Defra Quality gates.</p>"},{"location":"create-a-new-service/github/#snyk","title":"Snyk","text":"<p>A Snyk scan is run as part of the CI pipeline. By default, a project will be set to private even if it is open source.</p> <ol> <li>Navigate to Snyk</li> <li>select <code>Add project</code></li> <li>select the repository to scan</li> </ol> <p>Note: Due to Defra's SSO setup, new projects cannot be added to the Snyk organisation.</p>"},{"location":"create-a-new-service/jenkins/","title":"Setup a Jenkins CI pipeline","text":"<p>The following steps are only applicable for teams using the FCP Platform.  CI capability is automatically scaffolded for teams using CDP.</p> <p>Assuming the repository has been configured correctly, including setup of SonarCloud and Snyk, the following steps should be followed to setup a Jenkins CI pipeline.</p>"},{"location":"create-a-new-service/jenkins/#create-bounded-context-folder","title":"Create bounded context folder","text":"<p>Each service should group it's CI pipelines in a Jenkins folder matching the name of the bounded context.  This is to ensure that the Jenkins UI remains manageable as the number of services grows.</p> <p>For example, a repository named <code>ffc-pay-enrichment</code> would be placed in a folder named <code>ffc-pay</code>.</p> <p>If a bounded context folder does not exist, create one by:</p> <ol> <li>navigating to the Jenkins home page</li> <li>select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>ffc-&lt;bounded context&gt;</code>, for example <code>ffc-pay</code></li> <li>select <code>Folder</code></li> <li>select <code>Ok</code></li> </ol>"},{"location":"create-a-new-service/jenkins/#create-a-build-pipeline","title":"Create a build pipeline","text":"<ol> <li>navigate to your bounded context folder in Jenkins</li> <li>select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>&lt;repository name&gt;-build</code>, for example <code>ffc-demo-web-build</code></li> <li>select <code>Multibranch Pipeline</code></li> <li>select <code>Ok</code></li> <li>enter <code>GitHub</code> as a Branch Source</li> <li>for credentials select the <code>github-token</code> with the <code>ffcplatform</code> user</li> <li>enter your GitHub URL in the <code>HTTPS URL</code> field, for example <code>https://github.com/DEFRA/ffc-demo-web.git</code></li> <li>set <code>Discover branches</code> to <code>All branches</code></li> <li>delete <code>Discover pull requests from origin</code> and <code>Discover pull requests from forks</code></li> <li>set <code>Scan Multibranch Pipeline Triggers -&gt; Periodically if not otherwise run</code> to <code>true</code> with an interval of <code>1 hour</code></li> <li>set <code>Pipeline Action Triggers -&gt; Pipeline Delete Event</code> set <code>ffc-housekeeping/cleanup-on-branch-delete</code></li> <li>set <code>Pipeline Action Triggers -&gt; Include Filter</code> to be <code>*</code></li> <li>set <code>Pipeline Action Triggers -&gt; Additional Parameter -&gt; Parameter Name</code> to be <code>repoName</code> and <code>Parameter Value</code> to be the name of the repository, for example, <code>ffc-demo-web</code></li> </ol> <p>Definitions can be copied from existing pipelines.  To save time, when creating a new pipeline select <code>Copy from</code> and enter the name of an existing pipeline.  Remember to update the <code>repoName</code> and <code>GitHub URL</code> fields.</p>"},{"location":"create-a-new-service/jenkins/#create-a-deployment-pipeline","title":"Create a deployment pipeline","text":"<p>Although Azure DevOps is used for deployments, Jenkins is used to trigger the deployment pipeline following the successful run of a main branch build.  </p> <p>The deployment to the Sandpit environment is also orchestrated by Jenkins.</p> <ol> <li>navigate to your bounded context folder in Jenkins</li> <li>select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>&lt;repository name&gt;-deploy</code>, for example <code>ffc-demo-web-deploy</code></li> <li>select <code>Pipeline</code></li> <li>select <code>Ok</code></li> <li>select <code>This project is parameterized</code></li> <li>add the following parameters:</li> <li><code>environment</code> with a default value of <code>snd</code></li> <li><code>namespace</code> with a default value of <code>&lt;bounded context&gt;</code></li> <li><code>chartName</code> with a default value of <code>&lt;repository name&gt;</code></li> <li><code>chartVersion</code> with a default value of <code>1.0.0</code></li> <li><code>helmChartRepoType</code> with a default value of <code>acr</code></li> <li>select <code>Trigger builds remotely (e.g., from scripts)</code></li> <li>enter <code>Authentication token</code> which can be found in any existing deployment pipeline</li> <li>enter the below script in the <code>Pipeline</code> section with a <code>Definition</code> of <code>Pipeline script</code></li> </ol> <pre><code>@Library('defra-library@v-9') _\n\ndeployToCluster environment: params.environment, \n                namespace: params.namespace, \n                chartName: params.chartName, \n                chartVersion: params.chartVersion,\n                helmChartRepoType: params.helmChartRepoType\n</code></pre>"},{"location":"create-a-new-service/jenkins/#test-the-pipeline","title":"Test the pipeline","text":"<p>Commits to both feature branches and main branches should trigger the build pipeline.  Successful builds of the main branch should trigger the deployment pipeline.</p> <p>Following deployment the new service should be available in the Sandpit AKS cluster in a namespace matching the bounded context.  For example, <code>ffc-demo-web</code> would be deployed to the <code>ffc-demo</code> namespace.</p> <p>Builds can also be triggered manually by selecting <code>Build Now</code> from the pipeline page.</p>"},{"location":"development-patterns/analytics/","title":"Analytics","text":"<p>Google Tag Manager (GTM) is a tag management system that allows you to quickly and easily update tags and code snippets on your website or mobile app, such as those intended for traffic analysis and marketing optimisation. Once the GTM code is added to your site, you can configure tags via the GTM web interface without having to alter your website code.</p>"},{"location":"development-patterns/analytics/#setup-google-tag-manager","title":"Setup Google Tag Manager","text":"<p>A GTM container needed to be setup for a new service for both Non-Production and Production. </p> <p>Note that at time of writing ownership of GTM within Defra is still to be determined. Please contact a Principal Developer for support.</p> <p>Those requiring access will need create a google account using their Defra email address. Accounts can be created without using Gmail here.</p> <p>Once a container has been created, a GTM codes will be provided.  They may give the full JavaScript code snippets similar to the following:</p> <pre><code>&lt;!-- Google Tag Manager --&gt;\n&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\nnew Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\nj=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=\n'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n  })(window,document,'script','dataLayer','GTM-M5YK7JL');&lt;/script&gt;\n&lt;!-- End Google Tag Manager --&gt;\n\n&lt;!-- Google Tag Manager (noscript) --&gt;\n&lt;noscript&gt;&lt;iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-M5YK7JL\" \nheight=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"&gt;&lt;/iframe&gt;&lt;/noscript&gt;\n&lt;!-- End Google Tag Manager (noscript) --&gt;\n</code></pre> <p>Once the GTM has been provided, the GTM guidance explains how to add the GTM code to your site.</p>"},{"location":"development-patterns/cache/","title":"Caching","text":""},{"location":"development-patterns/cache/#redis","title":"Redis","text":"<p>When an external cache is needed, Azure Cache for Redis is the recommended choice.  For local development, a Docker image can be used.</p>"},{"location":"development-patterns/cache/#hapijs","title":"Hapi.js","text":"<p>Typically a cache is needed to work with a web server such as Hapi.js.  Hapi caching is described in their official documentation. The following guide specifically describes how server-side Redis caching can be added to a Hapi based microservice.</p>"},{"location":"development-patterns/cache/#server-side-caching-in-hapi","title":"Server-side caching in Hapi","text":"<p>Hapi server-side caching uses the catbox interface to abstract away the underlying caching technology being used (e.g. memory, Redis, Memcached).</p> <p>There are three main concepts to Hapi server-side caching: * The cache strategy (or provider): is the underlying caching technology being employed. Here catbox-redis, the Redis adapter for catbox, is the strategy. * The cache client: is the low-level cache abstraction, and is initialised using a cache stragegy (e.g. memory or Redis). Hapi initialises an in-memory cache client by default, and you can create additional cache clients using the same or different strategies (e.g. you can have one in-memory cache, and one Redis cache). * The cache policy: is a higher-level cache abstraction that sets a policy on the storage within the cache (e.g. expiry times). The cache policy also provides additional segmentation within the cache client. Typically the cache policy is how you would interact with cache values via the <code>get</code> and <code>set</code> menthods.</p>"},{"location":"development-patterns/cache/#configuring-the-default-cache-client","title":"Configuring the default cache client","text":"<p>As mentioned above, Hapi initialises an in-memory cache client by default. If you wish you can make the default cache client use a different strategy. For example, the following would make the default cache strategy use Redis or memory depending on the value of <code>config.useRedis</code>:</p> <pre><code>const catbox = config.useRedis ? require('@hapi/catbox-redis') : require('@hapi/catbox-memory')\nconst catboxOptions = config.useRedis\n  ? {\n      host: process.env.REDIS_HOSTNAME,\n      port: process.env.REDIS_PORT,\n      password: process.env.REDIS_PASSWORD,\n      partition: process.env.REDIS_PARTITION,\n      tls: process.env.NODE_ENV === 'production' ? {} : undefined\n    }\n  : {}\n\nconst server = hapi.server({\n  port: config.port,\n  cache: [{\n    provider: {\n      constructor: catbox,\n      options: catboxOptions\n    }\n  }]\n}\n</code></pre>"},{"location":"development-patterns/cache/#configuring-new-cache-clients","title":"Configuring new cache clients","text":"<p>Additional cache clients can be created when initialising the Hapi server by adding new definitions to the <code>cache</code> array. Additional caches are required to be given a <code>name</code>. For example, the following will create a new Redis cache client called session.</p> <pre><code>const catbox = require('@hapi/catbox-redis')\nconst catboxOptions = {\n  host: process.env.REDIS_HOSTNAME,\n  port: process.env.REDIS_PORT,\n  password: process.env.REDIS_PASSWORD,\n  partition: process.env.REDIS_PARTITION,\n  tls: process.env.NODE_ENV === 'production' ? {} : undefined\n}\n\nconst server = hapi.server({\n  port: config.port,\n  cache: [{\n    name: 'session',\n    provider: {\n      constructor: catbox,\n      options: catboxOptions\n    }\n  }]\n}\n</code></pre> <p>NOTE 1: This example will create two cache clients, the default in-memory cache client and a new cache client called <code>session</code> that uses Redis</p> <p>NOTE 2: Hapi will always use the default in-memory cache client unless you specify the <code>name</code> when using it (either directly or via the cache policy, see below)</p>"},{"location":"development-patterns/cache/#creating-and-using-a-cache-policy","title":"Creating and using a cache policy","text":"<p>Lastly we create the cache policy, which is typically how we interact with the cache (see the catbox policy documentation for more details and how set and get data in the cache).</p> <p>When creating a cache policy, if you don't explicitly provide the name of a cache client (via the <code>cache</code> property), it will use the default cache client.</p> <p>To create a cache policy using a segment within the default cache client:</p> <pre><code>myCache = server.cache({\n  expiresIn: 36000,\n  segment: 'mySegment'\n  // ... any other configuration\n})\n</code></pre> <p>To create a cache policy using a segment within a named cache client (in this case <code>session</code>):</p> <pre><code>myCache = server.cache({\n  cache: 'session'\n  expiresIn: 36000,\n  segment: 'mySegment'\n  // ... any other configuration\n})\n</code></pre>"},{"location":"development-patterns/cache/#integration-with-yar-session-cookies","title":"Integration with yar session cookies","text":"<p>Hapi yar is a plugin that adds unauthenticated session support (state across multiple browser request) to Hapi. By default it tries to fit session data into a session cookie, but will use server-side storage via the Hapi cache interface if the session data is greater than the max size specified when registering the plugin.</p> <p>Combining Hapi yar with Redis caching is one way to allow multiple replicates of a web server microservice to share server-side user session data.</p> <p>Example configuration using the default cache client:</p> <pre><code>server.register({\n  plugin: require('@hapi/yar'),\n  options: {\n    cache: {\n      expiresIn: 36000\n    },\n    maxCookieSize: 0 // this will force server-side caching\n    // ... other config here\n  }\n})\n</code></pre> <p>Example configuration using a named cache client:</p> <pre><code>server.register({\n  plugin: require('@hapi/yar'),\n  options: {\n    cache: {\n      cache: 'session'\n      expiresIn: 36000\n    },\n    maxCookieSize: 0 // this will force server-side caching\n    // ... other config here\n  }\n})\n</code></pre>"},{"location":"development-patterns/code-review/","title":"Code review","text":"<p>When code reviews are conducted to a high standard, they provide a valuable learning opportunity for the author, reviewer and any observers of the review process. FCP projects implement strict controls such that all changes must be made in feature branches and merged via pull request (PR). In order to merge a PR, it must first be approved by at least one reviewer.</p> <p>When reviewing a pull request on any FCP project, the following guidance should be followed.</p>"},{"location":"development-patterns/code-review/#tone-of-code-review-comments","title":"Tone of code review comments","text":"<p>The tone of communications is extremely important in fostering an inclusive, collaborative atmosphere within teams.</p> <p>Remember that your colleagues put a lot of effort into their work and may feel offended by harsh criticism, particularly if you make assumptions or imply a lack of thought. Approach code reviews with kindness and empathy, assuming the best intentions and applauding good solutions as well as making suggestions for improvement.</p> <p>Comments should be used to give praise for good solutions and to point out potential improvements. They should be not be used to criticise your colleagues or make strongly opinionated statements. Always be mindful of your tone, considering how others might perceive your comments, and be explicit about when a comment is non-blocking or unimportant.</p>"},{"location":"development-patterns/code-review/#be-constructive","title":"Be constructive","text":"<p>Don't use harsh language or criticise without making constructive suggestions. Do suggest alternative approaches and explain your reasoning.</p>"},{"location":"development-patterns/code-review/#be-specific","title":"Be specific","text":"<p>Don't make vague statements about the changes as a whole. Do point out specific issues and offer specific ideas for how the changes can be improved.</p>"},{"location":"development-patterns/code-review/#avoid-strong-opinions","title":"Avoid strong opinions","text":"<p>Don't make strong, opinionated statements or dictate specific changes. Do ask open-ended questions and keep an open mind about alternative solutions and reasoning that you may not have thought of.</p>"},{"location":"development-patterns/code-review/#scope-of-a-code-review","title":"Scope of a code review","text":"<p>Code reviews should focus on what is being changed and whether the change is appropriate.  The scope will be stated in the acceptance criteria of the ticket.</p> <p>Expanding the scope of a pull request at the review stage is not acceptable. It is generally more valuable to swiftly conclude a piece of work that the team has prioritised than to opportunistically seek additional changes, such as refactoring related code. That said, it can be useful to comment on refactoring opportunities without blocking the pull request. It may be that the author has some spare time they can use to make those changes, or can incorporate them with their next piece of work.</p> <p>Examples of things to look for and comment on in a code review:</p>"},{"location":"development-patterns/code-review/#what-has-changed","title":"What has changed","text":"<p>Are the changes focussed on a specific issue, referenced in the pull request description? If the changes go beyond the intended scope, should they be broken up to make the code review more manageable? If the code review is still a manageable size, consider making a non-blocking comment to remind the author that they could control the scope of future pull requests more carefully.</p>"},{"location":"development-patterns/code-review/#maintainability","title":"Maintainability","text":"<p>Is all new code extensible and easy for other developers to understand? Does it follow common design patterns? Look for unnecessary complexity and remember that this is subjective so take care not to be overly critical or opinionated when commenting on maintainability. Try to make specific suggestions for improvement, rather than simply stating a problem, but use open-ended language to encourage discussion.</p>"},{"location":"development-patterns/code-review/#duplication","title":"Duplication","text":"<p>Is there duplication within new code or between new and existing code? Could an existing abstraction be reused or should a new abstraction be created? When proposing an abstraction, consider using a non-blocking comment to open a dialogue about what the abstraction should be and whether it needs addressing now or as a separate piece of work.</p> <p>Identifying useful abstractions at the review stage may indicate that there wasn't enough collaboration before coding began. See this as a trigger to review the team's ways of working, rather than blocking a pull request unnecessarily.</p>"},{"location":"development-patterns/code-review/#reusability","title":"Reusability","text":"<p>Have any new abstractions been introduced? Are they sufficiently reusable? If other parts of the system could be updated to use the new abstractions, consider suggesting this in a non-blocking comment so it can be discussed and either addressed in the same pull request or added to the product backlog as technical debt.</p>"},{"location":"development-patterns/code-review/#impact-on-other-parts-of-the-system","title":"Impact on other parts of the system","text":"<p>Will the changes have knock-on effects or otherwise necessitate changes to other parts of the system?</p>"},{"location":"development-patterns/code-review/#unit-test-coverage","title":"Unit test coverage","text":"<p>Is all new code covered by detailed unit tests? Have any edge cases been missed? Don't rely on metrics such as code coverage. Inspect the code thoroughly and ensure that the tests contain appropriate assertions to confirm that all the intended functionality works as expected.</p>"},{"location":"development-patterns/code-review/#integration-tests","title":"Integration tests","text":"<p>Have integration tests been added to cover all changes to functionality?</p>"},{"location":"development-patterns/code-review/#suggesting-improvements","title":"Suggesting improvements","text":"<p>When concluding a review, there are three options:</p> <ol> <li>Comment</li> <li>Approve the PR</li> <li>Block the PR by requesting changes</li> </ol>"},{"location":"development-patterns/code-review/#when-to-comment","title":"When to comment","text":"<p>You should conclude with a comment when your review asks questions which need answering before you can determine whether the pull request is acceptable. If you haven't proposed a solution to a specific problem in the pull request, it's generally better to leave a neutral review than to block the PR with a request for change.</p> <p>If you have raised concerns or questions through inline comments, it may be helpful to give a concise summary in your concluding comment.</p>"},{"location":"development-patterns/code-review/#when-to-approve","title":"When to approve","text":"<p>You should approve a pull request when you have confirmed that:</p> <ol> <li>it meets the objectives set out in its description</li> <li>it doesn't introduce new defects or code that is hard to maintain</li> <li>all new and modified code has thorough unit test coverage</li> <li>integration tests have been added where appropriate</li> <li>there are no unresolved questions or comments against the pull request</li> </ol> <p>Occasionally, it may be prudent to accept a pull request which does not meet all of the above requirements, such as to resolve an urgent issue with the live product. Such cases must always be agreed between the product owner, author(s) and reviewer(s).</p> <p>If comments or questions on a pull request have been addressed elsewhere (e.g. face-to-face or on Slack), ensure that the outcome is recorded in comment replies so that it is visible to anyone looking back at the pull request in future for information.</p> <p>Use the Resolve button to make it clear which of your concerns have been addressed and which still need attention. A pull request should only be approved after all reviewers have explicitly indicated that each of their comments have been resolved. If several reviewers have replied to a comment thread, it may appropriate for any of them to resolve that conversation on behalf of the group.</p> <p>Sometimes, a reviewer may become unavailable after commenting on a pull request. When that happens, a second reviewer may accept the pull request without resolving the first reviewer's comments, as long as the author and second reviewer agree that they believe all legitimate concerns have been addressed. Before accepting the pull request, the second reviewer should reply to each unresolved comment to indicate that they believe it has been addressed.</p>"},{"location":"development-patterns/code-review/#when-to-request-changes","title":"When to request changes","text":"<p>You should request changes to a pull request if any of the following are true:</p> <ol> <li>it creates a defect in the product</li> <li>it exacerbates an existing defect</li> <li>it doesn't meet the objectives set out in its description</li> <li>it would make the product more difficult to maintain</li> <li>new or modified code lacks thorough unit tests</li> <li>required integration tests have not been included</li> </ol>"},{"location":"development-patterns/code-review/#when-not-to-request-changes","title":"When not to request changes","text":"<p>Perfection is the enemy of good</p> <p>A pull request does not need to be perfect to be good enough. Often, there are many solutions to a problem and one which is not the best is still good enough to meet current needs. Perhaps an alternative approach would be more efficient or an abstraction could reduce duplication, but those things may be less important than the next item in the product backlog.</p> <p>Make suggestions for improvement as comments without explicitly approving or rejecting the pull request. This way, your suggestions can open dialogue with the author about how important your suggestions are compared to other work you could each move on to.</p>"},{"location":"development-patterns/configuration/","title":"Configuration and secrets","text":"<p>The following guidance is only applicable for teams using the FCP Platform.  CDP has its own approach to configuration and secrets.</p> <p>Non sensitive application configuration data is persisted in the Platform repository for all environments other than Sandpit.</p> <p>For Sandpit, Azure Application Configuration is used.</p> <p>Sensitive application data is persisted in Azure Key Vault and referenced from the main configuration store.</p> <p>Each environment has it's own instance of the config stores and Key Vault.</p> <p>The configuration keys used must follow the naming convention below to ensure that CI and CD pipelines can retrieve values in a predictable and efficient way.</p> <p>ADP has a different approach to configuration and secrets management.  See the ADP documentation for more information.</p>"},{"location":"development-patterns/configuration/#helm-chart","title":"Helm chart","text":"<p>Configuration for an application running in a pod should be passed to the pod via a <code>ConfigMap</code> Kubernetes resource.</p> <p>Sensitive values should be passed to the pod via a <code>Secret</code> Kubernetes resource.</p> <p>Values for the <code>ConfigMap</code> and <code>Secret</code> should reference the <code>values.yaml</code> file within the Helm chart.</p>"},{"location":"development-patterns/configuration/#helm-values-in-deployment","title":"Helm values in deployment","text":"<p>During deployment, our CI or CD pipeline will read the <code>values.yaml</code> file in the Helm chart to identify all potential configuration values that may need to be sourced from Application Configuration and then matches each of them by key name.</p> <p>For example, if a helm chart contains the below, then the key to be matched will be <code>container.port</code>.</p> <pre><code>container:\n  port: 3000\n</code></pre> <p>If a key exists then the value, <code>3000</code> in the example, will be overwritten with the value in Application Configuration.</p>"},{"location":"development-patterns/configuration/#key-naming-standards","title":"Key naming standards","text":"<p>There are five accepted formats a key name can follow.  The examples all use the <code>container.port</code> key as a demonstration for simplicity.</p>"},{"location":"development-patterns/configuration/#commonkey","title":"<code>common/key</code>","text":"<p>These are values that are consistent, regardless of service or environment. </p> <p>For example <code>common/container.port</code> would mean all services would all get the same value wherever they deploy.</p> <p>The <code>common/</code> text is necessary due to the limitation the Azure CLI used in pipelines returns keys.  Without the prefix there would be no way to only return these common items.</p>"},{"location":"development-patterns/configuration/#environmentkey","title":"<code>environment/key</code>","text":"<p>These are values that are environment specific, but not service specific.  </p> <p>For example, <code>dev/container.port</code> would mean all services would get the same value when deploying to the <code>dev</code> environment.</p>"},{"location":"development-patterns/configuration/#servicecommonkey","title":"<code>service/common/key</code>","text":"<p>These are values that are service specific, but not environment specific.</p> <p>For example, <code>ffc-demo/common/container.port</code> would mean all deployments belonging to the <code>ffc-demo</code> service would get the same value wherever they deploy.</p> <p>As before the <code>/common/</code> text is necessary to ensure the Azure CLI returns only relevant values.</p>"},{"location":"development-patterns/configuration/#serviceenvironmentkey","title":"<code>service/environment/key</code>","text":"<p>These are values that are service and environment specific.</p> <p>For example, <code>ffc-demo/dev/container.port</code> would mean all deployments belonging to the <code>ffc-demo</code> service would get the same value when deploying to the <code>dev</code> environment.</p>"},{"location":"development-patterns/configuration/#prkey","title":"<code>pr/key</code>","text":"<p>These are only used for PR deployments in CI when you need to provide a different value for that PR deployment and still be able to provide individual microservie labels (see below).  They are also used by CI to obtain dynamic infrastructure values.</p> <p>For example, <code>pr/container.port</code> would mean all PR deployments would get the same port value.</p>"},{"location":"development-patterns/configuration/#microservice-specific-values","title":"Microservice specific values","text":"<p>When you want to provide a value depending on the specific microservice and not just the service you can add a different label to that key referencing the name of the microservice.</p> <p>For example, if you have a key <code>ffc-demo/common/container.port</code> with an unlabelled value of <code>4000</code> and a value of <code>5000</code> labelled <code>ffc-demo-web</code>, then all <code>ffc-demo</code> services would get a value of <code>4000</code> except the <code>ffc-demo-web</code> microservice which would get <code>5000</code>.</p>"},{"location":"development-patterns/configuration/#order-values-are-overridden","title":"Order values are overridden","text":"<p>Within the CI and CD pipelines, the values will be sourced and overwritten in the following order.</p> <ol> <li><code>common/key</code></li> <li><code>common/key</code> plus microservice label</li> <li><code>environment/key</code></li> <li><code>environment/key</code> plus microservice label</li> <li><code>service/common/key</code></li> <li><code>service/common/key</code> plus microservice label</li> <li><code>service/environment/key</code></li> <li><code>service/environment/key</code> plus microservice label</li> <li><code>pr/key</code> (CI PR deployment only)</li> <li><code>pr/key</code> plus microservice label (CI PR deployment only)</li> </ol>"},{"location":"development-patterns/configuration/#deciding-which-convention-to-use","title":"Deciding which convention to use","text":"<p>To improve performance in CI, keys should follow a convention that gives the narrowest scope possible.  </p> <p>For example, if a value is only used for one service or will vary depending on the service then it should be scoped to either <code>service/common</code> or <code>service/environment</code></p> <p>Only values that are used by everyone should be scoped to <code>common/</code> or <code>environment/</code></p>"},{"location":"development-patterns/configuration/#exceptions-to-the-rule","title":"Exceptions to the rule","text":"<p>The are some values that are exceptions to this rule because of dynamic infrastructure provisioning and related test execution in CI.</p> <p>Note: these exceptions only apply to the Sandpit (<code>SND</code>) environment.</p>"},{"location":"development-patterns/configuration/#ingress-values","title":"Ingress values","text":"<p>Helm charts define their ingress resources like the below:</p> <pre><code>ingress:\n  server:\n  endpoint:\n  class:\n</code></pre> <p>These values must use the hierarchy <code>environment/key</code> for example <code>dev/ingress.server</code> and use labels to separate each service's value.</p>"},{"location":"development-patterns/configuration/#database-values","title":"Database values","text":"<p>Helm charts define their database names like the below:</p> <pre><code>postgresService:\n  postgresUser:\n  postgresDb:\n</code></pre> <p>These values must use the hierarchy <code>environment/key</code> for example <code>dev/postgresService.postgresUser</code> and use labels to separate each service's value. </p>"},{"location":"development-patterns/configuration/#updating-values","title":"Updating values","text":"<p>In order for an application to pick up new configuration changes, the application must be redeployed by re-running the release pipeline.</p> <p>In Sandpit, values are added direct to Application Configuration via the Azure Portal or Azure CLI.  For other environments, the Platform repository is updated via a pull request, prior to re-running the pipeline.</p> <p>Full details for maintaining configuration in the Platform repository can be found in this wiki.</p> <p>Note: Key vault values MUST be added to Key Vault before merging any changes to the pipeline repository.  Otherwise all builds will fail for that service.  Only CCoE can update PreProduction and Production Key Vault values.</p>"},{"location":"development-patterns/containers/","title":"Developing in a container","text":"<p>The Architecture Vision prescribes a containerised microservice ecosystem.  Docker is the containerisation technology used within Defra.</p> <p>Containers are lightweight and fast. One of their main benefits for developers is that it is simple to replicate an application\u2019s environment and dependencies locally consistently. </p> <p>Crucially, they enable a workflow for your code that allows you to develop and test locally, push to upstream, and be confident that what you have built locally will work in CI and any environment.</p>"},{"location":"development-patterns/containers/#docker","title":"Docker","text":"<p>All microservices are built from supported Defra parent images for Node.js and .NET.  A <code>Dockerfile</code> will be included in each microservice repository containing a multi-stage build definition referencing these images.</p> <p>All microservice repositories created from the FFC Node template will include this <code>Dockerfile</code> already configured.</p> <p>Teams should use the LTS version of Node.js as the base image for their microservices.  This is to ensure that the microservice is supported for the longest period of time and regular security updates are applied.</p>"},{"location":"development-patterns/containers/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications using yaml configuration files.</p> <p>All microservice repositories created from the FCP Node template include a pre-configured set of Docker Compose yaml files to support local development and testing as well as some being a prerequisite for CI capability.</p> <p>An example Node.js <code>Dockerfile</code> showing a multi-stage build for both development and production.  Note that the <code>development</code> image runs the application in <code>watch</code> mode to support local development and testing, whilst <code>production</code> simply runs the application.</p> <p><code>development</code> is dependent on the local <code>package.json</code> including a watch script.  More on this below.</p> <pre><code>ARG PARENT_VERSION=2.3.0-node20.4.0\nARG PORT=3000\nARG PORT_DEBUG=9229\n\n# Development\nFROM defradigital/node-development:${PARENT_VERSION} AS development\nARG PARENT_VERSION\nLABEL uk.gov.defra.ffc.parent-image=defradigital/node-development:${PARENT_VERSION}\n\nARG PORT\nARG PORT_DEBUG\nENV PORT ${PORT}\nEXPOSE ${PORT} ${PORT_DEBUG}\n\nCOPY --chown=node:node package*.json ./\nRUN npm install\nCOPY --chown=node:node . .\nCMD [ \"npm\", \"run\", \"start:watch\" ]\n\n# Production\nFROM defradigital/node:${PARENT_VERSION} AS production\nARG PARENT_VERSION\nLABEL uk.gov.defra.ffc.parent-image=defradigital/node:${PARENT_VERSION}\n\nARG PORT\nENV PORT ${PORT}\nEXPOSE ${PORT}\n\nCOPY --from=development /home/node/app/ ./app/\nCOPY --from=development /home/node/package*.json ./\nRUN npm ci\nCMD [ \"node\", \"app\" ]\n</code></pre>"},{"location":"development-patterns/containers/#docker-composeyaml","title":"<code>docker-compose.yaml</code>","text":"<p>Used to define creation of the production image locally and in CI.  This file should include all configuration needed to create a clean production image.  Port and local volume mapping should be avoided in this file.</p> <p>The template repository will set a <code>container_name</code> property in this file so that containers created have shorter and more predictable names to support local development.  However, if local scaling of container instances is required, then this property should be removed as container names will need to be dynamic in that scenario.</p> <p>To avoid duplication, other dependent container images can be defined in this file such as PostgreSQL or Redis, but no volume or port bindings for those dependencies should be included.</p>"},{"location":"development-patterns/containers/#docker-composeoverrideyaml","title":"<code>docker-compose.override.yaml</code>","text":"<p>Used to apply overrides to <code>docker-compose.yaml</code> to support local development.  This is where port and volume mappings should be declared.</p> <p>If dependencies such as PostgreSQL or Redis are used, this is the file where volume and port bindings should be declared for those dependencies.</p> <p>This image will build the <code>development</code> image which typically is the same as production but will run the code in <code>watch</code> mode so changes made to the code locally are automatically picked up in the container and restart the application.</p>"},{"location":"development-patterns/containers/#avoiding-port-conflicts","title":"Avoiding port conflicts","text":"<p>When binding container ports to localhost, it is important to consider any conflicts that may occur with other services developers may wish to run locally.</p> <p>For example, if a service is made up of two microservices, both running on port <code>3000</code>.  Then both cannot be mapped to <code>localhost:3000</code> without a conflict.</p> <p>In this scenario, to successfully run both services on the same device with port binding, one of the services should bind the container's port <code>3000</code> to a different localhost port.</p> <p>The same consideration should be given to the debug port exposed to avoid a conflict on port <code>9229</code>, the default Node debug port.</p> <pre><code># service 1 docker-compose.override.yaml\nports:\n  - \"3000:3000\"\n  - \"9229:9229\"\n\n# service 2 docker-compose.override.yaml\nports:\n  - \"3001:3000\"\n  - \"9230:9229\"\n</code></pre> <p>This equally applies when binding dependency images such as PostgreSQL and Redis.</p>"},{"location":"development-patterns/containers/#docker-composedebugyaml","title":"<code>docker-compose.debug.yaml</code>","text":"<p>Used to start application in debug mode.  This is only required if you wish the application to wait for the debugger before starting the application.  If you just wish to attach a debugger to an already running instance, the override file is sufficient.</p>"},{"location":"development-patterns/containers/#docker-composetestyaml","title":"<code>docker-compose.test.yaml</code>","text":"<p>Used to run all tests in the repository.  This is a dependency of the FFC CI pipeline.  Port bindings should be avoided in this file to avoid conflicts between running builds.</p>"},{"location":"development-patterns/containers/#docker-composetestwatchyaml","title":"<code>docker-compose.test.watch.yaml</code>","text":"<p>Used as an override to <code>docker-compose.test.yaml</code> to run tests in <code>watch</code> mode to support Test Driven Development (TDD).  Changes to either application or test code will automatically trigger re-running of affected tests.</p> <p>All Node.js FFC microservices use Jest.  Jest has powerful capability to support multiple watch scenarios such as running individual tests, only tests affected by changes, only failed tests, filtering by regular expression as well as running the full suite.</p> <p>In order to understand which code has changed, Jest uses the local <code>.git</code> directory.  This means when running tests in a container, the local <code>.git</code> folder must be mounted to the container in <code>docker-compose.watch.yaml</code>.</p> <pre><code>volumes:\n  - ./.git:/home/node/.git\n</code></pre>"},{"location":"development-patterns/containers/#docker-composetestdebugyaml","title":"<code>docker-compose.test.debug.yaml</code>","text":"<p>Used to run Jest in watch mode but support debugging of tests.  This allows developers to have all the capability of <code>watch</code> but with the added bonus of being able to attach a debugger.</p> <p>Details of debugging tests are below.</p>"},{"location":"development-patterns/containers/#packagejson-scripts","title":"package.json scripts","text":"<p>To enable the capability provided by the above Docker Compose files, <code>package.json</code> needs to be configured to support the scripts referenced in the <code>command</code>.</p> <p>Below is an extract of the default <code>package.json</code> file provided by FFC Node template.</p> <pre><code>\"scripts\": {\n    \"pretest\": \"npm run test:lint\",\n    \"test\": \"jest --runInBand --forceExit\",\n    \"test:watch\": \"jest --coverage=false --onlyChanged --watch --runInBand\",\n    \"test:debug\": \"node --inspect-brk=0.0.0.0 ./node_modules/jest/bin/jest.js --coverage=false --onlyChanged --watch --runInBand --no-cache\",\n    \"test:lint\": \"standard\",\n    \"start:watch\": \"nodemon --inspect=0.0.0.0 --ext js --legacy-watch app/index.js\",\n    \"start:debug\": \"nodemon --inspect-brk=0.0.0.0 --ext js --legacy-watch app/index.js\"\n</code></pre>"},{"location":"development-patterns/containers/#pretest","title":"<code>pretest</code>","text":"<p>This will automatically run before the <code>test</code> script and will lint all JavaScript files in according with StandardJs standards.</p>"},{"location":"development-patterns/containers/#test","title":"<code>test</code>","text":"<p>This will run all Jest tests within the repository with no watch mode enabled and will output code coverage results on test completion.  This is primary used for CI, but can be run locally as a quick check of test status.</p> <p><code>--runInBand</code> will ensure that tests run sequentially rather than parallel.  Although this will result in slower running overall, it means that integration tests spanning containers have connections that are cleanly and predictably open and closed to avoid test disruption.</p> <p><code>--forceExit</code> will force Jest close a test with open connections 1 second after completion of the test.  Ideally this would not be needed, however in some scenarios Jest is unable to determine whether a Hapi server is still running even if it is cleanly shut down in the test.</p>"},{"location":"development-patterns/containers/#testwatch","title":"<code>test:watch</code>","text":"<p>This will run tests in <code>watch</code> mode and is the most commonly used by developers to support TDD.</p> <p><code>--coverage=false</code> - as typically only running a subset of tests, there is little value displaying a test coverage summary.  Disabling it also reduces lines written to the console to support developer focus.</p> <p><code>--onlyChanged</code> - start by only running tests that are affected by code changes.  Accuracy of this is dependent on the <code>.git</code> folder being mounted to the volume as described above as well as the folders containing test and application code.</p> <p><code>--watch</code> - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run.  Developers have the option to change the behaviour of watch mode.</p>"},{"location":"development-patterns/containers/#testwatch_1","title":"<code>test:watch</code>","text":"<p>This will run tests in <code>watch</code> mode and is the most commonly used by developers to support TDD.</p> <p><code>--coverage=false</code> - as typically only running a subset of tests, there is little value displaying a test coverage summary.  Disabling it also reduces lines written to the console to support developer focus.</p> <p><code>--onlyChanged</code> - start by only running tests that are affected by code changes.  Accuracy of this is dependent on the <code>.git</code> folder being mounted to the volume as described above.</p> <p><code>--watch</code> - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run.  Developers have the option to change the behaviour of watch mode.</p>"},{"location":"development-patterns/containers/#testdebug","title":"<code>test:debug</code>","text":"<p>This runs tests in <code>watch</code> mode and has all the same behaviour and running options as <code>test:watch</code>.  However, the key difference it this will wait for a debugger to be attached before starting test execution.  This enables developers to apply breakpoints in the test and application code to debug troublesome tests.</p> <p>An example Visual Studio Code debugging profile to use this script is provided below.</p> <p><code>--no-cache</code> - Jest caches files between test runs which can result in some breakpoints not being hit.  This disables that behaviour.</p>"},{"location":"development-patterns/containers/#testlint","title":"<code>test:lint</code>","text":"<p>Run linting with StandardJs only.</p>"},{"location":"development-patterns/containers/#startwatch","title":"<code>start:watch</code>","text":"<p>Starts the application in watch mode.  This is typically how developers will run all applications locally.  As code is changed, the running application in the container automatically identifies the changes and restarts the running application.</p> <p>Nodemon is used to orchestrate restarting of the application.</p> <p>This is dependent on the application code folder having a volume binding to the container.</p>"},{"location":"development-patterns/containers/#startdebug","title":"<code>start:debug</code>","text":"<p>This has the same behaviour as <code>start:watch</code> but like <code>test:debug</code> will wait for a debugger to be attached before executing any code.</p>"},{"location":"development-patterns/containers/#convenience-scripts","title":"Convenience scripts","text":"<p>Repositories created from FFC Node template will include two convenience scripts to support developers easily running the application utilising the above setup.</p>"},{"location":"development-patterns/containers/#scriptsstart","title":"<code>./scripts/start</code>","text":"<p>Run the application using Docker Compose.  Typically this is just a simple abstraction over <code>docker-compose up</code> however, is can be extended to ensure container runs in a specific container network or run database migrations prior to starting the application for example.</p>"},{"location":"development-patterns/containers/#scriptstest","title":"<code>./scripts/test</code>","text":"<p>Run tests.  Without any arguments provided will run the <code>test</code> script in <code>package.json</code>.</p> <p>To ensure clean running, all test containers are recreated.  Note that this will not affect data persisted in development databases for example if the above setup is followed.</p>"},{"location":"development-patterns/containers/#optional-arguments","title":"Optional arguments","text":"<p><code>-h</code> - shows all available arguments.</p> <p><code>-w</code> - runs tests in watch mode using <code>test:watch</code> script</p> <p><code>-d</code> - runs tests in debug mode using <code>test:debug</code> script</p>"},{"location":"development-patterns/containers/#debugging-code-running-in-a-container","title":"Debugging code running in a container","text":"<p>If the above setup is followed, then everything is in place to support debugging of applications and tests in containers.</p> <p>Developers are free to use their own choice of IDE, however, all example debug configurations within this guide will assume Visual Studio Code is used.</p> <p>These debug configurations should all be included in a <code>launch.json</code> file in the <code>.vscode</code> folder at the root of the repository.  This folder should be excluded from source control.</p>"},{"location":"development-patterns/containers/#application-debugging-profiles","title":"Application debugging profiles","text":""},{"location":"development-patterns/containers/#attach-to-an-already-running-container","title":"Attach to an already running container","text":"<pre><code>{\n  \"name\": \"Docker: Attach\",\n  \"type\": \"node\",\n  \"request\": \"attach\",\n  \"restart\": true,\n  \"port\": 9229,\n  \"remoteRoot\": \"/home/node\",\n  \"skipFiles\": [\n    \"&lt;node_internals&gt;/**\",\n    \"**/node_modules/**\"\n  ]\n}\n</code></pre> <p>This will attach to the node process exposed by the debug port.  Note that this uses the <code>localhost</code> port not the container port.  So if port <code>9229</code> is bound to a different port locally, then this value should be changed to match here.</p> <p><code>restart</code> - will ensure that as code is changed and the application restarted, the debugger is automatically reattached.</p> <p><code>remoteRoot</code> - must match the location of the code that matches the local workspace structure.  When using the Node.js Defra Docker base images the location will always be <code>home/node</code>.</p> <p><code>skipFiles</code> - an array of locations where debugging such skip.  Typically this would be internal Node.js code as well as those from third party npm modules.</p>"},{"location":"development-patterns/containers/#start-an-application-in-debug-mode","title":"Start an application in debug mode","text":"<pre><code>{\n      \"name\": \"Docker: Attach Launch\",\n      \"type\": \"node\",\n      \"request\": \"attach\",\n      \"remoteRoot\": \"/home/node\",\n      \"restart\": true,\n      \"port\": 9229,\n      \"skipFiles\": [\n        \"&lt;node_internals&gt;/**\",\n        \"**/node_modules/**\"\n      ],\n      \"preLaunchTask\": \"compose-debug-up\",\n      \"postDebugTask\": \"compose-debug-down\"\n    },\n</code></pre> <p>This will start a new container in debug mode using the <code>start:debug</code> <code>package.json</code> script.  The application will wait for a debugger before running any code.</p> <p>This is dependent on <code>preLaunchTask</code> and <code>postDebugTask</code> being defined in a <code>.vscode/tasks.json</code> file.</p> <p>An example of this is below.</p> <pre><code>{\n  \"version\": \"2.0.0\",\n  \"tasks\": [\n    {\n      \"label\": \"compose-debug-up\",\n      \"type\": \"shell\",\n      \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml up -d\"\n    },\n    {\n      \"label\": \"compose-debug-down\",\n      \"type\": \"shell\",\n      \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml down\"\n    }\n  ]\n}\n</code></pre>"},{"location":"development-patterns/containers/#test-debugging","title":"Test debugging","text":"<pre><code>{\n  \"name\": \"Docker: Jest Attach\",\n  \"type\": \"node\",\n  \"request\": \"attach\",\n  \"port\": 9229,\n  \"restart\": true,\n  \"timeout\": 10000,\n  \"remoteRoot\": \"/home/node\",\n  \"disableOptimisticBPs\": true,\n  \"continueOnAttach\": true,\n  \"skipFiles\": [\n    \"&lt;node_internals&gt;/**\",\n    \"**/node_modules/**\"\n  ]\n}\n</code></pre> <p>This assumes that the <code>./script/test -d</code> command referenced above has already been run and the test suit is waiting for the debugger to attach.  This profile will attach that debugger.</p> <p><code>disableOptimisticBPs</code> - this needs to be set as <code>true</code> as Jest takes a copy of all test files and uses these copies for execution.  If this is not disabled then this process can result in breakpoints not being mapped to their correct location.</p> <p><code>continueOnAttach</code> - instruct the pending test execution to continue once the debugger is attached.</p>"},{"location":"development-patterns/containers/#other-debugging-profiles","title":"Other debugging profiles","text":"<p>A range of different debugging profiles can be found in this repository as well as a test application setup to the above standards to test them.</p>"},{"location":"development-patterns/containers/#debugging-net-in-a-linux-container","title":"Debugging .NET in a Linux container","text":"<p>.NET services can be developed using VS Code or Visual Studio.</p> <p>As all FCP services are designed to be developed and run in Linux containers, debugging them requires the attachment of a debugger from the running container.  </p> <p>In the case of .NET, this is dependent on a remote debugger being present in the container image.</p> <p>All FCP services based on the Defra .NET development image include the <code>vsdbg</code> remote debugger.</p>"},{"location":"development-patterns/containers/#attaching-to-the-remote-debugger","title":"Attaching to the remote debugger","text":""},{"location":"development-patterns/containers/#vs-code","title":"VS Code","text":"<ul> <li>add your breakpoint</li> <li>start the container with <code>docker-compose up --build</code> from the repository root directory</li> <li>in VS Code create a <code>launch.json</code> configuration similar to the below substituting the name of the container, <code>ffc-demo-payment-service-core</code></li> </ul> <p><code>json   {     \"version\": \"0.2.0\",     \"configurations\": [       {         \"name\": \".NET Core Docker Attach\",         \"type\": \"coreclr\",         \"request\": \"attach\",         \"processId\": \"${command:pickRemoteProcess}\",         \"pipeTransport\": {           \"pipeProgram\": \"docker\",           \"pipeArgs\": [ \"exec\", \"-i\", \"ffc-demo-payment-service-core\" ],           \"debuggerPath\": \"/vsdbg/vsdbg\",           \"pipeCwd\": \"${workspaceRoot}\",           \"quoteArgs\": false         },         \"sourceFileMap\": {           \"/home/dotnet\": \"${workspaceFolder}\"         }       }     ]   }</code></p> <ul> <li>start the VS Code debugger using this launch configuration</li> <li>in the context menu, select the process matching the running application, eg. <code>FFCDemoPaymentService</code></li> <li>the breakpoint can now be hit within VS Code</li> </ul>"},{"location":"development-patterns/containers/#visual-studio","title":"Visual Studio","text":"<p>Visual Studio does not integrate with the WSL filesystem, so WSL users must clone the repository in Windows to debug using Visual Studio. </p> <p>It is important that the following git configuration setting is present to ensure that cloning in Windows does not alter existing line endings</p> <pre><code>git config --global core.autocrlf input\n</code></pre> <p>For services which require environment variables to be read from the host, it is recommended to store these in a <code>.env</code> file in the repository as Docker Compose will automatically read this file when running the container.  This file must be excluded from source control.</p> <p>This process has a prerequisite of the user having Docker Desktop installed which includes Docker Compose by default.</p> <ol> <li>add your break point</li> <li>using Powershell, start the container with <code>docker-compose up --build</code> from the repository root directory</li> <li>in Visual Studio, select <code>Debug -&gt; Attach to process</code></li> <li>select <code>Docker (Linux Container)</code> for <code>Connection type</code></li> <li>type the name of the container in <code>Connection target</code>, eg. <code>ffc-demo-payment-service</code></li> <li>click <code>Refresh</code></li> <li>select process matching running application, eg <code>FFCDemoPaymentService</code></li> <li>click <code>Attach</code></li> <li>select <code>Managed (.NET Core for Unix)</code> code type</li> <li>click <code>Ok</code></li> <li>the breakpoint can now be hit within Visual Studio</li> </ol> <p>Note volume mounts do not appear to work with this approach, so for changes to be picked up, the container will need to be recreated.</p>"},{"location":"development-patterns/containers/#other-useful-docker-development-guides","title":"Other useful Docker development guides","text":"<p>Defra has well documented standards and guidance on developing with containers, that provides further examples of good local development practice.</p>"},{"location":"development-patterns/databases/","title":"Databases","text":"<p>PostgreSQL is the preferred database for microservices in the FCP Platform.  This guide describes the process for creating a database for a microservice and configuring the microservice to use it.</p>"},{"location":"development-patterns/databases/#request-creation-of-microservice-database","title":"Request creation of microservice database","text":"<p>CCoE are the only team with access to create database within your Azure Database for PostgreSQL in the Sandpit environment.  The name of the database should match the microservice repository name.  Microservices should not share a database.</p> <p>For example, a microservice named <code>ffc-demo-claim-service</code> would have a database named <code>ffc_demo_claim_service</code></p> <p>Note here the use of underscores instead of the normal hyphen convention. Postgres hyphens require escaping with double quote marks so underscores are preferred.</p>"},{"location":"development-patterns/databases/#request-creation-of-microservice-database-role","title":"Request creation of microservice database role","text":"<p>Request Cloud Services to create a database role that is bound to the Managed Identity created for the microservice (Azure guidence), for example <code>ffc-snd-demo-claim-role</code>.</p> <p>This identity must also be assigned to the Jenkins VM to ensure that Liquibase migrations can run.</p>"},{"location":"development-patterns/databases/#create-a-liquibase-changelog","title":"Create a Liquibase changelog","text":"<p>The FCP Platform CI and deployment pipelines support database migrations using Liquibase.</p> <p>Create a Liquibase changelog defining the structure of your database available from the root of your microservice repository in <code>changelog/db.changelog.xml</code>.</p> <p>Guidance on creating a Liquibase changelog is outside of the scope of this guide, so please check current best practice with the FCP Platform Team.</p>"},{"location":"development-patterns/databases/#update-docker-compose-files-to-use-postgres-service-and-environment-variables","title":"Update Docker Compose files to use Postgres service and environment variables","text":"<p>Update <code>docker-compose.yaml</code>, <code>docker-compose.override.yaml</code>, and <code>docker-compose.test.yaml</code> to include a Postgres service and add Postgres environment variables to the microservice.</p> <p>An example Postgres service:</p> <pre><code>services:\n  # Microservice definition here\n  ffc-&lt;workstream&gt;-&lt;service&gt;-postgres:\n    image: postgres:11.4-alpine\n    environment:\n      POSTGRES_DB: ffc_&lt;workstream&gt;_&lt;service&gt;\n      POSTGRES_PASSWORD: ppp\n      POSTGRES_USER: postgres\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\nvolumes:\n  postgres_data: {}\n</code></pre> <p>Add dependency on the Postgres service and environment variables the microservice <code>services</code> definition:</p> <pre><code>services:\n  # Microservice definition here\n    depends_on:\n      - ffc-&lt;workstream&gt;-&lt;service&gt;-postgres\n    environment:\n      POSTGRES_DB: ffc_&lt;workstream&gt;_&lt;service&gt;\n      POSTGRES_PASSWORD: ppp\n      POSTGRES_USER: postgres\n      POSTGRES_HOST: ffc-&lt;workstream&gt;-&lt;service&gt;-postgres\n      POSTGRES_PORT: 5432\n      POSTGRES_SCHEMA_NAME: public\n</code></pre> <p>Replace <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"development-patterns/databases/#add-docker-compose-files-to-run-liquibase-migrations","title":"Add Docker Compose files to run Liquibase migrations","text":"<p>Add a <code>docker-compose.migrate.yaml</code> to the root of the microservice based on this example.</p>"},{"location":"development-patterns/databases/#update-microservice-helm-chart","title":"Update microservice Helm chart","text":"<p>Update the Helm chart values file (<code>helm/&lt;REPO_NAME&gt;/values.yaml</code>) with default values for the Postgres service:</p> <pre><code>postgresService:\n  postgresDb: ffc_&lt;workstream&gt;_&lt;service&gt;\n  postgresExternalName:\n  postgresHost: ffc-&lt;workstream&gt;-&lt;service&gt;-postgres\n  postgresPort: 5432\n  postgresSchema: public\n  postgresUser: postgres\n</code></pre> <p>replacing <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"development-patterns/databases/#update-configmap","title":"Update ConfigMap","text":"<p>Update the <code>ConfigMap</code> template of the Helm Chart (<code>helm/&lt;REPO_NAME&gt;/templates/config-map.yaml</code>) to include the environment variables for the Postgres database:</p> <pre><code>POSTGRES_DB: {{ quote .Values.postgresService.postgresDb }}\nPOSTGRES_HOST: {{ quote .Values.postgresService.postgresHost }}\nPOSTGRES_PORT: {{ quote .Values.postgresService.postgresPort }}\nPOSTGRES_SCHEMA_NAME: {{ quote .Values.postgresService.postgresSchema }}\n</code></pre>"},{"location":"development-patterns/databases/#createupdate-the-container-secret","title":"Create/Update the container Secret","text":"<p>Create (or update) the Secret template in <code>helm/&lt;REPO_NAME&gt;/templates/container-secret.yaml</code>:</p> <pre><code>{{- include \"ffc-helm-library.container-secret\" (list . \"&lt;REPO_NAME&gt;.container-secret\") -}}\n{{- define \"&lt;REPO_NAME&gt;.container-secret\" -}}\nstringData:\n  POSTGRES_USER: {{ .Values.postgresService.postgresUser | quote }}\n{{- end -}}\n</code></pre> <p>replacing <code>&lt;REPO_NAME&gt;</code> with the git repository name.</p> <p>Update the Helm chart values file (<code>helm/&lt;REPO_NAME&gt;/values.yaml</code>) with a name for the Secret:</p> <pre><code>containerSecret:\n  name: ffc-&lt;workstream&gt;-&lt;service&gt;-container-secret\n  type: Opaque\n</code></pre> <p>replacing <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"development-patterns/databases/#add-liquibase-migration-scripts","title":"Add Liquibase migration scripts","text":"<p>Create the following resources from the root level of your repository * <code>scripts/migration/database-down</code></p> <pre><code>#!/bin/bash\necho \"db update on $POSTGRES_HOST $SCHEMA_NAME as $SCHEMA_USERNAME\"\n/scripts/postgres-wait &amp;&amp; /liquibase/liquibase \\\n--driver=org.postgresql.Driver \\\n--changeLogFile=/changelog/db.changelog.xml \\\n--url=jdbc:postgresql://$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB \\\n--username=\"$SCHEMA_USERNAME\" --password=\"$SCHEMA_PASSWORD\" --defaultSchemaName=\"$SCHEMA_NAME\" \\\nrollback v0.0.0\n</code></pre> <ul> <li><code>scripts/migration/database-up</code></li> </ul> <pre><code>#!/bin/bash\necho \"db update on $POSTGRES_HOST $SCHEMA_NAME as $SCHEMA_USERNAME\"\n/scripts/postgres-wait &amp;&amp; /liquibase/liquibase \\\n--driver=org.postgresql.Driver \\\n--changeLogFile=/changelog/db.changelog.xml \\\n--url=jdbc:postgresql://$POSTGRES_HOST:$POSTGRES_PORT/$POSTGRES_DB \\\n--username=\"$SCHEMA_USERNAME\" --password=\"$SCHEMA_PASSWORD\" --defaultSchemaName=\"$SCHEMA_NAME\" \\\nupdate\n\n</code></pre> <ul> <li><code>scripts/postgres-wait</code></li> </ul> <pre><code>#!/bin/bash\necho \"waiting for postgres $POSTGRES_HOST:$POSTGRES_PORT\"\nresponse=''\nmax_tries=15\ncount=0\nwhile :\ndo\n  ((count++))\n  if [ \"$count\" -ge \"$max_tries\" ]; then\n        echo \"postgres did not respond in time\"\n        exit 1\n    fi\n  response=$(wget -SO- -T 1 -t 1 http://$POSTGRES_HOST:$POSTGRES_PORT 2&gt;&amp;1 | grep 'No data received')\n  if [ \"$response\" ]; then\n    echo \"postgres server available\"\n    break\n  fi\n  printf '.'\n  sleep 2\ndone\necho postgres started\n</code></pre>"},{"location":"development-patterns/databases/#add-values-to-azure-key-vault-and-app-configuration","title":"Add values to Azure Key Vault and App Configuration","text":"<p>Azure Key Vault is used to store the Postgres username and Azure App Configuration is used to stores values required by the Jenkins CI pipelines.</p> <p>Create the following secret in Azure Key Vault via the Azure Portal:</p> <ul> <li>Name: <code>snd-postgres&lt;workstream&gt;&lt;service&gt;User</code>; Value: <code>&lt;managed-identity&gt;@&lt;azure-postgres-instance&gt;</code> (e.g. <code>ffc-snd-demo-web-role@mypostgresserver</code>)</li> </ul> <p>Create the following entries in Azure App Configuraiton via the Azure Portal:</p> <ol> <li> <p>A key-value entry where Key: <code>&lt;environment&gt;/postgresService.postgresDb</code> (e.g. <code>dev/postgresService.postgresDb</code>); Value: <code>ffc_&lt;workstream&gt;_&lt;service&gt;</code> (e.g. <code>ffc_demo_claim_service</code>); Label: <code>&lt;REPO_NAME&gt;</code> (e.g. <code>ffc-demo-claim-service</code>)</p> </li> <li> <p>A Key Vault reference entry where Key: <code>dev/postgresService.postgresUser</code>; Key Vault Secret Key: <code>dev-postgres&lt;workstream&gt;&lt;service&gt;User</code>; Label: <code>&lt;REPO_NAME&gt;</code> (e.g. <code>ffc-demo-claim-service</code>)</p> </li> </ol> <p>where <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> refer to those parts of the queue name described above.</p> <p>Note in environments beyond Sandpit, Azure DevOps will provision databases suffixed with the target environment.  The values should be ammended accordingly. e.g. <code>ffc_demo_claim_service_dev</code></p>"},{"location":"development-patterns/databases/#add-database-code-to-the-microservice","title":"Add database code to the microservice","text":"<p>Update your microservice code using the relevant Azure authentication SDKs for your language.</p> <p>Patterns for using a Postgres database in microservice code are outside of the scope of this guide. An example is shown below for a Node.js microservice, but please check current best practice with the FCP Platform Team.</p>"},{"location":"development-patterns/databases/#nodejs-example","title":"Node.js example","text":"<p>Install the Azure Authentication SDK NPM package: <code>npm install @azure/identity</code>.</p> <p>With the Managed Identity bound to your microservice in the Kubernetes cluster (following the guidence above), you can then access the database using the username <code>&lt;managed-identity&gt;@&lt;azure-postgres-instance&gt;</code> (e.g. <code>ffc-snd-demo-web-role@mypostgresserver</code>) and an access token as the password:</p> <pre><code>async function example() {\n  const { DefaultAzureCredential } = require('@azure/identity')\n  const credential = new DefaultAzureCredential()\n  const accessToken = await credential.getToken('https://ossrdbms-aad.database.windows.net', { requestOptions: { timeout: 1000 } })\n  cfg.password = accessToken.token\n}\n</code></pre>"},{"location":"development-patterns/documentation/","title":"Documentation","text":"<p>FCP services are delivered following an agile methodology based on evidenced user needs. Documentation is expected to follow this mantra and be iterated with the code as the product evolves.</p> <p>Documentation should be \"just enough\" to cover what needs to be communicated and presented as simply and effectively as possible.</p>"},{"location":"development-patterns/documentation/#documentation-location","title":"Documentation Location","text":"<p>Code documentation will be stored in the same source code repository as the product it supports unless it contains sensitive information.</p> <p>FCP developments will mostly be within an open source microservices ecosystem. Each microservice will have its own repository and its own documentation.</p>"},{"location":"development-patterns/documentation/#readme","title":"Readme","text":"<p>A readme file must exist in every repo in markdown format (.md). It must include the following if they apply</p> <ul> <li>description of service</li> <li>prerequisites</li> <li>development tools setup</li> <li>test tools setup</li> <li>how to run in development</li> <li>how to run tests</li> <li>how to make changes to the code</li> </ul>"},{"location":"development-patterns/documentation/#additional-detail","title":"Additional Detail","text":"<p>Depending on the complexity of the service, the following may be included in the readme, or it may be more effective to capture in separate documentation/diagrams referenced by the readme.</p> <ul> <li>how product fits into wider architecture</li> <li>internal product architecture</li> <li>data structure</li> <li>API end points</li> <li>monitoring</li> <li>error handling</li> <li>audit</li> <li>user access</li> <li>security</li> <li>complexity worth documenting</li> <li>pipelines</li> </ul>"},{"location":"development-patterns/documentation/#architectural-decisions","title":"Architectural Decisions","text":"<p>Architectural decisions are discussed and agreed at the Architectural Design Authority.  A library of Architectural Decision records should be maintained.</p>"},{"location":"development-patterns/documentation/#technical-debt","title":"Technical Debt","text":"<p>Technical debt should be captured on the backlog and regularly addressed as part of the development process.</p>"},{"location":"development-patterns/documentation/#code","title":"Code","text":"<p>Effective software engineering can negate the need for additional documentation. Code should follow Defra's digital standards and apply appropriate recognised design patterns.</p> <p>We should aim to write readable code that requires minimal comments as per Defra's digital standards. However in some scenarios effective code comments can be used to explain complexity.</p> <p>Well named and considered unit tests can also act as effective documentation.</p>"},{"location":"development-patterns/documentation/#backlog-items","title":"Backlog items","text":"<p>Backlog items are captured in the form of backlog items in Azure DevOps.</p> <p>There is no need to duplicate backlog items in additional documentation.</p> <p>Items should be should be referenced in associated Pull Requests.</p>"},{"location":"development-patterns/documentation/#sensitive-documentation","title":"Sensitive Documentation","text":"<p>Sensitive documentation should be stored within a private location with access restricted to those who need it.</p>"},{"location":"development-patterns/documentation/#apis","title":"APIs","text":""},{"location":"development-patterns/documentation/#rest","title":"REST","text":"<p>Teams need to document their API using OpenAPI 3.0 specification which can be found here.</p> <p>This approach is supported by GDS technical standards which states \"The open standards board recommends that government organisations use OpenAPI version 3 to describe RESTful APIs\" for more information you can read</p> <p>The OpenAPI file should be kept in a <code>docs</code> folder in the top level of the folder and named <code>openapi.yaml</code> unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named <code>openapi-&lt;version&gt;.yaml</code>. This should be documented in the <code>API</code> section of the <code>Microservice Reference Library</code>.</p> <p>An example of how to do this can be found in ffc-demo-payment-service.</p>"},{"location":"development-patterns/documentation/#asynchronous","title":"Asynchronous","text":"<p>Teams need to document their API using AsyncAPI specification which can be found here</p> <p>The AsyncAPI file should be kept in a <code>docs</code> folder in the top level of the folder and named <code>asyncapi.yaml</code> unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named <code>asyncapi-&lt;version&gt;.yaml</code>. This should be documented in the <code>Events</code> section of the <code>Microservice Reference Library</code>.</p> <p>An example of how to do this can be found in ffc-demo-payment-service.</p>"},{"location":"development-patterns/events-and-messages/","title":"Events and messages","text":"<p>Where possible, service to service communication will be asynchronous using Azure Service Bus.  </p> <p>Depending on the pattern being applied the term <code>event</code> or <code>message</code> may be more appropriate.  For simplicity, for the remainder of this page, the term <code>event</code> will be used to refer to both these definitions.</p>"},{"location":"development-patterns/events-and-messages/#avoid-tight-coupling","title":"Avoid tight coupling","text":"<p>Services should be as loosely coupled as possible.  This reduces the dependency a change in one service could have on related services.</p> <p>Therefore, events should be published to <code>Topics</code> as opposed to <code>Queues</code> where possible.</p>"},{"location":"development-patterns/events-and-messages/#format","title":"Format","text":"<p>When sending an event to Azure Service Bus, it will be decorated by default broker properties.  For full details on the specification, see the Microsoft documentation</p>"},{"location":"development-patterns/events-and-messages/#specification","title":"Specification","text":"<p>CloudEvents.io is a specification for describing event data in a common way and should be considered if it is appropriate for the use case.</p>"},{"location":"development-patterns/events-and-messages/#local-development-with-azure-service-bus","title":"Local development with Azure Service Bus","text":"<p>Until recently, Azure Service Bus did not have an official emulator, however one has now been released</p> <p>Teams now have several options for local development.</p>"},{"location":"development-patterns/events-and-messages/#use-the-microsoft-azure-service-bus-emulator","title":"Use the Microsoft Azure Service Bus Emulator","text":"<p>The official emulator can be used to run a local instance of Azure Service Bus.</p>"},{"location":"development-patterns/events-and-messages/#setup","title":"Setup","text":"<p>Add a definition for both the emulator and SQL Server to your <code>docker-compose.yaml</code> file.</p> <pre><code>services:\n  servicebus-emulator:\n    container_name: servicebus-emulator\n    image: mcr.microsoft.com/azure-messaging/servicebus-emulator\n    volumes:\n      - \"./config.json:/ServiceBus_Emulator/ConfigFiles/Config.json\"\n    ports:\n      - \"5672:5672\"\n    environment:\n      SQL_SERVER: sqledge\n      MSSQL_SA_PASSWORD: Some-really-strong-password!123\n      ACCEPT_EULA: \"Y\"\n    depends_on:\n      - sqledge\n\n  sqledge:\n    container_name: sqledge\n    image: \"mcr.microsoft.com/azure-sql-edge\"\n    environment:\n      ACCEPT_EULA: \"Y\"\n      MSSQL_SA_PASSWORD: Some-really-strong-password!123\n</code></pre>"},{"location":"development-patterns/events-and-messages/#creating-queues-topics-and-subscriptions","title":"Creating queues, topics and subscriptions","text":"<p>A <code>config.json</code> file can be used to pre-configure the emulator with queues, topics and subscriptions during startup.</p> <pre><code>{\n  \"UserConfig\": {\n    \"Namespaces\": [\n      {\n        \"Name\": \"sbemulatorns\",\n        \"Queues\": [\n          {\n            \"Name\": \"test-queue\",\n            \"Properties\": {\n              \"DeadLetteringOnMessageExpiration\": false,\n              \"DefaultMessageTimeToLive\": \"PT1H\",\n              \"DuplicateDetectionHistoryTimeWindow\": \"PT20S\",\n              \"ForwardDeadLetteredMessagesTo\": \"\",\n              \"ForwardTo\": \"\",\n              \"LockDuration\": \"PT1M\",\n              \"MaxDeliveryCount\": 10,\n              \"RequiresDuplicateDetection\": false,\n              \"RequiresSession\": false\n            }\n          }\n        ],\n        \"Topics\": [\n          {\n            \"Name\": \"test-topic\",\n            \"Properties\": {\n              \"DefaultMessageTimeToLive\": \"PT1H\",\n              \"DuplicateDetectionHistoryTimeWindow\": \"PT20S\",\n              \"RequiresDuplicateDetection\": false\n            },\n            \"Subscriptions\": [\n              {\n                \"Name\": \"test-subscription\",\n                \"Properties\": {\n                  \"DeadLetteringOnMessageExpiration\": false,\n                  \"DefaultMessageTimeToLive\": \"PT1H\",\n                  \"LockDuration\": \"PT1M\",\n                  \"MaxDeliveryCount\": 10,\n                  \"ForwardDeadLetteredMessagesTo\": \"\",\n                  \"ForwardTo\": \"\",\n                  \"RequiresSession\": false\n                }\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"Logging\": {\n      \"Type\": \"File\"\n    }\n  }\n}\n</code></pre>"},{"location":"development-patterns/events-and-messages/#connecting-to-the-emulator","title":"Connecting to the emulator","text":"<p>Using the <code>@azure/service-bus</code> SDK, the following example shows how to connect.</p> <pre><code>import { ServiceBusClient } from '@azure/service-bus'\n\nconst connectionString = 'Endpoint=sb://localhost;SharedAccessKeyName=anything;SharedAccessKey=anything;UseDevelopmentEmulator=true'\nconst client = new ServiceBusClient(connectionString)\n</code></pre> <p>Note: The <code>UseDevelopmentEmulator=true</code> flag is required to connect to LocalSandbox.  <code>SharedAccessKeyName</code> and <code>SharedAccessKey</code> can be any value, but must be present.</p> <p>Once connected, the SDK can be used as normal as if connected to Azure Service Bus.</p>"},{"location":"development-patterns/events-and-messages/#using-across-different-services","title":"Using across different services","text":"<p>As FCP follows a multi-repository approach, it is likely that services created with different Compose files will need to communicate with each other through a single emulator instance.  Equally, these services may need to run in isolation.</p> <p>One way to achieve this is by declaring the emulator instance in both Compose files and using the <code>label</code> property to identify if an instance is already running.</p> <p>The Compose files must also declare a common network to allow the services to communicate.</p>"},{"location":"development-patterns/events-and-messages/#service-a","title":"Service A","text":"<pre><code>services:\n  ffc-service-a:\n    image: ffc-service-a\n    networks:\n      - ffc-service\n    depends_on:\n      - servicebus-emulator\n\n  servicebus-emulator:\n    container_name: servicebus-emulator\n    image: mcr.microsoft.com/azure-messaging/servicebus-emulator\n    environment:\n      SQL_SERVER: sqledge\n      MSSQL_SA_PASSWORD: Some-really-strong-password!123\n      ACCEPT_EULA: \"Y\"\n    depends_on:\n      - sqledge\n    labels:\n      com.docker.compose.service.role: ffc-service-servicebus-emulator\n    networks:\n      - ffc-service\n\n  sqledge:\n    container_name: sqledge\n    image: \"mcr.microsoft.com/azure-sql-edge\"\n    environment:\n      ACCEPT_EULA: \"Y\"\n      MSSQL_SA_PASSWORD: Some-really-strong-password!123\n    labels:\n      com.docker.compose.service.role: ffc-service-servicebus-emulator-sqledge\n    networks:\n      - ffc-service\n\nnetworks:\n  ffc-service:\n    driver: bridge\n    name: ffc-service\n</code></pre>"},{"location":"development-patterns/events-and-messages/#service-b","title":"Service B","text":"<pre><code>services:\n  ffc-service-b:\n    image: ffc-service-b\n    networks:\n      - ffc-service\n    depends_on:\n      - servicebus-emulator\n\n  servicebus-emulator:\n    container_name: servicebus-emulator\n    image: mcr.microsoft.com/azure-messaging/servicebus-emulator\n    environment:\n      SQL_SERVER: sqledge\n      MSSQL_SA_PASSWORD: Some-really-strong-password!123\n      ACCEPT_EULA: \"Y\"\n    depends_on:\n      - sqledge\n    labels:\n      com.docker.compose.service.role: ffc-service-servicebus-emulator\n    networks:\n      - ffc-service\n\n  sqledge:\n    container_name: sqledge\n    image: \"mcr.microsoft.com/azure-sql-edge\"\n    environment:\n      ACCEPT_EULA: \"Y\"\n      MSSQL_SA_PASSWORD: Some-really-strong-password!123\n    labels:\n      com.docker.compose.service.role: ffc-service-servicebus-emulator-sqledge\n    networks:\n      - ffc-service\n\nnetworks:\n  ffc-service:\n    driver: bridge\n    name: ffc-service\n</code></pre> <p>Each repository can include a <code>start</code> script to run <code>docker compose up</code> programmatically deciding whether to start the emulator container based on the presence of the <code>com.docker.compose.service.role</code> label.</p> <pre><code>if [ -n \"$(docker container ls --filter label=com.docker.compose.service.role=ffc-service-servicebus-emulator --format={{.ID}})\" ]; then\n  echo \"Service Bus container already exists, skipping creation\"\n  args=\"--scale servicebus-emulator=0 --scale sqledge=0\"\nfi\n\ndocker compose up $@ # $@ passes any arguments to the script\n</code></pre> <p>An example repository using the emulator can be found here</p>"},{"location":"development-patterns/events-and-messages/#use-localsandbox-emulator","title":"Use LocalSandbox emulator","text":"<p>An unofficial emulator called LocalSandbox is available.  This can be used to run a local instance of Azure Service Bus with the majority of the features available in the cloud.</p>"},{"location":"development-patterns/events-and-messages/#setup_1","title":"Setup","text":"<p>Add a definition for <code>local-sandbox</code> to your <code>docker-compose.yaml</code> file.</p> <pre><code>services:\n  local-sandbox:\n    image: localsandbox/localsandbox\n    container_name: local-sandbox\n</code></pre>"},{"location":"development-patterns/events-and-messages/#creating-queues-topics-and-subscriptions_1","title":"Creating queues, topics and subscriptions","text":"<p>By default, LocalSandBox will create a queue called <code>default</code>.  </p> <p>To create additional topics and subscriptions, you can use <code>docker-exec</code> to run the <code>az</code> CLI command within the LocalSandbox container.</p> <p>Note: LocalSandbox alias the <code>az</code> command to <code>azl</code></p> <pre><code>docker exec local-sandbox \\\n  azl servicebus queue create \\\n  --name test-queue \\\n  --namespace-name default \\\n  --resource-group default\n\ndocker exec local-sandbox \\\n  azl servicebus topic create \\\n  --name test-topic \\\n  --namespace-name default \\\n  --resource-group default\n\ndocker exec local-sandbox \\\n  azl servicebus topic subscription create \\\n  --name test-subscription \\\n  --namespace-name default \\\n  --resource-group default \\\n  --topic-name test-topic\n</code></pre> <p>If using Docker Compose version 1.30.0 or later, you can use the <code>post_start</code> lifecycle event to run these commands automatically when the container starts.</p> <pre><code>services:\n  local-sandbox:\n    image: localsandbox/localsandbox\n    container_name: local-sandbox\n    post_start:\n      command: &gt;\n        azl servicebus queue create --name test-queue --namespace-name default --resource-group default\n        azl servicebus topic create --name test-topic --namespace-name default --resource-group default\n        azl servicebus topic subscription create --name test-subscription --namespace-name default --resource-group default\n</code></pre>"},{"location":"development-patterns/events-and-messages/#connecting-to-localsandbox","title":"Connecting to LocalSandbox","text":"<p>LocalSandbox creates a Service Bus namespace named <code>default.default.default.localhost</code>.</p> <p>Using the <code>@azure/service-bus</code> SDK, the following example shows how to connect.</p> <pre><code>import { ServiceBusClient } from '@azure/service-bus'\n\nconst connectionString = 'Endpoint=sb://default.default.default.localhost;SharedAccessKeyName=anything;SharedAccessKey=anything;UseDevelopmentEmulator=true'\nconst client = new ServiceBusClient(connectionString)\n</code></pre> <p>Note: The <code>UseDevelopmentEmulator=true</code> flag is required to connect to LocalSandbox.  <code>SharedAccessKeyName</code> and <code>SharedAccessKey</code> can be any value, but must be present.</p> <p>Once connected, the SDK can be used as normal as if connected to Azure Service Bus.</p>"},{"location":"development-patterns/events-and-messages/#using-across-different-services_1","title":"Using across different services","text":"<p>Similar to the Microsoft Azure Service Bus Emulator, a single emulator can be used across different services.</p>"},{"location":"development-patterns/events-and-messages/#service-a_1","title":"Service A","text":"<pre><code>services:\n  ffc-service-a:\n    image: ffc-service-a\n    networks:\n      - ffc-service\n    depends_on:\n      - local-sandbox\n\n  local-sandbox:\n    image: localsandbox/localsandbox\n    container_name: local-sandbox\n    labels:\n      com.docker.compose.service.role: ffc-service-local-sandbox\n    networks:\n      - ffc-service\n\nnetworks:\n  ffc-service:\n    driver: bridge\n    name: ffc-service\n</code></pre>"},{"location":"development-patterns/events-and-messages/#service-b_1","title":"Service B","text":"<pre><code>services:\n  ffc-service-b:\n    image: ffc-service-b\n    networks:\n      - ffc-service\n    depends_on:\n      - local-sandbox\n\n  local-sandbox:\n    image: localsandbox/localsandbox\n    container_name: local-sandbox\n    labels:\n      com.docker.compose.service.role: ffc-service-local-sandbox\n    networks:\n      - ffc-service\n\nnetworks:\n  ffc-service:\n    driver: bridge\n    name: ffc-service\n</code></pre> <p>Each repository can include a <code>start</code> script to run <code>docker compose up</code> programmatically deciding whether to start the LocalSandbox container based on the presence of the <code>com.docker.compose.service.role</code> label.</p> <pre><code>if [ -n \"$(docker container ls --filter label=com.docker.compose.service.role=ffc-service-local-sandbox --format={{.ID}})\" ]; then\n  echo \"LocalSandbox container already exists, skipping creation\"\n  args=\"--scale local-sandbox=0\"\nfi\n\ndocker compose up $@ # $@ passes any arguments to the script\n</code></pre> <p>An example repository using LocalSandbox can be found here</p>"},{"location":"development-patterns/events-and-messages/#use-the-azure-service-bus-instance-in-the-sandpit-environment","title":"Use the Azure Service Bus instance in the Sandpit environment","text":"<p>Local development may use a connection to the Azure Service Bus instance in the Sandpit environment.</p> <p>To avoid collisions between different developers, each developer should have their own set of queues, topics and subscriptions in the Sandpit environment suffixed with their initials.</p> <p>A repository has been created to support faster creation and deletion of these resources at scale.</p> <p>Once created, developers can set a local environment variable with their initials.</p> <pre><code>export MESSAGE_SUFFIX=-jw\n</code></pre> <p>Then reference this in the Docker Compose file of the relevant service.</p> <pre><code>services:\n  ffc-service:\n    environment:\n      MESSAGE_TEST_QUEUE: test-queue${MESSAGE_SUFFIX}\n      MESSAGE_TEST_TOPIC: test-topic${MESSAGE_SUFFIX}\n      MESSAGE_TEST_SUBSCRIPTION: test-subscription${MESSAGE_SUFFIX}\n</code></pre>"},{"location":"development-patterns/events-and-messages/#naming-conventions","title":"Naming conventions","text":"<p>The naming convention for queues, topics and subscriptions should be as follows:</p> <ul> <li>all entities should be prefixed with <code>ffc-&lt;service&gt;</code> where <code>&lt;service&gt;</code> is the name of the service, eg <code>ffc-demo</code></li> <li>topics and queues should be named to denote the entity they are related to, eg <code>ffc-demo-payment</code> or <code>ffc-demo-claim</code></li> <li>topics and queues should be suffixed with the environment they are deployed to, eg <code>ffc-demo-payment-dev</code> or <code>ffc-demo-payment-prd</code></li> <li>subscriptions should match the name of the consuming microservice, eg <code>ffc-demo-payment-web</code></li> </ul>"},{"location":"development-patterns/events-and-messages/#pull-requests-provisioning","title":"Pull Requests provisioning","text":"<p>Queues, topics and subscriptions can be automatically provisioned for each microservice PR by the Jenkins CI pipeline to ensure encapsulation of infrastructure.</p> <p>Create a <code>provision.azure.yaml</code> file in the root directory of your microservice, and populate with the Service Bus infrastructure that need provisioning:</p> <pre><code>resources:\n  queues:\n    - name: &lt;queue_identifier&gt;\n  topics:\n    - name: &lt;topic_identifier&gt;\n</code></pre> <p>where the <code>&lt;queue_identifier&gt;</code> and/or <code>&lt;topic_identifier&gt;</code> relates to the part of the queue or topic name described above. For example for the queue <code>ffc-demo-payment-dev</code>, <code>&lt;queue_identifier&gt;</code> would be replaced with <code>payment</code>.</p> <p>For every topic specified in <code>provision.azure.yaml</code>, the CI pipeline will also create a subscription, so subscriptions do not need to be explicitly specified.</p> <p>Add a <code>name</code> entry in the <code>provision.azure.yaml</code> for each required queue and topic/subscription.</p>"},{"location":"development-patterns/events-and-messages/#standards","title":"Standards","text":"<ul> <li>services should guarantee at least once delivery of messages, using the \"outbox pattern\" where appropriate</li> <li>consuming services should be idempotent and safely handle duplicate messages</li> <li>teams should ensure a suitable delivery count, retry mechanism, message lock and dead letter queue policy is in place for each entity</li> <li>teams should use session queues for request/response message patterns</li> </ul>"},{"location":"development-patterns/helm-charts/","title":"Helm charts","text":"<p>Helm charts allow multiple Kubernetes resource definitions to be deployed and un-deployed as a single unit.</p>"},{"location":"development-patterns/helm-charts/#source-control","title":"Source control","text":"<ul> <li>Helm charts are source controlled in the same repository as the microservice they relate to</li> <li>Helm charts are saved in a <code>helm</code> directory and named the same as the repository. For example, <code>./helm/ffc-demo-web</code> directory</li> <li>Helm chart versions are automatically updated by CI in line with the application version</li> </ul>"},{"location":"development-patterns/helm-charts/#helm-chart-library","title":"Helm chart library","text":"<ul> <li>to keep Helm charts DRY, the FCP Helm Chart Library is used as a base for all resource definitions</li> <li>consuming Helm charts only need to define where there is variation from the base Helm chart</li> <li>the library helps teams abstract complexity of Kubernetes resource definitions as well as providing a consistent approach to Helm chart development and container security</li> </ul> <p>Note: For services using ADP, ADP has it's own Helm chart library for both application and infrastructure.</p>"},{"location":"development-patterns/helm-charts/#helm-chart-repository","title":"Helm chart repository","text":"<ul> <li>Helm charts are published by CI to a Helm chart repository within Azure Container Registry</li> <li>Helm charts are only published on a main branch build</li> <li>Helm charts are automatically versioned by CI pipeline to be consistent with the application version</li> <li>Helm charts for in flight Pull Requests are not published</li> </ul>"},{"location":"development-patterns/helm-charts/#labelling-resources","title":"Labelling resources","text":"<p>Labels can be added to Kubernetes resources to categorise and query objects.</p> <p>In order to take full advantage of using labels, they should be applied on every resource object within a Helm chart. i.e. all deployments, services, ingresses etc.</p> <p>When using the FCP Helm Chart Library, these labels are automatically applied to all resource objects.</p>"},{"location":"development-patterns/helm-charts/#required-labels","title":"Required labels","text":"<p>Each Helm chart templated resource should have the below labels. Example placeholder references to the <code>values.yaml</code> file are provided.</p> <pre><code>metadata:\n  labels:\n    app: {{ quote .Values.namespace }}\n    app.kubernetes.io/name: {{ quote .Values.name }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: {{ quote .Values.labels.version }}\n    app.kubernetes.io/component: {{ quote .Values.labels.component }}\n    app.kubernetes.io/part-of: {{ quote .Values.namespace }}\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\n    environment: {{ quote .Values.environment }}\n</code></pre> <p>Note: <code>Deployment</code> resource objects should have two sets of labels, one for the actual deployment and another for the pod template the deployment manages.</p>"},{"location":"development-patterns/helm-charts/#selectors","title":"Selectors","text":"<p>Services selectors should be matched by app and name. Selectors should be consistent once set, otherwise updates to Helm charts will be rejected.</p> <pre><code>selector:\n  app: {{ quote .Values.name }}\n  app.kubernetes.io/name: {{ quote .Values.name }}\n</code></pre>"},{"location":"development-patterns/helm-charts/#resources","title":"Resources","text":"<p>Predictable demands are important to help Kubernetes find the right place for a pod within a cluster. When it comes to resources such as a CPU and memory, understanding the resource needs of a pod will help Kubernetes allocate the pod to the most appropriate Node and generally improve the stability of the cluster.</p>"},{"location":"development-patterns/helm-charts/#declaring-a-profile","title":"Declaring a profile","text":"<ul> <li>all pods declare both a <code>request</code> and <code>limit</code> value for CPU and memory</li> <li>pods with consistent usage patterns are run as<code>guaranteed</code> ie equal <code>request</code> and <code>limit</code> values</li> <li>pods with spiky usage patterns can be run as <code>burstable</code> ie <code>request</code> lower than <code>limit</code> values</li> <li>pods without <code>limit</code> values or <code>request</code> values are run as <code>best-effort</code>.  These are not used in FCP as they can cause instability in the cluster</li> </ul>"},{"location":"development-patterns/helm-charts/#resource-quotas","title":"Resource quotas","text":"<p>Clusters will limit available resources within a service's namespace using a <code>ResourceQuota</code>.</p> <p>Teams should frequently review the resource usage of their pods and adjust the <code>ResourceQuota</code> accordingly.</p> <p>The resource quota should cover all pods running at their limit values at full level of replicas.</p> <p>The quota is deployed by the FCP Platform's <code>platform-aks-configuration</code> pipeline.</p>"},{"location":"development-patterns/helm-charts/#resource-profiling","title":"Resource profiling","text":"<p>Performance testing an end to end service is the only way to fully understand required resource needs.  Teams should ensure a suitable performance test strategy is in place.</p>"},{"location":"development-patterns/helm-charts/#priority-classes","title":"Priority classes","text":"<p>FCP Platform has three priority classes. </p> <p>Priority indicates the importance of a pod relative to other pods. If a pod cannot be scheduled, the scheduler tries to pre-empt (evict) lower priority pods to make scheduling of the pending pod possible.</p> <p>In the event of over utilisation of a cluster, Kubernetes will start to kill lower priority pods first to maintain stability.</p>"},{"location":"development-patterns/helm-charts/#high-1000","title":"<code>high</code> (1000)","text":"<p>Reserved primarily for customer facing or critical workload pods.</p>"},{"location":"development-patterns/helm-charts/#default-600","title":"<code>default</code> (600)","text":"<p>Default option suitable for most pods.</p>"},{"location":"development-patterns/helm-charts/#low-200","title":"<code>low</code> (200)","text":"<p>For pods where downtime is more tolerable.</p> <p>When using the FCP Helm Chart Library, the <code>Default</code> priority class is automatically applied to all pods.  This value can be overwritten by adding the below to the <code>values.yaml</code> file.</p> <pre><code>deployment:\n  priorityClassName: high\n</code></pre>"},{"location":"development-patterns/helm-charts/#resource-profile-impact","title":"Resource profile impact","text":"<p>In the event a cluster has to make a choice between killing one of two services sharing the same priority level, the resource profile configuration will influence which is killed in the order below.   1. Best effort   2. Burstable   3. Guaranteed</p>"},{"location":"development-patterns/probes/","title":"Probes","text":"<p>Kubernetes has two types of probes, readiness and liveness.</p> <p>Kubernetes uses readiness probes to know when a container is ready to start accepting traffic.</p> <p>Kubernetes uses liveness probes to know when to restart a container.</p> <p>The FCP Helm chart library includes templates for both readiness and liveness probes.</p>"},{"location":"development-patterns/probes/#configuring-probes","title":"Configuring probes","text":"<p>Probes can be configured in the Helm chart on a <code>Deployment</code> resource, under the container node.</p> <p>The above is a simple example of an Http readiness and liveness probes.</p> <pre><code>readinessProbe:\n  path: /healthy\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n\nlivenessProbe:\n  path: /healthz\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n</code></pre> <p>In this example, the cluster will wait for <code>10</code> seconds after the pod is deployed.  It will then poll both the liveness and readiness endpoints on port <code>3000</code> every <code>10</code> seconds.  </p> <p>If it receives three successive status codes other than <code>200</code> for the readiness probe it will stop routing traffic to that pod.</p> <p>Note: the readiness probe only prevents traffic routing through the internal cluster network, it does not for example stop a service from consuming messages via Azure Service Bus.</p> <p>If it receives three successive status codes other than <code>200</code> for the liveness probe it will assume the pod is unresponsive and kill it.</p> <p>A liveness probe works in conjunction with the <code>restartPolicy</code> value. In order to restart the <code>restartPolicy</code> must be set to <code>Always</code> or <code>OnFailure</code>.  The FCP Helm chart default is <code>Always</code>.</p>"},{"location":"development-patterns/probes/#values","title":"Values","text":"<p><code>path</code>: the URL route the liveness probe should sent a response to.</p> <p><code>port</code>: the port on which the service is exposing</p> <p><code>initialDelaySeconds</code>: how long before the first probe should be sent. This should be safely longer than it takes the pod to start up, otherwise the pod could be stuck in a reboot loop</p> <p><code>periodSeconds</code>: how often the liveness probe should check the pod is responsive. Recommendation is between 10 and 20 seconds</p> <p><code>failureThreshold</code>: how many probe failures before the pod is automatically restarted</p> <p><code>timeoutSeconds</code>: how long to wait for a response before considering the probe failed</p> <p>As well as Http probes, there are also command and TCP based probes, full details can be found in the documentation</p>"},{"location":"development-patterns/probes/#role-in-deployments","title":"Role in deployments","text":"<p>During deployment from Helm, if a pod does not report healthy within <code>5</code> minutes, the deployment will be rolled back automatically and the deployment pipeline will fail.</p>"},{"location":"development-patterns/release-notes/","title":"Release notes","text":""},{"location":"development-patterns/release-notes/#automated-release-notes","title":"Automated release notes","text":"<p>Pull requests are squash merged into the main branch. Following this the FCP CI pipeline will automatically create an immutable GitHub release.</p> <p>This release will automatically include the version and a release note.</p> <p>Teams should therefore consider the content of the squash commit.</p>"},{"location":"development-patterns/release-notes/#manual-release-notes","title":"Manual release notes","text":"<p>For scenarios where automated release notes are not applied, follow the below template.</p>"},{"location":"development-patterns/release-notes/#title","title":"Title","text":"<p>semver Version (e.g. <code>1.0.0</code>)</p>"},{"location":"development-patterns/release-notes/#description","title":"Description","text":""},{"location":"development-patterns/release-notes/#major","title":"Major","text":"<ul> <li>bullet list of major changes</li> </ul>"},{"location":"development-patterns/release-notes/#minor","title":"Minor","text":"<ul> <li>bullet list of minor changes</li> </ul>"},{"location":"development-patterns/release-notes/#patch","title":"Patch","text":"<ul> <li>bullet list of patch changes</li> </ul> <p>Each of the above will be optional and dependant on the changes that have been made.</p> <p>For complex changes include code snippets in the relevant description heading.</p>"},{"location":"development-patterns/release-notes/#production-deployments","title":"Production deployments","text":"<p>For all production deployments, an approved change request in myIT (ServiceNow) is required.  Each change request should include the microservice name and version being deployed.</p> <p>Teams typically maintain their own release calendar to track their daily deployments to Production.</p>"},{"location":"development-patterns/shared-assets/","title":"Shared assets","text":"<p>To better enable developer agility, the FCP provide several shared assets to reduce the effort needed to deliver a microservice ecosystem within.</p> <p>Teams are expected to use these shared assets.</p>"},{"location":"development-patterns/shared-assets/#development-guide","title":"Development guide","text":"<p>The development guide is a library of FCP standards and guides useful to support developers within the FCP programme.</p> <ul> <li>Development guide</li> </ul>"},{"location":"development-patterns/shared-assets/#docker-parent-images","title":"Docker parent images","text":"<p>Docker parent images have been created for Node.js and .NET.  These parent images prevent duplication in each microservice by abstracting common layers away from each microservice.</p>"},{"location":"development-patterns/shared-assets/#dockerhub","title":"DockerHub","text":"<ul> <li>Node.js</li> <li>Node.js development</li> <li>.NET</li> <li>.NET development</li> </ul>"},{"location":"development-patterns/shared-assets/#github","title":"GitHub","text":"<ul> <li>Node.js</li> <li>.NET</li> </ul>"},{"location":"development-patterns/shared-assets/#jenkins-library","title":"Jenkins library","text":"<p>The Jenkins shared library abstracts common CI stages away from each microservice repository and ensures that all adequate assurance steps are completed.  The library supports the addition of custom steps at key points in the pipeline.</p>"},{"location":"development-patterns/shared-assets/#github_1","title":"GitHub","text":"<ul> <li>Jenkins library</li> </ul>"},{"location":"development-patterns/shared-assets/#helm-chart-library","title":"Helm chart library","text":"<p>The Helm chart library keeps microservice Helm charts dry by abstracting common resource definitions away from each microservice repository.  The packaged chart is hosted in a GitHub repository.</p>"},{"location":"development-patterns/shared-assets/#github_2","title":"GitHub","text":"<ul> <li>Helm chart library</li> <li>Helm chart repository</li> </ul>"},{"location":"development-patterns/shared-assets/#microservice-template","title":"Microservice template","text":"<p>A GitHub template repository has been created to allow teams to quickly get started with new microservice.  The template includes all common setup steps such as Docker, Helm charts and Jenkins.</p>"},{"location":"development-patterns/shared-assets/#github_3","title":"GitHub","text":"<ul> <li>Node.js</li> </ul>"},{"location":"development-patterns/shared-assets/#net-sonarcloud-scanning","title":".NET SonarCloud scanning","text":"<p>Analysing containerised .NET microservices with SonarCloud in CI is challenging.  To simplify this process a Docker image has been created to abstract this complexity away from both microservices and the Jenkins library.</p>"},{"location":"development-patterns/shared-assets/#dockerhub_1","title":"DockerHub","text":"<ul> <li>.NET SonarCloud Analysis</li> </ul>"},{"location":"development-patterns/shared-assets/#github_4","title":"GitHub","text":"<ul> <li>.NET SonarCloud Analysis</li> </ul>"},{"location":"development-patterns/shared-assets/#secret-scanning","title":"Secret scanning","text":"<p>All FCP repositories are regularly scanned for committed secrets.  A utility repository has been created in support of that.</p>"},{"location":"development-patterns/shared-assets/#github_5","title":"GitHub","text":"<ul> <li>Git secret scanning</li> <li>Secret scanning</li> </ul>"},{"location":"development-patterns/shared-assets/#npm-publishing","title":"npm publishing","text":"<p>Simplify publishing packages to npm, including switching between pre-release and full releases.</p>"},{"location":"development-patterns/shared-assets/#github_6","title":"GitHub","text":"<ul> <li>npm publish</li> </ul>"},{"location":"development-patterns/shared-assets/#dockerhub_2","title":"DockerHub","text":"<ul> <li>npm publish</li> </ul>"},{"location":"development-patterns/shared-assets/#kafka-admin-client","title":"Kafka admin client","text":"<p>Simplify viewing and deleting Kafka consumer groups</p>"},{"location":"development-patterns/shared-assets/#github_7","title":"GitHub","text":"<ul> <li>Kafka admin client</li> </ul>"},{"location":"development-patterns/shared-assets/#messaging-npm-package","title":"Messaging npm package","text":"<p>npm package to simplify Azure Service Bus sending and consuming in line with FCP standards</p>"},{"location":"development-patterns/shared-assets/#github_8","title":"GitHub","text":"<ul> <li>ffc-messaging</li> </ul>"},{"location":"development-patterns/shared-assets/#npm","title":"npm","text":"<ul> <li>ffc-messaging</li> </ul>"},{"location":"development-patterns/shared-assets/#events-npm-package","title":"Events npm package","text":"<p>npm package to simplify Azure Event Hubs sending and consuming in line with FCP standards</p>"},{"location":"development-patterns/shared-assets/#github_9","title":"GitHub","text":"<ul> <li>ffc-events</li> </ul>"},{"location":"development-patterns/shared-assets/#npm_1","title":"npm","text":"<ul> <li>ffc-events</li> </ul>"},{"location":"development-patterns/version-control/","title":"Version Control","text":"<p>These standards are based on Defra Digital's version control standards, but are extended to cover not only the application version, but version control of Docker images and Helm charts.</p>"},{"location":"development-patterns/version-control/#semantic-versioning","title":"Semantic Versioning","text":"<p>We use semantic versioning to manage code, images and charts.</p> <p>Given a version number MAJOR.MINOR.PATCH, increment the: - <code>MAJOR</code> version when you make incompatible API changes, - <code>MINOR</code> version when you add functionality in a backwards compatible manner, and - <code>PATCH</code> version when you make backwards compatible bug fixes.</p> <p>Additional labels for pre-release and build metadata are available as extensions to the <code>MAJOR.MINOR.PATCH</code> format.</p> <p>Full details can be found at semver.org.</p>"},{"location":"development-patterns/version-control/#application","title":"Application","text":"<ul> <li> <p>for a Node.js microservice, the version is updated in the <code>package.json</code> file's <code>version</code> property.  </p> </li> <li> <p>for a .NET microservice, the version is updated in the <code>.csproj</code> file in a <code>&lt;Version&gt;</code> tag.</p> </li> </ul>"},{"location":"development-patterns/version-control/#image","title":"Image","text":"<ul> <li> <p>prior to pushing to a container registry, all Docker images must be tagged by CI. The tag must match the application version.</p> </li> <li> <p>if the image is created as part of a pull request workflow, then the PR number is instead used as the image tag.</p> </li> </ul>"},{"location":"development-patterns/version-control/#helm-chart","title":"Helm chart","text":"<ul> <li> <p>Helm chart versions are updated in the <code>Chart.yaml</code> file's <code>version</code> property.</p> </li> <li> <p>as Helm charts are saved in the same repository as the application they manage, the version numbers are updated in sync with the application by CI.</p> </li> <li> <p>Helm charts for PR deployments are not be pushed to a Helm repository.</p> </li> </ul> <p>Note when using the FFC Jenkins library, Helm chart versioning is handled automatically, so no action is needed</p>"},{"location":"development-patterns/version-control/#releases","title":"Releases","text":"<ul> <li>releases are packaged in the source code repository using the version number of the application by CI</li> </ul>"},{"location":"development-patterns/version-control/#databases","title":"Databases","text":"<ul> <li> <p>databases use Liquibase changesets for deployment and rollback.  </p> </li> <li> <p>all changesets are versioned independently of the application.</p> </li> <li> <p>there must be an initial <code>0.0.0</code> version before any changeset, to enable full rollback of a database</p> </li> </ul>"},{"location":"development-patterns/authentication/defra-identity/","title":"Defra Identity","text":"<p>Defra Identity is a service that provides external user authentication and authorisation for Defra services. It is based on the OAuth 2.0 and OpenID Connect standards and is backed by Azure B2C.</p> <p>Defra Identity supports authentication through a Government Gateway or Rural Payments account.</p> <p>FCP services should use Defra Identity with a Rural Payments account.  This is because the majority of users will have one and it is only possible to retrieve customer and land data data if the user is authenticated with a Rural Payments account.</p> <p>In the future, GOV.UK One Login will be available as an authentication method via Defra Identity.  It is expected this will be an opportunity to migrate users from their Rural Payments account to GOV.UK One Login.</p> <p>Defra Identity only supports external user authentication and cannot be used for internal users.</p>"},{"location":"development-patterns/authentication/defra-identity/#example","title":"Example","text":"<p>An example repository has been created to demonstrate how to implement Defra Identity in an FCP service.  The repository can be found in GitHub</p>"},{"location":"development-patterns/authentication/defra-identity/#credentials","title":"Credentials","text":"<p>When users sign in with a Rural Payments account, they input their Customer Reference Number (CRN) and password.  </p> <p>The CRN is a unique identifier for the individual.</p>"},{"location":"development-patterns/authentication/defra-identity/#permissions","title":"Permissions","text":"<p>Typically, user data and permissions exist in the Defra Customer service.  However, for Rural Payments accounts, this data is stored in Siti Agri.</p> <p>Once a user has been authenticated, a service must retrieve this data from Siti Agri via an API call.  A complication of this is that this API cannot retrieve all permissions for an individual, instead it can only retrieve permissions for a specific organisation the person is associated with.</p> <p>For this reason, FCP journey's should be based around an organisation, rather than an individual.  A typical journey would be:</p> <ol> <li>User logs in with their Rural Payments account</li> <li>User selects an organisation they wish to act on behalf of</li> <li>Service retrieves permissions for that organisation</li> <li>User can then perform actions on behalf of that organisation</li> </ol>"},{"location":"development-patterns/authentication/defra-identity/#login-page","title":"Login page","text":"<p>The login page is part of Defra Identity as opposed to the consuming service.  Whenever a user needs to authenticate, the service should redirect the user to the Defra Identity login page.  Once authenticated the user will be redirected back to the service.  More details on how to implement this are below.</p> <p>The login page can be customised by the Defra Identity team for each consuming service.</p>"},{"location":"development-patterns/authentication/defra-identity/#organisation-picker","title":"Organisation picker","text":"<p>To support selection of an organisation, Defra Identity provides an organisation picker.  Similar to the login page, this is owned by Defra Identity and can be customised for each consuming service, however data is limited to SBI, organisation Id, and organisation name.</p> <p>More details on how to interact with this component are below.</p> <p>The organisation picker can also be disabled.  This can be useful if no permissions or existing data is required, however, it is expected that the majority of FCP services will require the picker.</p>"},{"location":"development-patterns/authentication/defra-identity/#authorisation-code-flow","title":"Authorisation Code Flow","text":"<p>Defra Identity uses the Authorisation Code Flow to authenticate users.  This is a secure method of authentication that involves the following steps:</p> <ol> <li>User is redirected to the Defra Identity login page</li> <li>User logs in</li> <li>User selects an organisation</li> <li>User is redirected back to the consuming service with an authorisation code</li> <li>Consuming service exchanges the authorisation code for an access token</li> <li>Consuming service stores JWT token in session</li> </ol>"},{"location":"development-patterns/authentication/defra-identity/#single-sign-on","title":"Single Sign on","text":"<p>Defra Identity supports single sign on (SSO).  This means that once a user has authenticated with one service, they will not need to re-authenticate with another service until their session expires.  When SSO is enabled, consuming services should still redirect to Defra Identity for authentication, however, the user will not need to enter their credentials.</p> <p>SSO session is managed through a session cookie on the Defra Identity domain.  The session cookie will expire after 30 minutes without interaction with Defra Identity or if the user closes the browser.</p> <p>FCP services should be setup to support SSO.</p>"},{"location":"development-patterns/authentication/defra-identity/#implementing-defra-identity-in-an-fcp-service","title":"Implementing Defra Identity in an FCP service","text":""},{"location":"development-patterns/authentication/defra-identity/#example-repository","title":"Example repository","text":"<p>An example repository has been created to demonstrate how to implement Defra Identity in an FCP service.  The repository can be found in GitHub.</p> <p>The example repository uses a combination of <code>@hapi/bell</code> and <code>@hapi/cookie</code> to manage the session and authenticate the user.</p> <p><code>@hapi/bell</code> offers a simple way to authenticate users with Defra Identity and is the recommended approach.  However, this guide will include a more verbose setup to demonstrate the underlying principles.</p>"},{"location":"development-patterns/authentication/defra-identity/#onboard-with-defra-identity","title":"Onboard with Defra Identity","text":"<p>Contact the Defra Identity team to onboard your service.</p> <p>They will provide you with the following information.</p> <ul> <li>Client ID</li> <li>Client Secret</li> <li>Service ID</li> <li>Well Known URL</li> <li>Policy name</li> </ul> <p>all FCP services should use the <code>signupsigninsfi</code> policy to ensure users are authenticated with a Rural Payments account</p> <p>During onboarding, you will need to provide Defra Identity with the following information:</p> <ul> <li>Redirect URL for each environment (<code>GET</code> endpoint Defra Identity should redirect the user to after authentication and organisation selection)</li> <li>Sign out URL for each environment (<code>GET</code> endpoint Defra Identity should redirect the user to after sign out)</li> </ul> <p>Note: a <code>localhost</code> and port should also be provided to support local development.  If a URL is not provided to Defra Identity, authentication will not work in that environment.</p>"},{"location":"development-patterns/authentication/defra-identity/#decide-session-setup","title":"Decide session setup","text":"<p>Defra Identity will return a JWT token to the consuming service after authentication.  This token should be stored in FCP service's session.  Teams are free to determine the most appropriate way to manage their session dependent on their use case.</p> <p>Some potential options are: - Store the JWT in a session cookie - Store the JWT in a session store (e.g. Redis) and use a session cookie to reference the session store</p> <p>FCP services must provide a mechanism to clear the session when the user logs out.</p> <p>FCP services must automatically end the session if the user closes the browser.</p> <p>For simplicity, this guide will assume <code>@hapi/yar</code> is used to manage the overall session, whilst the JWT token is stored in a separate session cookie.</p>"},{"location":"development-patterns/authentication/defra-identity/#understand-endpoints","title":"Understand endpoints","text":"<p>The Well Known URL provided by Defra Identity will provide details of all endpoints in JSON format.  Services can use this to programmatically interact with Defra Identity.</p> <p>The most important endpoints are: - <code>authorization_endpoint</code> - URL to redirect the user to for authentication - <code>token_endpoint</code> - URL to exchange the authorisation code for an access token - <code>end_session_endpoint</code> - URL to redirect the user to after sign out - <code>jwks_uri</code> - URL to retrieve the public key to verify the JWT token</p>"},{"location":"development-patterns/authentication/defra-identity/#implement-login","title":"Implement login","text":"<p>When a user needs to authenticate, the service should redirect the user to the <code>authorization_endpoint</code>.</p> <p>This redirection should include teh following query parameters:</p> <ul> <li><code>p</code> - the policy name provided by Defra Identity</li> <li><code>client_id</code> - the client ID provided by Defra Identity</li> <li><code>service_id</code> - the service ID provided by Defra Identity</li> <li><code>state</code> - value to maintain state between the request and the callback as well as protect against CSRF attacks (see below)</li> <li><code>nonce</code> - value to protect against token replay attacks (see below)</li> <li><code>redirect_uri</code> - the URL to redirect the user to after authentication</li> <li><code>scope</code> - always set to <code>openid offline_access</code></li> <li><code>response_type</code> - always set to <code>code</code></li> <li><code>response_mode</code> - always set to <code>query</code></li> <li><code>forceReselection</code> - optional parameter to force the user to reselect an organisation on the picker page, even if they have already selected one previously</li> <li><code>relationshipId</code> - optional parameter to preselect an organisation and skip the picker page.  This must be an Organisation Id from Rural Payments</li> <li><code>prompt</code> - optional parameter to force the user to reauthenticate if set to true.</li> </ul> <p>Once the user has authenticated and selected an organisation, they will be redirected back to the service with an authorisation code in the query parameters.  The code can then be exchanged for an access token via the <code>token_endpoint</code>.</p>"},{"location":"development-patterns/authentication/defra-identity/#state","title":"State","text":"<p>The <code>state</code> parameter is used to maintain state between the request and the callback.  This is important to protect against CSRF attacks.  The service should generate a random value for each request and store it in the session.  The value should be included in the request to Defra Identity and checked against the value in the session when the user is redirected back to the service.</p> <p>The <code>state</code> parameter is a <code>base64</code> encoded string.</p> <p>An example of how to generate the <code>state</code> parameter using the <code>uuid</code> npm package is below:</p> <pre><code>const state = Buffer.from(JSON.stringify({\n  id: uuidv4(), // Unique identifier for the request\n})).toString('base64')\n\nrequest.yar.set('state', state)\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#nonce-initialisation-vector","title":"Nonce / Initialisation Vector","text":"<p>The <code>nonce</code> parameter is used to protect against token replay attacks.  The service should generate a random value for each request and store it in the session.  The value should be included in the request to Defra Identity and checked against the value in the session when the user is redirected back to the service.</p> <p>As a random value is used, the term <code>nonce</code> is not strictly accurate and \"Initialisation Vector\" is a more accurate description.</p> <p>Unlike the state which is returned direct from Defra Identity, the nonce is included in the final JWT token.</p> <p>An example of how to generate the <code>nonce</code> parameter using the <code>uuid</code> npm package is below:</p> <pre><code>const initialisationVector = uuidv4()\n\nrequest.yar.set('initialisationVector', initialisationVector)\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#handle-callback-from-defra-identity","title":"Handle callback from Defra Identity","text":"<p>Once the user has authenticated and selected an organisation, they will be redirected back to the service with an authorisation code in the query parameters.</p> <p>The service should validate the <code>state</code> property and exchange the authorisation code for an access token via the <code>token_endpoint</code>.</p> <p>Once the token is retrieved, the <code>nonce</code> property should be validated and the token stored in session state.</p> <pre><code>// redirect route from Defra Identity\nserver.route({\n  method: 'GET',\n  path: '/sign-in-oidc',\n  handler: async (request, h) =&gt; {\n    const { code, state } = request.query\n\n    // validate state\n    const sessionStateId = request.yar.get('state')\n    const { id } = JSON.parse(Buffer.from(state, 'base64').toString())\n\n    if (id !== sessionStateId) {\n      throw new Error('Invalid state, possible CSRF attack')\n    }\n\n    // get token\n    const { payload } = await Wreck.post(`${token_endpoint}?client_id=${clientId}&amp;client_secret=${clientSecret}&amp;code=${code}&amp;grant_type=authorization_code&amp;redirect_uri=${redirectUri}`, {\n      headers: {\n        'Content-Type': 'application/x-www-form-urlencoded',\n      },\n      json: true\n    })\n\n    const { access_token } = payload\n\n    // validate initialisation vector\n    const sessionInitialisationVector = request.yar.get('initialisationVector')\n    const { nonce } = JSON.parse(Buffer.from(access_token.split('.')[1], 'base64').toString())\n\n    if (nonce !== sessionInitialisationVector) {\n      throw new Error('Invalid Initialisation Vector, possible token replay attack')\n    }\n\n    // store token in session cookie and redirect user to root\n    return h.redirect('/').state('auth_cookie', access_token, {\n      ttl: null // session cookie\n      encoding: 'none', // JWT is already encoded\n      isSameSite: 'Lax',\n      isSecure: true, // for local development, set to false\n      isHttpOnly: true,\n      clearInvalid: false,\n      strictHeader: true,\n      path: '/' // ensure cookie is available on all routes\n    })\n  }\n})\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#return-user-to-specific-location-after-authentication","title":"Return user to specific location after authentication","text":"<p>The <code>state</code> can also be used to store additional information such as where to redirect the user to once they have authenticated.  This strategy avoids users ending up in an unexpected fixed location after authentication after moving between services, or clicking links to specific locations.</p> <p>If using Hapi.js, the user's intended destination can be taken from the <code>request.url.pathname</code> property and stored in the <code>state</code> parameter.  Once authentication is complete, the user can be redirected back to the intended destination.</p>"},{"location":"development-patterns/authentication/defra-identity/#example_1","title":"Example","text":"<p>User tries to access <code>/my-account</code> but is not authenticated.  The service catches a <code>401</code> error and redirects the user to <code>/sign-in</code> with a query parameter taken from their intended destination.</p> <pre><code>// plugin to handle 401 errors\nserver.ext('onPreResponse', (request, h) =&gt; {\n  if (request.response.isBoom &amp;&amp; request.response.output.statusCode === 401) {\n    return h.redirect(`/sign-in?redirect=${request.url.pathname}`)\n  }\n  return h.continue\n})\n</code></pre> <pre><code>// sign-in route\nserver.route({\n  method: 'GET',\n  path: '/sign-in',\n  handler: async (request, h) =&gt; {\n    const state = Buffer.from(JSON.stringify({\n      id: uuidv4(), // Unique identifier for the request\n      redirect: request.query.redirect, // Intended destination\n    })).toString('base64')\n\n    request.yar.set('state', state)\n\n    return h.redirect(`https://${defraIdentity}?p=${policy}&amp;client_id=${clientId}&amp;service_id=${serviceId}&amp;state=${state}&amp;redirect_uri=${redirectUri}&amp;scope=openid%20offline_access&amp;response_type=code&amp;response_mode=query`)\n  }\n})\n</code></pre> <pre><code>// redirect route from Defra Identity\nserver.route({\n  method: 'GET',\n  path: '/sign-in-oidc',\n  handler: async (request, h) =&gt; {\n    const { code, state } = request.query\n\n    // validation of state and token exchange omitted\n\n    const { redirect } = JSON.parse(Buffer.from(state, 'base64').toString())\n\n    return h.redirect(redirect) // setting of cookie omitted\n  }\n})\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#set-up-authentication-strategy","title":"Set up authentication strategy","text":"<p>A Hapi.js plugin can be used to setup an appropriate authentication strategy.  This plugin should be registered with the server and can be used to protect routes that require authentication.</p> <p>For example, if storing the JWT in a session cookie, the plugin could utilise the <code>@hapi/cookie</code> or <code>hapi-auth-jwt2</code> plugins.</p> <p>The below example uses the <code>hapi-auth-jwt2</code> plugin to validate the JWT token.</p> <pre><code>await server.register(require('hapi-auth-jwt2'))\nawait server.register({\n  plugin: {\n    name: 'auth',\n    register: async (server, options) =&gt; {\n      server.auth.strategy('jwt', 'jwt', {\n        key: async () =&gt; {\n          const { payload } = await Wreck.get(jwks_uri, {\n            json: true\n          }) // get public key from Defra Identity, `jwks_uri` is part of well known URL\n\n          const { keys } = payload\n          const pem = jwkToPem(keys[0]) // convert to pem format\n          return { key: pem }\n        },\n        validate: async (decoded, request, h) =&gt; {\n          // validate the decoded token\n          return { isValid: true, credentials: { name: `${decoded.firstName} ${decoded.lastName}` } }\n        },\n        verifyOptions: {\n          algorithms: ['RS256']\n        }\n      })\n\n      server.auth.default({ strategy: 'jwt', mode: 'try' }) // try will attempt to authenticate but not fail if not authenticated.  This allows all routes to have access to the credentials object\n    }\n  }\n})\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#project-routes","title":"Project routes","text":"<p>If a <code>try</code> authentication strategy is used then all routes will attempt to authenticate the user but not fail if not authenticated.  This allows the service to determine if the user is authenticated and act accordingly.</p> <p>If a route requires authentication, then mode can be set to <code>required</code>.  A <code>401</code> error will be thrown if the user is not authenticated.  Catching a <code>401</code> error and redirecting the user to the sign in page is a common pattern.</p> <pre><code>// route that requires authentication\nserver.route({\n  method: 'GET',\n  path: '/my-account',\n  options: {\n    auth: {\n      strategy: 'jwt',\n      mode: 'required'\n    }\n  },\n  handler: async (request, h) =&gt; {\n    return h.response('My account')\n  }\n})\n</code></pre> <p>If no authentication is required, then authentication can be set to false.  For example, the sign in route should be accessible at all times.</p> <pre><code>// route that does not require authentication\nserver.route({\n  method: 'GET',\n  path: '/public',\n  options: {\n    auth: false\n  },\n  handler: async (request, h) =&gt; {\n    return h.response('Public')\n  }\n})\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#role-based-authentication","title":"Role based authentication","text":"<p>Typically as well as ensuring the user is authenticated, services will also want to ensure the user has the correct permissions to access a route.  For services using Government Gateway, the JWT token will contain the user's permissions for the selected organisation.</p> <p>However, for services using Rural Payments, the JWT token will not contain the user's permissions.  Instead, the service will need to retrieve the permissions from Siti Agri via an API call.</p>"},{"location":"development-patterns/authentication/defra-identity/#azure-api-management","title":"Azure API Management","text":"<p>Unlike FCP services that are hosted in Azure, Siti Agri is deployed to Crown Hosting.  Connectivity from Azure to Crown Hosting is only possible through an Azure API Management (APIM) service</p> <p>An endpoint has been made available through APIM to access this API from FCP services running in Azure Kubernetes Service.</p> <p><code>https://&lt;ENVIRONMENT&gt;/rural-payments-vet-visits/v1/SitiAgriApi/authorisation/organisation/&lt;ORGANISATION_ID&gt;/authorisation</code></p> <p>The endpoint requires the following headers:</p> <ul> <li><code>crn</code> - the user's Customer Reference Number</li> <li><code>X-Forwarded-Authorization</code> - the JWT token</li> <li><code>Authorization</code> - a <code>Bearer</code> token from APIM</li> <li><code>Ocp-Apim-Subscription-Key</code> - the subscription key for the APIM service</li> </ul> <p>Before the endpoint can be called, the service must first authenticate with APIM.  This is done by calling the following endpoint:</p> <p><code>https://login.microsoftonline.com/&lt;TENANT&gt;/oauth2/v2.0/token</code></p> <p>This endpoint requires a <code>form-data</code> type POST request with query parameters.</p> <p>The following parameters are common to all FCP services and can be found in Azure Key Vault</p> <ul> <li>clientId</li> <li>clientSecret</li> <li>scope</li> </ul> <p>Example using <code>@hapi/wreck</code>:</p> <pre><code>const Wreck = require('@hapi/wreck')\nconst FormData = require('form-data')\n\n  const data = new FormData()\n  data.append('client_id', `${clientId}`)\n  data.append('client_secret', `${clientSecret}`)\n  data.append('scope', `${scope}`)\n  data.append('grant_type', 'client_credentials')\n\n  return Wreck.post(apimConfig.authorizationUrl, {\n    headers: data.getHeaders(),\n    payload: data,\n    json: true\n  })\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#parsing-siti-agri-response","title":"Parsing Siti Agri response","text":"<p>Siti Agri will return a response containing all roles and permissions for all users associated with the business.  The service must parse this response to determine if the user has the correct permissions.</p> <p>Example response:</p> <pre><code>{\n  \"data\": {\n    \"personRoles\": [{\n      \"personId\": 1000001,\n      \"role\": \"Farmer\"\n    }, {\n      \"personId\": 1000002,\n      \"role\": \"Agent\"\n    }],\n    \"personPrivileges\": [{\n      \"personId\": 1000001,\n      \"privilege\": \"View\"\n    }, {\n      \"personId\": 1000001,\n      \"privilege\": \"Edit\"\n    }, {\n      \"personId\": 1000002,\n      \"privilege\": \"Edit\"\n    }]\n  }\n}\n</code></pre> <p>A complexity of this is that all permissions are returned, not just for the logged in user.  The service must filter the response based on <code>personId</code>.  <code>personId</code> is not included in the JWT token and must be retrieved from a separate APIM endpoint.</p> <p><code>https://&lt;ENVIRONMENT&gt;/rural-payments-vet-visits/v1/person/3337243/summary</code></p> <p>Note: <code>3337243</code> is a fixed value and does not need to be changed.</p> <p>The endpoint requires the following headers:</p> <ul> <li><code>crn</code> - the user's Customer Reference Number</li> <li><code>X-Forwarded-Authorization</code> - the JWT token</li> <li><code>Authorization</code> - a <code>Bearer</code> token from APIM</li> <li><code>Ocp-Apim-Subscription-Key</code> - the subscription key for the APIM service</li> </ul> <p>Example flow:</p> <ol> <li>User logs in with their Rural Payments account</li> <li>User selects an organisation</li> <li>Service retrieves organisation Id from <code>currentRelationshipId</code> property in JWT token</li> <li>Service retrieves <code>personId</code> via APIM</li> <li>Service retrieves permissions from Siti Agri via APIM</li> <li>Service filters permissions based on <code>personId</code></li> <li>Service persists permissions in session</li> </ol>"},{"location":"development-patterns/authentication/defra-identity/#updating-credentials","title":"Updating credentials","text":"<p>The roles should be stored in the <code>scope</code> property of <code>credentials</code> during authentication.  As the JWT does not include the roles, they must be added to the <code>credentials</code> object during each request.</p> <pre><code>validate: async (decoded, request, h) =&gt; {\n  // get roles already retrieved from Siti Agri from session\n  const roles = request.yar.get('roles') // must be array\n  return { isValid: true, credentials: { name: `${decoded.firstName} ${decoded.lastName}`, scope: roles } }\n},\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#protecting-routes","title":"Protecting routes","text":"<p>Once a role has been mapped to a service specific role, the service can protect routes by using the <code>scope</code> property.</p> <p>If the user does not have the correct role, a <code>403</code> error will be thrown and the user will not be able to access the route.  Services can catch this error and redirect the user to an appropriate page.</p> <pre><code>// route that requires authentication and a specific role\nserver.route({\n  method: 'GET',\n  path: '/my-account',\n  options: {\n    auth: {\n      strategy: 'jwt',\n      scope: ['View'] // role required to access route\n    }\n  },\n  handler: async (request, h) =&gt; {\n    return h.response('My account')\n  }\n})\n</code></pre>"},{"location":"development-patterns/authentication/defra-identity/#refresh-tokens","title":"Refresh tokens","text":"<p>Defra Identity will also return a Refresh Token when the user authenticates.  This token can be used to obtain a new Access Token when the current Access Token expires.</p> <p>The Refresh Token should also be stored in session storage, however due to it's size, it's recommended to store it in a session store (e.g. Redis).</p> <p>Libraries such as <code>hapi-auth-jwt2</code> and <code>@hapi/cookie</code> do not support Refresh Tokens out of the box.  The service will need to implement this functionality manually.  By default, both of these libraries will fail validation if the token is expired.  This behaviour can be disabled if the service wishes to refresh the token before or after it expires.</p>"},{"location":"development-patterns/authentication/defra-identity/#sign-out","title":"Sign out","text":"<p>When a user signs out, the service should redirect the user to the <code>end_session_endpoint</code>.  This will clear the session cookie on the Defra Identity domain and the user will be signed out.</p> <p>Similar to the login page, the user will be redirected back to the service after sign out.  The service should clear the session cookie and any other session data at this point.</p>"},{"location":"development-patterns/authentication/defra-identity/#cross-service-user-journeys","title":"Cross service user journeys","text":"<p>As FCP grows in scale, it is expected that users will need to move between services within the same session.  We do not want users to have to re-authenticate or re-select an organisation when moving between services.</p> <p>Services should therefore be designed so that an <code>organisationId</code> can be passed to any route as a query string.  The service will then pass this to Defra Identity as the <code>relationshipId</code> parameter during the first login process which will mean the picker page is skipped.</p> <p>If the user has an active session in Defra Identity, the user will automatically not be asked for credentials.</p>"},{"location":"development-patterns/authentication/entra/","title":"Microsoft Entra","text":"<p>Microsoft Entra is a cloud-based identity and access management service that helps organizations manage user identities and control access to resources. It provides features such as single sign-on (SSO), multi-factor authentication (MFA), and conditional access policies to enhance security and streamline user access to applications and services.</p> <p>FCP services should use Microsoft Entra for authentication and authorization of internal users.</p>"},{"location":"development-patterns/authentication/entra/#example","title":"Example","text":"<p>An example repository has been created to demonstrate how to implement Microsoft Entra in an FCP service.  The repository can be found in GitHub</p>"},{"location":"development-patterns/authentication/entra/#app-registration","title":"App Registration","text":"<p>An Azure App Registration is created to represent the application in Microsoft Entra. This registration includes information about the application, such as its name, redirect URIs, and permissions.</p>"},{"location":"development-patterns/authentication/entra/#naming-convention","title":"Naming convention","text":"<p>App Registrations should follow a naming convention that is consistent and descriptive and should include the environment.</p> <p>For example:</p> <ul> <li><code>FCP-Payments-Dev</code></li> <li><code>FCP-Payments-Prod</code></li> </ul>"},{"location":"development-patterns/authentication/entra/#permissions","title":"Permissions","text":"<p>Permissions can be added to the App Registration and subsequently users can be placed in those roles.  This allows for a more granular control of access to the application.</p> <p>Permissions should follow the principle of least privilege, meaning that users should only be granted the permissions necessary to perform their job functions.</p>"},{"location":"development-patterns/authentication/entra/#naming-convention_1","title":"Naming convention","text":"<p>Permissions should follow a naming convention that is consistent and descriptive and follow a dot notation format.</p> <p>For example:</p> <ul> <li><code>FCP.Payments.View</code></li> <li><code>FCP.Payments.Create</code></li> </ul>"},{"location":"development-patterns/authentication/entra/#credentials","title":"Credentials","text":"<p>Internal users will authenticate using their organisation Microsoft account.  No new credentials need to be created specifically for FCP services.</p> <p>For those without a Defra Microsoft account, a <code>@defra.onmicrosoft.com</code> account can be used.</p>"},{"location":"development-patterns/authentication/entra/#tenants","title":"Tenants","text":"<p>Defra has two Microsoft Entra tenants that require the App Registration to be created in.</p>"},{"location":"development-patterns/authentication/entra/#o365_defradev","title":"O365_DEFRADEV","text":"<p>The <code>O365_DEFRADEV</code> tenant is used for proof of concept work and is a pre-requisite environment for Production use.</p> <p>Teams must prove their setup in the <code>O365_DEFRADEV</code> tenant.</p> <p>Teams can request a new App Registration including permission setup for this tenant through a ServiceNow ticket.</p>"},{"location":"development-patterns/authentication/entra/#defra","title":"Defra","text":"<p>Once the proof of concept work has been completed, the App Registration can be created in the <code>Defra</code> tenant.</p> <p>Any changes to the <code>Defra</code> tenant require a change request.</p> <p>A series of standard changes exist to support the creation of the App Registration as well as some of the common subsequent tasks.</p> <p>Teams can decide whether to use these standard changes or to raise a bespoke normal change to cover all activities.</p> <p>The following ServiceNow standard change for a Type 1 registration can be used to create the App Registration.</p> <p>It is recommended to create at least two App Registrations in the <code>Defra</code> tenant, one for Production and one for Non-Production.  </p> <p>This allows users to be placed in roles to support lower environment testing without risking access to Production data.</p>"},{"location":"development-patterns/authentication/entra/#permissions_1","title":"Permissions","text":"<p>Once the App Registration has been created, the permissions can be added to the App Registration through a further standard change</p> <p>Users can be added to the roles through a further standard change</p> <p>Project teams typically support the management of their own permissions initially, but longer term, access requests will be managed through the business through ServiceNow.</p>"},{"location":"development-patterns/authentication/entra/#app-registration-approval","title":"App registration approval","text":"<p>Once the App Registration has been created, it must be approved by CCoE before it can be used in Production.</p> <p>Until it is services will receive the following message when attempting to use the App Registration.</p> <p></p> <p>There does not seem to be a standard change for this approval process, so teams should engage with CCoE to discuss how to request this approval.</p>"},{"location":"development-patterns/authentication/entra/#fcp-platform","title":"FCP Platform","text":"<p>For services deployed to the FCP Platform, instead of creating the new App Registration in the <code>Defra</code> tenant, the App Registration should be created in the following tenants for each environment:</p> <ul> <li><code>DefraCloudDev</code> for the <code>Development</code> and <code>Test</code> FCP Platform environments</li> <li><code>DefraCloudPreProd</code> for the <code>Pre-Production</code> FCP Platform environment</li> <li><code>DefraCloud</code> for the <code>Production</code> FCP Platform environment</li> </ul> <p>Due to the increased number of tenants utilised by the FCP Platform, there is no need to create multiple App Registrations in a single tenant as Production is already isolated.</p>"},{"location":"development-patterns/authentication/entra/#configuration-rotation","title":"Configuration rotation","text":"<p>App Registration credentials expire after a set period of time.  This is typically 1 or 2 years.</p> <p>These credentials should be rotated before they expire and the application reconfigured to use the new credentials.</p>"},{"location":"getting-started/arrange-access/","title":"Arrange access","text":"<p>Developers will need access to several resources and communication channels in order to be productive.</p> <p>Access to some resources can take time to arrange, so it's important to start this process as soon as possible.</p>"},{"location":"getting-started/arrange-access/#microsoft-teams","title":"Microsoft Teams","text":"<p>Microsoft Teams is Defra's primary collaboration tool.  All meetings should be arranged in Teams.</p> <p>External suppliers can join Defra Teams meetings with their own email address if they do not have a Defra account.</p>"},{"location":"getting-started/arrange-access/#slack","title":"Slack","text":"<p>Slack is used for collaboration and support across all of Defra's digital services.</p>"},{"location":"getting-started/arrange-access/#defra-digitalslackcom","title":"<code>defra-digital.slack.com</code>","text":"<p>This is Defra's primary Slack workspace for digital delivery.</p> <p>Whilst not essential to be part of this workspace, it may be beneficial for cross Defra collaboration.</p> <p>Anyone with a <code>defra.gov.uk</code> or an ALB email address can join the Defra Digital Slack workspace automatically.</p> <p>For those with other email addresses, a request should be raised in the <code>#slack-support</code> channel by someone with access.</p> <p>Developers are recommended to join the following channels:</p>"},{"location":"getting-started/arrange-access/#development","title":"<code>#development</code>","text":"<p>General development discussion and support across all of Defra's digital services.</p>"},{"location":"getting-started/arrange-access/#ask-principal-developers","title":"<code>#ask-principal-developers</code>","text":"<p>Ask for advice from Defra's Principal Developers.</p>"},{"location":"getting-started/arrange-access/#github-support","title":"<code>#github-support</code>","text":"<p>For support with GitHub including managing permissions.</p>"},{"location":"getting-started/arrange-access/#sonar-support","title":"<code>#sonar-support</code>","text":"<p>For support with SonarCloud including managing permissions.</p>"},{"location":"getting-started/arrange-access/#defra-digital-teamslackcom","title":"<code>defra-digital-team.slack.com</code>","text":"<p>This secondary Defra Digital Slack workspace is used for collaboration and support across all of Defra's digital services.</p> <p>This is a paid instance so has additional collaboration features.</p> <p>Developers are recommended to join the following channels:</p>"},{"location":"getting-started/arrange-access/#fcp-platform-support","title":"<code>#fcp-platform-support</code>","text":"<p>Support and collaboration across all services using the FCP Platform.  It is also where the FCP Platform team will post updates and announcements.</p>"},{"location":"getting-started/arrange-access/#cdp-support","title":"<code>#cdp-support</code>","text":"<p>Support and collaboration across all services using the Core Data Platform.  It is also where the CDP team will post updates and announcements.</p>"},{"location":"getting-started/arrange-access/#software-development","title":"<code>#software-development</code>","text":"<p>General development discussion and support across all of Defra's digital services.</p>"},{"location":"getting-started/arrange-access/#azure-ad-administrative-account","title":"Azure AD administrative account","text":"<p>Access to Azure and AWS cloud environments are only accessible with an administrative account setup in Microsoft Entra by CCoE.</p> <p>These accounts follow the naming convention <code>a-&lt;initials&gt;.&lt;surname&gt;@defra.onmicrosoft.com</code> or <code>&lt;initials&gt;.&lt;surname&gt;@defra.onmicrosoft.com</code> and are a requirement for both FCP Platform and CDP.</p> <p>The must be requested from ServiceNow under the catalogue item <code>Cloud Accounts</code> (subheading <code>M365/AzureAD Platform Admin - Cloud Account service request</code>)</p> <p>The comments should make clear that the request is for a <code>a-</code> Microsoft administrative account for Azure Portal.</p> <p>Accounts without the <code>a-</code> prefix can be created but cannot be used for access to Production resources. </p>"},{"location":"getting-started/arrange-access/#openvpn","title":"OpenVPN","text":"<p>Access to cloud resources from non-Defra supplied devices is restricted to VPN access only and requires the installation of the OpenVPN client.</p> <p>If using FCP Platform, a request should be raised in ServiceNow for CCoE to provide access to OpenVPN and all FCP networks.</p> <p>If using CDP, then the request should be made in the <code>#cdp-support</code> channel.</p>"},{"location":"getting-started/arrange-access/#access-to-the-azure-portal","title":"Access to the Azure Portal","text":"<p>Once the account has been created, the user will be able to access the Azure Portal.</p> <p>If using the FCP Platform, a request should be raised in ServiceNow for CCoE to provide access to the FCP Azure subscriptions.</p> <p>FCP environments are split across three Azure tenants, <code>DefraCloudDev</code>, <code>DefraCloudPreProd</code> and <code>DefraCloudProd</code>.</p> <p>If a developer has Security Clearance, they can request access to the <code>DefraCloudPreProd</code> and <code>DefraCloudProd</code> tenants.  Otherwise they will only be able to access the Development tenant.  The request should make clear which environments the developer requires access to.  </p> <p>Evidence of Security Clearance will be requested by CCoE prior to completing the request</p> <p>If using CDP, then no direct access to the AWS portal is required.  Instead, the CDP portal should be used to manage resources.</p>"},{"location":"getting-started/arrange-access/#access-to-azure-kubernetes-service-aks","title":"Access to Azure Kubernetes Service (AKS)","text":"<p>As AKS is the primary compute hosting service for the FCP Platform, developers using it will need to be added to the appropriate Kubernetes Cluster role.  This will allow access to Kubernetes using client tools such as <code>kubectl</code>.</p> <p>As with the Azure Portal, developers can only be added to the PreProduction and Production clusters if they have Security Clearance.</p> <p>A read only role, is all that is permitted for developers in PreProduction and Production.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to the FCP AKS clusters.</p>"},{"location":"getting-started/arrange-access/#jenkins","title":"Jenkins","text":"<p>Jenkins is used for Continuous Integration pipelines on the FCP Platform.  Developers will need a <code>@dtz.local</code> account to access Jenkins.</p> <p>Jenkins is hosted on an Azure Virtual Machine and can be accessed on the Defra network or via OpenVPN.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to Jenkins.</p>"},{"location":"getting-started/arrange-access/#snyk","title":"Snyk","text":"<p>Snyk is used for security scanning of code dependencies.  </p> <p>An FCP organisation has been created in Snyk and developers will need to be added to it.</p> <p>Contact the Platform team via the <code>#fcp-platform-support</code> channel to arrange access.</p> <p>CDP CI pipeline does not include Snyk integration by default, but teams can extend their GitHub actions to integrate with the FCP Snyk organisation.</p>"},{"location":"getting-started/arrange-access/#github","title":"GitHub","text":"<p>GitHub is Defra's strategic source control tool and all code should be hosted in the Defra organisation.</p> <p>As per GDS standards, code is open source by default.</p> <p>Developers will need to be added to the <code>FFC</code> GitHub team.</p> <p>Two Factor Authentication (2FA) and commit signing are mandatory for all GitHub accounts.</p> <p>Contact one of Defra's GitHub admins via the <code>#github-support</code> Slack channel to arrange access.</p>"},{"location":"getting-started/arrange-access/#sonarcloud","title":"SonarCloud","text":"<p>SonarCloud is used for static code analysis during Continuous Integration pipelines in FCP.</p> <p>FCP projects are hosted in the SonarCloud Defra organisation. Within the Defra organisation an FCP group has been created to manage access to all FCP projects.</p> <p>In order to be added to the FCP group:</p> <ol> <li>Sign up to SonarCloud</li> <li>Contact one of the Sonar admins via the <code>#sonar-support</code> Slack channel and request to be added to the <code>Farming and Countryside Programme</code> group, providing your email address used to sign in.</li> </ol> <p>Note: you must have signed into SonarCloud at least once before you can be added to a group.</p>"},{"location":"getting-started/arrange-access/#azure-devops-ado","title":"Azure DevOps (ADO)","text":"<p>Azure DevOps is used for Continuous Delivery pipelines in FCP.  ADO is also used for work item tracking and Wiki creation.</p> <p>Developers will need to be added to the DEFRA-FFC project.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to ADO.</p>"},{"location":"getting-started/arrange-access/#sharepoint","title":"SharePoint","text":"<p>SharePoint is Defra's primary document library.</p> <p>An FCP SharePoint site has been created.</p> <p>Not all developers will need access to SharePoint and it may not be possible for external suppliers to be provided with access.</p> <p>Should access be required, it should be request to an FCP Programme Delivery Manager.</p>"},{"location":"getting-started/arrange-access/#confluence","title":"Confluence","text":"<p>Significant content exists in Defra Confluence.</p> <p>New licenses are often challenging to obtain, but should be requested through the <code>defra-digital.slack.com</code> Slack workspace in the <code>#jira-support</code> channel.</p>"},{"location":"getting-started/arrange-access/#cdp-portal","title":"CDP Portal","text":"<p>CDP has it's own onboarding process that teams should follow.</p>"},{"location":"getting-started/cicd/","title":"Continuous Integration and Continuous Delivery (CI/CD)","text":"<p>The expectation is that teams will regularly deploy, small, low risk, zero downtime changes to production.  </p> <p>Many teams in FCP are deploying to production multiple times a day.</p>"},{"location":"getting-started/cicd/#cdp","title":"CDP","text":"<p>When using CDP, the platform will scaffold CI/CD capability for new services.</p> <p>For CI, a GitHub actions workflow will be created to build and test the service.  Teams can extend these pipelines as required.</p> <p>Deployments are managed through the CDP portal.</p> <p>Full guidance is included in the CDP documentation as opposed to these pages.</p>"},{"location":"getting-started/cicd/#fcp-platform","title":"FCP Platform","text":"<p>For the FCP Platform, Jenkins is used for CI pipelines, whilst Azure DevOps is used for CD.</p> <p>Originally the FCP Platform was primarily hosted on AWS where Jenkins was prescribed as the CI/CD tool.  However, as the platform has migrated to Azure, Azure DevOps was prescribed as the CD tool.  Jenkins is still used for CI pipelines as it is a well established tool within the programme and redevelopment of the pipelines in Azure DevOps would be a significant undertaking, however this remains an ambition of the FCP Platform team.</p>"},{"location":"getting-started/cicd/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>FCP adopt a \"shift left\" approach to security and quality, meaning that security and quality checks are performed as early as possible in the development lifecycle.  The CI pipeline is designed around this principle and looks to provide fast feedback to developers and testers prior to merging code to the main branch.</p> <p>Feature branch development allows for small changes to be made and tested in isolation.  This is supported by the CI pipeline which will run a subset of tests and checks on a feature branch build.  The pipeline will also provision a dedicated environment for the feature branch build to be deployed to in <code>Sandpit 2</code>, allowing for a more comprehensive set of tests to be run.</p> <p>In order to avoid duplication across all microservices, a common Jenkins pipeline has been created.</p> <p>Note that a feature branch must have an open Pull Request in order for the CI pipeline to run.</p>"},{"location":"getting-started/cicd/#supported-technologies","title":"Supported technologies","text":"<p>The CI pipeline supports the following technologies:</p> <ul> <li>Containerised Node.js applications targetting Azure Kubernetes Service (AKS)</li> <li>Containerised .NET applications targetting Azure Kubernetes Service (AKS)</li> <li>Helm charts</li> <li>Azure Functions</li> <li>Docker images</li> <li>npm packages</li> </ul>"},{"location":"getting-started/cicd/#pipeline-stages","title":"Pipeline stages","text":""},{"location":"getting-started/cicd/#build-steps","title":"Build steps","text":"<p>The steps for a feature or main branch build are more or less the same.  The most significant difference is there is no feature branch deployment for a main build.  Instead additional build assets are created ready to be promoted to higher environments.</p> <p>Below shows the pipeline steps for building a Node.js application targetting AKS.  The steps for other technologies are similar.  Anywhere <code>CUSTOM INJECTION</code> is shown custom steps can be injected.</p> <p>Some steps are optional depending on the content of the repository.</p> <ol> <li>Validate semantic version of package</li> <li>Lint Helm chart</li> <li>npm Audit vulnerability check</li> <li>Snyk vulnerability check</li> <li><code>CUSTOM INJECTION</code></li> <li>Build test Docker image</li> <li>Provision dynamic infrastructure for feature branch and to support integration tests</li> <li><code>CUSTOM INJECTION</code></li> <li>Run containerised tests (linting, unit, integration, contract)  </li> <li><code>CUSTOM INJECTION</code></li> <li>Publish contracts to Pact Broker</li> <li>SonarCloud static code analysis</li> <li>Build production Docker image</li> <li>Publish image to to Azure Container Registry</li> <li>Deploy image to dynamic AKS namespace (feature branch build only)</li> <li>AXE accessibility tests (optional)</li> <li>BrowserStack containerised UI and compatibility tests (optional)</li> <li>Zap security tests (optional)</li> <li>JMeter performance tests (optional)</li> <li>Publish Helm chart to AZure Container Registry (main branch build only)</li> <li><code>CUSTOM INJECTION</code></li> <li>Create GitHub release (main branch build only)</li> <li>Clean up build specific dynamic infrastructure</li> <li><code>CUSTOM INJECTION</code></li> </ol>"},{"location":"getting-started/cicd/#clean-up-steps","title":"Clean up steps","text":"<p>Once a feature branch is deleted, the clean up process will automatically run to remove any dynamic infrastructure.</p> <p>Teams should delete merged and defunct feature branches as soon as possible to avoid unnecessary resource usage and costs.</p>"},{"location":"getting-started/cicd/#continuous-delivery-cd","title":"Continuous Delivery (CD)","text":"<p>Once a feature branch has been merged into the main branch, the CI pipeline will automatically run to package the assets and then trigger Azure DevOps to deploy the application sequentially through the environments.</p> <p>ADO will deploy the application to the following environments once the previous environment has been successfully deployed to:</p> <ul> <li>Sandpit 2 (automatic)</li> <li>Development (automatic)</li> <li>Test (requires approval from anyone in the team)</li> <li>PreProduction (requires approval from anyone in the team)</li> <li>Production (requires approval from CCoE and an Request for Change (RFC))</li> </ul> <p>Note that some of the more mature teams in FCP have approval to run their own changes to Production using a standard change.</p>"},{"location":"getting-started/platform/","title":"Platform","text":""},{"location":"getting-started/platform/#platform-engineering","title":"Platform engineering","text":"<p>To support rapid, highly assured delivery, FCP follows the principles of Platform Engineering to deliver common capabilities.</p>"},{"location":"getting-started/platform/#fcp-platform","title":"FCP Platform","text":"<p>FCP was the first Defra programme to adopt this approach and the FCP Platform engineering team has delivered common Azure environments, PaaS components, delivery pipelines and supporting tools.</p> <p>These are collectively referred to as the <code>FCP Platform</code>.</p>"},{"location":"getting-started/platform/#core-delivery-platform-cdp","title":"Core Delivery Platform (CDP)","text":"<p>In recent years, the Core Delivery Platform (CDP) has emerged as Defra's strategic platform engineering product.</p> <p>It is the expectation that all new services will be delivered using CDP.</p> <p>The majority of existing services will be migrated to CDP and the FCP Platform will be scaled down.</p>"},{"location":"getting-started/platform/#azure-development-platform-adp","title":"Azure Development Platform (ADP)","text":"<p>The Azure Development Platform (ADP) was developed as an alternative to CDP and is essentially an iteration of the FCP Platform.</p> <p>ADP is to be decommissioned and all projects will be migrated to CDP.</p>"},{"location":"getting-started/platform/#choosing-a-platform","title":"Choosing a platform","text":"<p>All teams should develop new services and components on CDP.</p> <p>Teams currently utilising FCP Platform should assess how to migrate existing capabilities to CDP.</p> <p>No new capabilities should be added to FCP Platform or ADP.</p>"},{"location":"getting-started/platform/#fcp-platform_1","title":"FCP Platform","text":""},{"location":"getting-started/platform/#reference-architecture","title":"Reference architecture","text":""},{"location":"getting-started/platform/#hosting","title":"Hosting","text":"<p>The FCP Platform is hosted on Azure in the North Europe region (Dublin, Ireland).</p> <p>Subscriptions are split across three tenants, <code>DefraCloudDev</code>, <code>DefraCloudPreProd</code> and <code>DefraCloudProd</code>.</p>"},{"location":"getting-started/platform/#environments","title":"Environments","text":""},{"location":"getting-started/platform/#sandpit-1","title":"Sandpit 1","text":"<p>The <code>Sandpit 1</code> environment is used for experimentation and supporting local development.</p> <p>Developers have a high level of access to the Sandpit environment to create and destroy resources as required.</p> <p>Unlike all subsequent environments, provisioning of Azure resources such as Managed Identities, PostgreSQL is not automated.  Teams must create their own resources in this environment using the guides in this repository.</p> <p>Although the majority of patterns between <code>Sandpit 1</code> and higher environments are the same, a key difference is the use of Azure App Configuration for configuration management and the use of Jenkins for any deployments.</p> <p>The reason for this is that originally all environments used Azure App Configuration, however automation of configuration from an Azure repo has replaced this in all other environments.</p> <p>The <code>Sandpit 1</code> environment is in the <code>DefraCloudDev</code> tenant.</p>"},{"location":"getting-started/platform/#sandpit-2","title":"Sandpit 2","text":"<p>The <code>Sandpit 2</code> environment is the first automation only deployment environment for new services following merge to the <code>main</code> branch.  It also hosts dynamic feature branch deployments. Different teams utilise it for many purposes such as demonstrations and testing.</p> <p>It is intended as a replacement for the <code>Sandpit 1</code> environment.</p> <p>Developers have full access to this environment, but cannot directly create or destroy resources.  Instead, they must use the automation provided to deploy their services.</p> <p>The Development environment is in the <code>DefraCloudDev</code> tenant.</p>"},{"location":"getting-started/platform/#development","title":"Development","text":"<p>Like <code>Sandpit 2</code>, the <code>Development</code> is utilised in a flexible way by teams.</p> <p>Developers have full access to this environment, but cannot directly create or destroy resources.  Instead, they must use the automation provided to deploy their services.</p> <p>The Development environment is in the <code>DefraCloudDev</code> tenant.</p>"},{"location":"getting-started/platform/#test","title":"Test","text":"<p>The <code>Test</code> environment is used for more formal testing and assurance activities such as integration testing and user acceptance testing.</p> <p>Note: FCP principles are to apply a \"shift left\" approach to testing where as much testing as possible is done during development prior to merging to <code>main</code>, as well as a principle of deploying frequent small changes to <code>Production</code>. Therefore, it is not expected that changes build up in <code>Test</code> ahead of a large scale deployment to higher environments.  For incomplete features or those awaiting wider scale testing, feature toggles are used to disable new behaviour and not block deployment through to <code>Production</code>.</p> <p>Developers have full access to this environment, but cannot directly create or destroy resources.  Instead, they must use the automation provided to deploy their services.</p> <p>Note: Development and Test share the same subscription and PaaS resources.</p> <p>The Test environment is in the <code>DefraCloudDev</code> tenant.</p>"},{"location":"getting-started/platform/#pre-production","title":"Pre-Production","text":"<p>The <code>Pre-Production</code> environment is used as a staging area prior to deployment to <code>Production</code>.  Whilst some testing activities can be performed here, it is recommended to use <code>Test</code> for most testing activities as access to this environment is more restricted and requires Security Clearance.</p> <p>Developers with Security Clearance can request read only access to this environment.</p> <p>The Pre-Production environment is in the <code>DefraCloudPreProd</code> tenant.</p>"},{"location":"getting-started/platform/#production","title":"Production","text":"<p>The <code>Production</code> environment is the live environment for the service.  It is the only environment that is accessible to the public.</p> <p>Developers with Security Clearance can request read only access to this environment.</p> <p>The Production environment is in the <code>DefraCloudProd</code> tenant.</p>"},{"location":"local-development-setup/","title":"Setup local development environment","text":"<p>Defra Developers and directly appointed contractors are provided with a Defra device to use for development work.</p> <p>Typically, Defra devices are not suitable for software development due to the restrictions placed on them by the default ZScaler profile. </p> <p>To work around this, developers are placed in an \"exceptions\" ZScaler profile which allows more flexibility to install and configure software on their devices.</p> <p>Developers can also request the <code>MakeMeAdmin</code> app through ServiceNow to give them local admin rights on their device.</p> <p>Supplier developers typically have their own devices which are used for development work and should be used in line with any commercial agreement.</p> <p>Regardless of the device origin, all developers are expected to adhere to Defra's guidance on the use of unmanaged devices. </p>"},{"location":"local-development-setup/#operating-systems","title":"Operating systems","text":"<p>The most common operating systems is Windows with Windows Subsystem for Linux (WSL) Ubuntu distro or macOS.</p> <p>Visual Studio Code is the most popular code editor used within Defra.  However, developers are free to use any compatible equivalent if it aides in their productivity.</p> <p>This guide will be targeted towards the above setups. For Windows, all mention of terminal commands should be run in WSL unless specified otherwise.</p>"},{"location":"local-development-setup/#local-development-principles","title":"Local development principles","text":"<ul> <li>teams must not constrain their local development setup to a specific device or operating system to maintain developer mobility and agility</li> <li>local development should maximise the use of emulation and avoid tight coupling to cloud services where possible</li> <li>repositories should include full instructions for anyone to be able to easily run the service locally</li> </ul>"},{"location":"local-development-setup/install-azure-cli/","title":"Install Azure CLI","text":"<p>Azure CLI enables interaction with Azure services from the command line.</p>"},{"location":"local-development-setup/install-azure-cli/#installation","title":"Installation","text":"<p>Follow Microsoft's setup guide</p>"},{"location":"local-development-setup/install-azure-cli/#login-to-azure-tenant","title":"Login to Azure tenant","text":"<pre><code>az login --tenant TENANT.onmicrosoft.com\n</code></pre> <p>Follow the instructions displayed and sign in using your <code>a-</code> Microsoft administration account.</p>"},{"location":"local-development-setup/install-docker-desktop/","title":"Install Docker Desktop","text":"<p>Docker Desktop is a tool to support the building and sharing of containerized applications and microservices. </p> <p>It is the fastest and easiest way to get started with Docker on your local machine and also includes Compose as part of the installation.</p>"},{"location":"local-development-setup/install-docker-desktop/#installation","title":"Installation","text":"<p>If not already installed as part of the Windows Subsystem for Linux (WSL) setup, follow the below steps to install Docker Desktop.</p> <ol> <li>Download the Docker Desktop installer for your operating system.</li> <li>Run the installer and follow the prompts to install Docker Desktop.</li> </ol>"},{"location":"local-development-setup/install-docker-desktop/#wsl","title":"WSL","text":"<p>Installing Docker Desktop will automatically configure WSL to use Docker.  The Docker installation will then be available in both Windows and WSL.</p> <ol> <li>Ensure that Docker Desktop is set to use the WSL2 backend.  This can be done by right clicking the Docker Desktop icon in the system tray and selecting <code>Settings</code>.  From there, select <code>General</code> and ensure <code>Use the WSL 2 based engine</code> is checked.</li> <li>Ensure that Docker Desktop is available to your WSL distro.  This can be done by right clicking the Docker Desktop icon in the system tray and selecting <code>Settings</code>.  From there, select <code>Resources</code>, <code>WSL Integration</code> and ensure that the distro you are using is checked.</li> <li>If using a device that has a proxy server, such as a Defra device, you will need to configure Docker Desktop to work around the proxy to avoid network conflicts.  This can be done by right clicking the Docker Desktop icon in the system tray and selecting <code>Settings</code>.  From there, select <code>Resources</code>, <code>Proxies</code> and ensure that <code>Manual proxy configuration</code> is checked.</li> </ol>"},{"location":"local-development-setup/install-docker-desktop/#licence","title":"Licence","text":"<p>Docker Desktop requires a licence to be used within Defra.  Defra developers should request they are added to the <code>defradigital</code> Docker organisation to ensure they are fully licenced to use Docker Desktop.</p> <p>Supplier developers should acquire a licence through their own organisation.</p>"},{"location":"local-development-setup/install-docker-desktop/#native-installation","title":"Native installation","text":"<p>Docker can be installed within WSL natively without Docker Desktop by following this guide.  No additional proxy setup is required with this method.</p>"},{"location":"local-development-setup/install-dotnet-sdk/","title":"Install .NET SDK","text":"<p>Whilst Node.js is Defra's primary framework, there are use cases where .NET is required.</p> <p>The .NET SDK is a set of libraries and tools that allow developers to create .NET applications and libraries.</p>"},{"location":"local-development-setup/install-dotnet-sdk/#choose-version-or-use-lts","title":"Choose version (or use LTS)","text":"<p>Select the version required then either a package manager or binary for your OS from Microsoft</p> <p>Note On Mac, creation of symbolic links is missing from the installer after installation. To create them manually run</p> <pre><code>ln -s /usr/local/share/dotnet/dotnet /usr/local/bin/\n</code></pre>"},{"location":"local-development-setup/install-dotnet-sdk/#verify-installation","title":"Verify installation","text":"<p>Check your installation was successful with</p> <pre><code>dotnet --version\n</code></pre>"},{"location":"local-development-setup/install-dotnet-sdk/#install-net-tools","title":"Install .NET tools","text":"<p>As per Microsoft's setup guide the Entity Framework tools allow the creation and application of code first migrations can be installed with the command</p> <pre><code>dotnet tool install --global dotnet-ef\n</code></pre>"},{"location":"local-development-setup/install-dotnet-sdk/#vs-code","title":"VS Code","text":"<p>The C# extension for VS Code is required to enable debugging and IntelliSense for .NET projects and should be installed.</p>"},{"location":"local-development-setup/install-github/","title":"Install GitHub CLI","text":"<p>GitHub allows interaction with GitHub via the command line.  Simplifying creation of remote branches and associated Pull Requests.</p> <p>For example, a new local branch can automatically be pushed to GitHub with a draft Pull Request with:</p> <pre><code>gh pr create -d\n</code></pre>"},{"location":"local-development-setup/install-github/#installation","title":"Installation","text":"<p>Follow the appropriate guide for your OS:</p> <ul> <li>Install guide</li> <li>Linux (including WSL) guide</li> </ul>"},{"location":"local-development-setup/install-helm/","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes. Helm Charts are the package definitions which help you install and upgrade Kubernetes applications.</p> <p>Helm 3 was released in November 2019.</p> <p>Until then, Helm 2 had been the default version of Helm. There are significant differences between the two versions, one of the most substantial being the removal of Tiller. This has resulted in a much easier installation process along with improving the management experience.</p> <p>All charts created by FCP are done so using Helm 3.</p>"},{"location":"local-development-setup/install-helm/#installation","title":"Installation","text":"<p>Installation instructions vary based on OS. Refer to the official docs for details.</p>"},{"location":"local-development-setup/install-kubectl/","title":"Install kubectl","text":"<p>kubectl is a command line tool for interacting with Kubernetes clusters.</p>"},{"location":"local-development-setup/install-kubectl/#installation","title":"Installation","text":"<p>Follow the Kubernetes documentation</p> <p>Note For WSL use the 'Install on Linux' instructions</p>"},{"location":"local-development-setup/install-kubectl/#setup-shell-autocompletion-optional","title":"Setup shell autocompletion (optional)","text":"<pre><code>sudo sh -c 'kubectl completion bash &gt; /etc/bash_completion.d/kubectl'\n</code></pre> <p>Notes: 1. assumes you are using <code>bash</code> shell 1. you will need to reload your shell for the change to be picked up</p> <p>Reference</p> <ul> <li>https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion</li> </ul>"},{"location":"local-development-setup/install-kubectl/#add-command-alias-for-kubectl-optional","title":"Add command alias for kubectl (optional)","text":"<p>To add the frequently used alias <code>k</code> for <code>kubectl</code> add the following lines to your <code>.bashrc</code> file (the 2nd line adds autocomplete for the alias):</p> <pre><code>alias k=kubectl\ncomplete -o default -F __start_kubectl k\n</code></pre>"},{"location":"local-development-setup/install-kubectl/#add-further-aliases-for-kubectl-optional","title":"Add further aliases for Kubectl (optional)","text":"<p>Download the following file and save to your home directory.</p> <p>https://github.com/ahmetb/kubectl-aliases/blob/0533366d8e3e3b3987cc1b7b07a7e8fcfb69f93c/.kubectl_aliases</p> <p>Update your <code>.bashrc</code> file with the below to enable autocomplete on all aliases in the file.</p> <pre><code># Kubectl\n[ -f ~/.kubectl_aliases ] &amp;&amp; source ~/.kubectl_aliases\nsource &lt;(kubectl completion bash)\n\nfor a in $(sed '/^alias /!d;s/^alias //;s/=.*$//' ~/.kubectl_aliases); do\n  complete -F _complete_alias \"$a\"\ndone\n</code></pre> <p>For quick switching of Kubernetes contexts and namespaces, it may be beneficial to append the following lines to the <code>kubectl_aliases</code> file.</p> <pre><code>alias kns='kubectl config set-context --current --namespace'\nalias kc='kubectl config use-context'\n</code></pre> <p>Full details are available in this blog post</p>"},{"location":"local-development-setup/install-kubelogin/","title":"Install kubelogin","text":"<p>kubelogin is a kubectl plugin that implements Azure authentication.</p>"},{"location":"local-development-setup/install-kubelogin/#installation","title":"Installation","text":"<p>Follow the kubelogin documentation for your operating system.</p>"},{"location":"local-development-setup/install-neostandard/","title":"Install neostandard","text":"<p>neostandard is Defra's agreed linting tool for JavaScript and should be used on all new JavaScript projects.</p> <p>It should also be added as a <code>devDependency</code> to new Node.js projects and all JavaScript code should be validated against it.</p>"},{"location":"local-development-setup/install-neostandard/#installation","title":"Installation","text":"<p>Run the following command to install neostandard globally:</p> <pre><code>npm install -g neostandard\n</code></pre>"},{"location":"local-development-setup/install-neostandard/#vs-code","title":"VS Code","text":"<p>These settings should be added to your <code>settings.json</code> file to disable the default JavaScript validation and automatically fix linting issues when saving files</p> <pre><code>{\n  \"javascript.validate.enable\": false,\n  \"eslint.format.enable\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.fixAll\": \"explicit\"\n  }\n}\n</code></pre>"},{"location":"local-development-setup/install-neostandard/#standard-js","title":"Standard JS","text":"<p>Standard JS was previously Defra's choice of linter, however this is no longer maintained as it once was.</p> <p>neostandard is the spiritual successor to Standard JS and is maintained by many of the same people.</p> <p>Whilst there is no immediate requirement that teams migrate existing codebases to neostandard, new projects should use neostandard.</p> <p>As many existing FCP projects will be configured to use Standard JS, it may be beneficial to also install the Standard JS CLI and VS Code extension.</p> <pre><code>npm install -g standard\n</code></pre>"},{"location":"local-development-setup/install-neostandard/#vs-code_1","title":"VS Code","text":"<p>VS Code can support both neostandard and Standard JS.  The decision will automatically be based on what is included within the project's <code>package.json</code> file.</p> <p>These settings should be added to your <code>settings.json</code> file to disable the default JavaScript validation and automatically fix linting issues when saving files</p> <pre><code>{\n  \"javascript.validate.enable\": false,\n  \"standard.autoFixOnSave\": true\n}\n</code></pre>"},{"location":"local-development-setup/install-node-version-manager/","title":"Install Node Version Manager","text":"<p>Node Version Manager allows a developer to easily install and switch between multiple Node versions on a single operating system.</p>"},{"location":"local-development-setup/install-node-version-manager/#installation","title":"Installation","text":"<p>Follow the installation guide</p>"},{"location":"local-development-setup/install-openvpn/","title":"Install OpenVPN","text":"<p>Access to cloud resources from devices is restricted to VPN access only.</p>"},{"location":"local-development-setup/install-openvpn/#install-client","title":"Install client","text":"<p>Once CCoE have created an OpenVPN account, install the client from CCoE's hosted location.</p>"},{"location":"local-development-setup/install-snyk/","title":"Install Snyk CLI","text":""},{"location":"local-development-setup/install-snyk/#installation","title":"Installation","text":"<p>Follow the setup guide in the official Snyk documentation.</p>"},{"location":"local-development-setup/install-sonarqube/","title":"Install SonarQube for IDE","text":"<p>SonarQube Cloud is a cloud-based code quality and security analysis platform that helps developers identify and fix issues in their code. It provides a comprehensive set of tools for analyzing code quality, including static code analysis, code coverage, and security vulnerability detection.</p> <p>All Defra projects are required to setup their repositories within the SonarQube Cloud Defra organisation</p> <p>SonarQube for IDE is an IDE extension that identifies code quality issues as you code.</p>"},{"location":"local-development-setup/install-sonarqube/#dependencies","title":"Dependencies","text":"<ul> <li>Java Runtime Environment v17+</li> </ul> <p>With Ubuntu, the open source version of the Java Runtime Environment (JRE) can be installed using the following command.</p> <pre><code>sudo apt-get install openjdk-17-jre\n</code></pre>"},{"location":"local-development-setup/install-sonarqube/#vs-code","title":"VS Code","text":"<ol> <li> <p>install SonarQube IDE extension</p> </li> <li> <p>set location of JRE in VS Code settings.  The below example is the install location of the above command    <code>json    {      \"sonarlint.ls.javaHome\": \"/usr/lib/jvm/java-11-openjdk-amd64\"    }</code></p> </li> </ol> <p>This will give you Sonar code analysis using default quality gates for languages supported by SonarQube.</p>"},{"location":"local-development-setup/install-sonarqube/#connected-mode","title":"Connected mode","text":"<p>Connected mode binds the extension to the actual project in SonarQube Cloud.  This allows SonarQube IDE to use the same rules, quality gates, and exclusions as the SonarQube project.</p> <p>Follow the documentation to connect your SonarQube IDE to the SonarQube Cloud project.</p>"},{"location":"local-development-setup/install-vs-code/","title":"Install Visual Studio Code","text":"<p>Developers are free to use their own IDE/text editor of choice.  However, this guide will assume Visual Studio Code will be used as that is the most commonly used tool in FCP and Defra.</p> <p>If not using VS Code, this guide should still be read to ensure that the equivalent installation steps are applied.</p> <p>Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and Mac. It includes support for debugging, embedded Git control and GitHub, syntax highlighting, intelligent code completion, snippets, and code refactoring.</p>"},{"location":"local-development-setup/install-vs-code/#installation-vs-code","title":"Installation VS Code","text":"<p>Install for your operating system following Microsoft's guidance</p>"},{"location":"local-development-setup/install-vs-code/#configure-autosave","title":"Configure autosave","text":"<p>If you prefer your code to autosave, add the following to settings.json.</p> <pre><code>\"files.autoSave\": \"afterDelay\"\n\"files.autoSaveDelay\": 600\n</code></pre>"},{"location":"local-development-setup/install-vs-code/#indentation","title":"Indentation","text":"<p>VS Code supports multiple indentation settings for each language by updating the <code>settings.json</code> file.</p> <p>The below example sets a default indentation to two spaces, but uses four spaces for C# files as per Microsoft standards.</p> <pre><code>{\n  \"editor.tabSize\": 2,\n  \"[csharp]\": {\n    \"editor.tabSize\": 4,\n  }\n}\n</code></pre>"},{"location":"local-development-setup/install-vs-code/#wsl-configuration","title":"WSL Configuration","text":""},{"location":"local-development-setup/install-vs-code/#install-remote-wsl-extension","title":"Install Remote WSL extension","text":"<p>The Remote - WSL allows VS Code installed in Windows to be used in WSL.</p> <p>This means when code is source controlled in WSL, VS Code can read, debug and interact with it without workarounds.</p> <p>In WSL2 it is recommended to always use the remote WSL approach.  A current directory can be opened in VS Code with <code>code .</code>.</p>"},{"location":"local-development-setup/install-vs-code/#turn-on-signed-commits","title":"Turn on signed commits","text":"<p>To turn on signed commits (see section on signed commits for more information) in the UI make sure the <code>Enable commit signing with GPG or X.509</code> is selected. </p> <p>For the JSON version of the settings use <code>\"git.enableCommitSigning\": true</code></p>"},{"location":"local-development-setup/install-wsl/","title":"Install Windows Subsystem for Linux (WSL)","text":"<p>Windows Subsystem for Linux (WSL) is a feature of Windows that allows you to run a Linux environment on your Windows machine, without the need for a separate virtual machine or dual booting.</p> <p>WSL is designed to provide a seamless and productive experience for developers who want to use both Windows and Linux at the same time.</p> <p>Official Documentation</p>"},{"location":"local-development-setup/install-wsl/#wsl1-or-wsl2","title":"WSL1 or WSL2","text":"<p>WSL2 is the default and built using a full Linux kernal and is optimised for size and performance.  It also solves many of the networking and Docker integration challenges that were present in WSL1.</p> <p>WSL1 requires significantly more manual setup.  The remainder of this guide will assume WSL2 is being used.</p> <p>For further information on the differences between WSL1 and WSL2, see the Microsoft documentation.</p>"},{"location":"local-development-setup/install-wsl/#installation","title":"Installation","text":"<p>Follow the below Microsoft guide to install the distro of your choice. The Ubuntu distro is recommended.</p> <p>Installation Guide</p> <p>Within the guide there is also a recommended setup of a development environment using WSL.</p> <p>This guide should also be followed as it includes setup of Docker Desktop, Git, Windows Terminal and VS Code.</p>"},{"location":"local-development-setup/install-wsl/#quick-links","title":"Quick links","text":"<ul> <li>Setup Docker Desktop</li> <li>Setup Git</li> <li>Setup Visual Studio Code</li> <li>Setup Windows Terminal</li> </ul>"},{"location":"local-development-setup/mac-setup-scripts/","title":"Mac Setup Scripts","text":"<p>Mac users may want to consider the use of Homebrew as their go-to package manager which simplifies the installation of many packages, desktop applications, command line tools etc. by using a single command for the majority of if not all installations.</p> <p>A set of scripts have been created and are free to use by anyone to aid setting up a Mac for the first time. Please refer to the repository for full instructions. </p> <p>These scripts cover typical installations required to develop on the FCP Platform and CDP:</p> <ul> <li>Command Line Tools for Xcode</li> <li>Homebrew</li> <li>kubectl </li> <li>Azure CLI </li> <li>Helm</li> <li>Snyk CLI</li> <li>GitHub CLI</li> <li>Python</li> <li>Pip</li> <li>pre-commit </li> <li>detect-secrets</li> <li>.NET SDK</li> <li>Docker</li> <li>Visual Studio Code</li> </ul> <p>The scripts do not cover neostandard or Node Version Manager (NVM).</p> <p>There is also an optional script which installs a range of mostly desktop applications and other packages that may be useful for day-to-day development.</p>"},{"location":"local-development-setup/setup-macos-command-line-tools/","title":"Setup command line tools","text":"<p>Mac users who prefer to have a more traditional Unix toolkit accessible to them through the Terminal may wish to install the optional Command Line Tools subsection of the Xcode IDE. </p> <p>This is possible installing the entire Xcode package, no developer account is required either.</p>"},{"location":"local-development-setup/setup-macos-command-line-tools/#install-command-line-tools","title":"Install command line tools","text":"<pre><code>xcode-select --install\n</code></pre>"},{"location":"local-development-setup/setup-sudoers/","title":"Add user to sudoers file","text":"<p><code>sudo</code> is a command line programme that allows trusted users to execute commands as root user.</p> <p>Running a <code>sudo</code> command will prompt for a password.  For local development in WSL, developers may prefer to add their WSL user to the <code>sudoers</code> file to avoid the need for a password.</p>"},{"location":"local-development-setup/setup-sudoers/#update-sudoers-file","title":"Update <code>sudoers</code> file","text":"<ol> <li>Open sudoers file with root permission to edit with <code>sudo nano /etc/sudoers</code> </li> <li>Append <code>username  ALL=(ALL) NOPASSWD:ALL</code> to bottom of file replacing <code>username</code> with your WSL username</li> <li>Save and exit the file</li> </ol>"},{"location":"local-development-setup/sign-commits/","title":"Setup commit signing","text":"<p>You can sign commits and tags locally, so other people can verify that your work comes from a trusted source. If a commit or tag has a GPG or S/MIME signature that is cryptographically verifiable, GitHub marks the commit or tag as verified.</p> <p>If a commit or tag has a signature that cannot be verified, GitHub marks the commit or tag as unverified.</p> <p>You can check the verification status of your signed commits or tags on GitHub and view why your commit signatures might be unverified. </p>"},{"location":"local-development-setup/sign-commits/#setup","title":"Setup","text":"<p>Follow GitHub's instructions for setting up and configuring a GPG key.</p> <ol> <li>Creating a new GPG key</li> <li>Adding a new GPG key to your GitHub account</li> <li>Setup Git to use new GPG key</li> </ol>"},{"location":"local-development-setup/sign-commits/#signing-commits-via-vs-code","title":"Signing commits via VS code","text":"<p>To turn on signed commits in the VS Code UI, make sure the <code>Enable commit signing with GPG or X.509</code> is ticked. For the JSON version of the settings use <code>\"git.enableCommitSigning\": true</code></p>"},{"location":"local-development-setup/wsl/","title":"WSL configuration","text":"<p>When using WSL, repositories should be cloned to the Linux filesystem rather than the Windows filesystem. This will result in better performance and compatibility with Linux tools.</p> <p>Should you need to interact with the Windows filesystem from WSL, all Windows drives will automatically be mounted under the <code>/mnt/</code> directory.</p> <p>For example, the <code>C:</code> drive will be accessible on <code>/mnt/c/</code>.</p>"},{"location":"local-development-setup/wsl/#windows-permissions","title":"Windows permissions","text":"<p>In order to allow WSL to change Windows file permissions, the drives need to be mounted with the <code>metadata</code> option.</p> <ol> <li>Create or edit WSL config file by entering the below terminal command.</li> </ol> <pre><code>sudo nano /etc/wsl.conf\n</code></pre> <ol> <li>Add the following content to the file, then save and exit.</li> </ol> <pre><code>[automount]\noptions = \"metadata\"\n</code></pre> <ol> <li>Restart WSL for changes to take effect.</li> </ol>"},{"location":"local-development-setup/wsl/#proxy","title":"Proxy","text":"<p>If using a device that has a proxy server, such as a Defra device, you will need to configure WSL to work around the proxy to avoid network conflicts.</p> <ol> <li> <p>Create or edit a <code>.wslconfig</code> in your Windows profile directory</p> </li> <li> <p>Add the following content to the file, then save and exit.</p> </li> </ol> <pre><code>[wsl2]\nnetworkingMode=mirrored\nautoProxy=false\n</code></pre> <ol> <li>Restart WSL for changes to take effect.</li> </ol>"}]}