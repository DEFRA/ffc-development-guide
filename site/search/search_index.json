{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Farming and Countryside Programme (FCP) Development Guide This is a repository of standards and supporting guidance for all developers working within the Farming and Countryside Programme (FCP). The purpose of the standards are to ensure that delivery supports the Architecture Vision set out by the programme and better enable developer agility and mobility through consistent patterns and practices. The guide also provides help and support for common setup and problem solving. Architecture Vision The FCP Architecture vision is to deliver a modern, cloud native, event driven microservice ecosystem. Containerised Node.js with JavaScript microservices are the primary delivery mechanism for the programme. These are deployed to the Azure Kubernetes service (AKS) and are expected to be highly available, scalable and resilient. For scenarios where Node.js is not appropriate, Defra's secondary framework and language is .NET and C#. Defra standards The standards and guidance contained here is intended to be an FCP context specific layer over Defra's wider development standards. This guide assumes that Developers already understand and follow these core standards . GDS standards The Government Digital Service (GDS) also have a set of standards that all government services must adhere to. These are also assumed to be understood and followed by developers working within FCP. Read the full GDS Service Manual for more information. Getting started The best place to start as a new developer joining FCP is to read the Getting Started guide.","title":"Farming and Countryside Programme (FCP) Development Guide"},{"location":"#farming-and-countryside-programme-fcp-development-guide","text":"This is a repository of standards and supporting guidance for all developers working within the Farming and Countryside Programme (FCP). The purpose of the standards are to ensure that delivery supports the Architecture Vision set out by the programme and better enable developer agility and mobility through consistent patterns and practices. The guide also provides help and support for common setup and problem solving.","title":"Farming and Countryside Programme (FCP) Development Guide"},{"location":"#architecture-vision","text":"The FCP Architecture vision is to deliver a modern, cloud native, event driven microservice ecosystem. Containerised Node.js with JavaScript microservices are the primary delivery mechanism for the programme. These are deployed to the Azure Kubernetes service (AKS) and are expected to be highly available, scalable and resilient. For scenarios where Node.js is not appropriate, Defra's secondary framework and language is .NET and C#.","title":"Architecture Vision"},{"location":"#defra-standards","text":"The standards and guidance contained here is intended to be an FCP context specific layer over Defra's wider development standards. This guide assumes that Developers already understand and follow these core standards .","title":"Defra standards"},{"location":"#gds-standards","text":"The Government Digital Service (GDS) also have a set of standards that all government services must adhere to. These are also assumed to be understood and followed by developers working within FCP. Read the full GDS Service Manual for more information.","title":"GDS standards"},{"location":"#getting-started","text":"The best place to start as a new developer joining FCP is to read the Getting Started guide.","title":"Getting started"},{"location":"getting-started/","text":"Getting started As a new developer joining FCP, this page details a helpful set of steps to get you up and running. Understand the Platform Arrange access Setup your local development environment","title":"Getting started"},{"location":"getting-started/#getting-started","text":"As a new developer joining FCP, this page details a helpful set of steps to get you up and running. Understand the Platform Arrange access Setup your local development environment","title":"Getting started"},{"location":"getting-started/arrange-access/","text":"Arrange access Developers will need access to several resources and communication channels in order to be productive. Access to some resources can take time to arrange, so it's important to start this process as soon as possible. Azure AD administrative account FCP environments are hosted on Azure. Access to the Azure Portal is restricted to those with an Azure AD administrative account. These accounts follow the naming convention a-<initials>.<surname>@defra.onmicrosoft.com . The must be requested from ServiceNow under the catalogue item O365/AzureAD Platform Admin . The comments should make clear that the request is for a a- Microsoft administrative account for Azure Portal. Access to the Azure Portal Once the account has been created, the user will be able to access the Azure Portal. A request should be raised in ServiceNow for CCoE to provide access to the FCP Azure subscriptions. FCP environments are split across three Azure tenants, Development, PreProduction and Production. If a developer has Security Clearance, they can request access to the PreProduction and Production tenants. Otherwise they will only be able to access the Development tenant. The request should make clear which environments the developer requires access to. Evidence of Security Clearance will be requested by CCoE prior to completing the request Access to Azure Kubernetes Service (AKS) As AKS is the primary compute hosting platform for FCP, developers will need to be added to the appropriate Kubernetes Cluster role. This will allow access to Kubernetes using client tools such as kubectl or Lens . As with the Azure Portal, developers can only be added to the PreProduction and Production clusters if they have Security Clearance. A read only role, is all that is permitted for developers in PreProduction and Production. A request should be raised in ServiceNow for CCoE to provide access to the FCP AKS clusters. Jenkins Jenkins is used for Continuous Integration pipelines in FCP. Developers will need a .dtz account to access Jenkins. A request should be raised in ServiceNow for CCoE to provide access to Jenkins. Synk Synk is used for security scanning of code dependencies. An FCP organisation has been created in Synk and developers will need to be added to it. Contact one of Defra's Principal Developers to arrange access. GitHub FCP uses GitHub for source control within the Defra organisation. As per GDS standards, code is open source by default. Developers will need to be added to the FFC GitHub team. Two Factor Authentication (2FA) and commit signing are mandatory for all GitHub accounts. Contact one of Defra's Principal Developers to arrange access. SonarCloud SonarCloud is used for static code analysis during Continuous Integration pipelines in FCP. A Defra organisation has been created and developers will need to be added to it using their GitHub account. Contact one of Defra's Principal Developers to arrange access. Azure DevOps (ADO) Azure DevOps is used for Continuous Delivery pipelines in FCP. ADO is also used for work item tracking and Wiki creation. Developers will need to be added to the DEFRA-FFC project. A request should be raised in ServiceNow for CCoE to provide access to ADO. Microsoft Teams Microsoft Teams is the primary communication tool used across Defra. Teams supports access from third party teams tenants to allow collaboration across corporate boundaries. Developers should be added to the following Teams chat channels: DEFRA FCP - Platform Support - Teams Chat This chat is used for general support and collaboration across all services using the FCP Platform. FCP Payments, Documents, Demo and Progressive Reductions releases This chat is used to arrange and coordinate releases across the majority of FCP services. As other services onboard to the FCP Platform, they can also use this chat. FCP Developer Forum This chat is used for developer collaboration and support across all teams within the FCP Programme and those using the FCP Platform. Contact one of Defra's Principal Developers to arrange access. Slack ffc-notifications.slack.com Jenkins will publish alerts to Slack in the ffc-notifications.slack.com workspace. Developers will need to be added to the following channels: #masterbuildfailures Notifications of failures relating to main branches. #generalbuildfailures Notifications of failures relating to feature branches. #secretdetection Notifications of detected secrets found in public GitHub repositories for review. Contact one of Defra's Principal Developers to arrange access. defra-digital.slack.com The wider Defra Digital Slack workspace is used for collaboration and support across all of Defra's digital services. Whilst not essential to be part of this workspace, it may be beneficial for cross Defra collaboration. Developers are recommended to join the following channel: #development This channel is used for general development discussion and support across all of Defra's digital services. For FCP Platform or service specific support, the above Teams chats should be used instead. Anyone with a defra.gov.uk or an ALB email address can join the Defra Digital Slack workspace automatically. For those with other email addresses, a request should be raised in the #slack-support channel by someone with access. SharePoint SharePoint is Defra's primary document library. An FCP SharePoint site has been created. Not all developers will need access to SharePoint and it may not be possible for external suppliers to be provided with access. Should access be required, it should be request to an FCP Programme Delivery Manager. Confluence Whilst ADO Wiki's are preferred, much historic content exists in Defra Confluence . New licenses are often challenging to obtain, but should be requested through the defra-digital.slack.com Slack workspace in the #jira-support channel.","title":"Arrange access"},{"location":"getting-started/arrange-access/#arrange-access","text":"Developers will need access to several resources and communication channels in order to be productive. Access to some resources can take time to arrange, so it's important to start this process as soon as possible.","title":"Arrange access"},{"location":"getting-started/arrange-access/#azure-ad-administrative-account","text":"FCP environments are hosted on Azure. Access to the Azure Portal is restricted to those with an Azure AD administrative account. These accounts follow the naming convention a-<initials>.<surname>@defra.onmicrosoft.com . The must be requested from ServiceNow under the catalogue item O365/AzureAD Platform Admin . The comments should make clear that the request is for a a- Microsoft administrative account for Azure Portal.","title":"Azure AD administrative account"},{"location":"getting-started/arrange-access/#access-to-the-azure-portal","text":"Once the account has been created, the user will be able to access the Azure Portal. A request should be raised in ServiceNow for CCoE to provide access to the FCP Azure subscriptions. FCP environments are split across three Azure tenants, Development, PreProduction and Production. If a developer has Security Clearance, they can request access to the PreProduction and Production tenants. Otherwise they will only be able to access the Development tenant. The request should make clear which environments the developer requires access to. Evidence of Security Clearance will be requested by CCoE prior to completing the request","title":"Access to the Azure Portal"},{"location":"getting-started/arrange-access/#access-to-azure-kubernetes-service-aks","text":"As AKS is the primary compute hosting platform for FCP, developers will need to be added to the appropriate Kubernetes Cluster role. This will allow access to Kubernetes using client tools such as kubectl or Lens . As with the Azure Portal, developers can only be added to the PreProduction and Production clusters if they have Security Clearance. A read only role, is all that is permitted for developers in PreProduction and Production. A request should be raised in ServiceNow for CCoE to provide access to the FCP AKS clusters.","title":"Access to Azure Kubernetes Service (AKS)"},{"location":"getting-started/arrange-access/#jenkins","text":"Jenkins is used for Continuous Integration pipelines in FCP. Developers will need a .dtz account to access Jenkins. A request should be raised in ServiceNow for CCoE to provide access to Jenkins.","title":"Jenkins"},{"location":"getting-started/arrange-access/#synk","text":"Synk is used for security scanning of code dependencies. An FCP organisation has been created in Synk and developers will need to be added to it. Contact one of Defra's Principal Developers to arrange access.","title":"Synk"},{"location":"getting-started/arrange-access/#github","text":"FCP uses GitHub for source control within the Defra organisation. As per GDS standards, code is open source by default. Developers will need to be added to the FFC GitHub team. Two Factor Authentication (2FA) and commit signing are mandatory for all GitHub accounts. Contact one of Defra's Principal Developers to arrange access.","title":"GitHub"},{"location":"getting-started/arrange-access/#sonarcloud","text":"SonarCloud is used for static code analysis during Continuous Integration pipelines in FCP. A Defra organisation has been created and developers will need to be added to it using their GitHub account. Contact one of Defra's Principal Developers to arrange access.","title":"SonarCloud"},{"location":"getting-started/arrange-access/#azure-devops-ado","text":"Azure DevOps is used for Continuous Delivery pipelines in FCP. ADO is also used for work item tracking and Wiki creation. Developers will need to be added to the DEFRA-FFC project. A request should be raised in ServiceNow for CCoE to provide access to ADO.","title":"Azure DevOps (ADO)"},{"location":"getting-started/arrange-access/#microsoft-teams","text":"Microsoft Teams is the primary communication tool used across Defra. Teams supports access from third party teams tenants to allow collaboration across corporate boundaries. Developers should be added to the following Teams chat channels:","title":"Microsoft Teams"},{"location":"getting-started/arrange-access/#defra-fcp-platform-support-teams-chat","text":"This chat is used for general support and collaboration across all services using the FCP Platform.","title":"DEFRA FCP - Platform Support - Teams Chat"},{"location":"getting-started/arrange-access/#fcp-payments-documents-demo-and-progressive-reductions-releases","text":"This chat is used to arrange and coordinate releases across the majority of FCP services. As other services onboard to the FCP Platform, they can also use this chat.","title":"FCP Payments, Documents, Demo and Progressive Reductions releases"},{"location":"getting-started/arrange-access/#fcp-developer-forum","text":"This chat is used for developer collaboration and support across all teams within the FCP Programme and those using the FCP Platform. Contact one of Defra's Principal Developers to arrange access.","title":"FCP Developer Forum"},{"location":"getting-started/arrange-access/#slack","text":"","title":"Slack"},{"location":"getting-started/arrange-access/#ffc-notificationsslackcom","text":"Jenkins will publish alerts to Slack in the ffc-notifications.slack.com workspace. Developers will need to be added to the following channels:","title":"ffc-notifications.slack.com"},{"location":"getting-started/arrange-access/#masterbuildfailures","text":"Notifications of failures relating to main branches.","title":"#masterbuildfailures"},{"location":"getting-started/arrange-access/#generalbuildfailures","text":"Notifications of failures relating to feature branches.","title":"#generalbuildfailures"},{"location":"getting-started/arrange-access/#secretdetection","text":"Notifications of detected secrets found in public GitHub repositories for review. Contact one of Defra's Principal Developers to arrange access.","title":"#secretdetection"},{"location":"getting-started/arrange-access/#defra-digitalslackcom","text":"The wider Defra Digital Slack workspace is used for collaboration and support across all of Defra's digital services. Whilst not essential to be part of this workspace, it may be beneficial for cross Defra collaboration. Developers are recommended to join the following channel:","title":"defra-digital.slack.com"},{"location":"getting-started/arrange-access/#development","text":"This channel is used for general development discussion and support across all of Defra's digital services. For FCP Platform or service specific support, the above Teams chats should be used instead. Anyone with a defra.gov.uk or an ALB email address can join the Defra Digital Slack workspace automatically. For those with other email addresses, a request should be raised in the #slack-support channel by someone with access.","title":"#development"},{"location":"getting-started/arrange-access/#sharepoint","text":"SharePoint is Defra's primary document library. An FCP SharePoint site has been created. Not all developers will need access to SharePoint and it may not be possible for external suppliers to be provided with access. Should access be required, it should be request to an FCP Programme Delivery Manager.","title":"SharePoint"},{"location":"getting-started/arrange-access/#confluence","text":"Whilst ADO Wiki's are preferred, much historic content exists in Defra Confluence . New licenses are often challenging to obtain, but should be requested through the defra-digital.slack.com Slack workspace in the #jira-support channel.","title":"Confluence"},{"location":"getting-started/platform/","text":"Platform FCP Platform To support rapid, highly assured delivery, the programme has delivered common Azure environments, PaaS components, delivery pipelines and supporting tools. These are collectively referred to as the FCP Platform . All FCP developed services are currently deployed to the FCP Platform. Guidance for use of the FCP Platform is hosted within this repository. ADP Platform Following the success of the FCP Platform, Defra has created a Defra wide iteration known as the ADP Platform . The ADP Platform is built on the same principles as the FCP Platform, but takes into account lessons learned throughout the lifetime of the FCP Platform to provide a better experience for teams utilising it. It is the long term intention of the programme to migrate all FCP Platform hosted services to the ADP Platform once it becomes mature enough to provide feature parity. Guidance for use of the ADP Platform is hosted within the ADP Documentation . Choosing a Platform When starting a new project, the choice of platform should be made in consultation with the FCP Architecture team. However, it is expected that all new projects will be hosted on the ADP Platform unless there is a compelling reason not to. Early engagement with the ADP Platform team is encouraged to ensure that the platform is suitable for the project's needs.","title":"Platform"},{"location":"getting-started/platform/#platform","text":"","title":"Platform"},{"location":"getting-started/platform/#fcp-platform","text":"To support rapid, highly assured delivery, the programme has delivered common Azure environments, PaaS components, delivery pipelines and supporting tools. These are collectively referred to as the FCP Platform . All FCP developed services are currently deployed to the FCP Platform. Guidance for use of the FCP Platform is hosted within this repository.","title":"FCP Platform"},{"location":"getting-started/platform/#adp-platform","text":"Following the success of the FCP Platform, Defra has created a Defra wide iteration known as the ADP Platform . The ADP Platform is built on the same principles as the FCP Platform, but takes into account lessons learned throughout the lifetime of the FCP Platform to provide a better experience for teams utilising it. It is the long term intention of the programme to migrate all FCP Platform hosted services to the ADP Platform once it becomes mature enough to provide feature parity. Guidance for use of the ADP Platform is hosted within the ADP Documentation .","title":"ADP Platform"},{"location":"getting-started/platform/#choosing-a-platform","text":"When starting a new project, the choice of platform should be made in consultation with the FCP Architecture team. However, it is expected that all new projects will be hosted on the ADP Platform unless there is a compelling reason not to. Early engagement with the ADP Platform team is encouraged to ensure that the platform is suitable for the project's needs.","title":"Choosing a Platform"},{"location":"local-development-setup/","text":"Setup local development environment Developers are free to use any Defra approved device as long as they adhere to Defra's guidance on use of unmanaged devices . The most common environments are Windows with Windows Subsystem for Linux (WSL) Ubuntu distro and macOS. Visual Studio Code is the preferred code editor. However, developers are free to use any compatible equivalent if it aides in their productivity. This guide will be targeted towards the above setups. For Windows, all mention of terminal commands should be run in WSL unless specified otherwise. Microsoft InTune Microsoft InTune allows an off network device access to Defra O365 resources such as Outlook, Teams and SharePoint. New InTune enrolments are currently banned which means if a user does not already have an InTune profile, they will not be able to access these resources from their device. There is no alternative at present. InTune requires that the user profile on a device is an organisation account. So either this must be setup when first configuring the device or a new user profile will need to be created on the device. It is not possible to convert an existing user profile to an InTune profile. Setup steps Windows Install Windows Subsystem for Linux (WSL) Mount Windows drives in WSL Add user to sudoers file (optional) macOS Setup command line tools Common Install Docker Desktop Install IDE Install StandardJs Install SonarLint Setup commit signing Install Docker Compose Install Detect Secrets Install Node Version Manager (NVM) Install .NET SDK Install kubectl Install Helm Install Azure CLI Install Snyk CLI Install GitHub CLI Install OpenVPN","title":"Setup local development environment"},{"location":"local-development-setup/#setup-local-development-environment","text":"Developers are free to use any Defra approved device as long as they adhere to Defra's guidance on use of unmanaged devices . The most common environments are Windows with Windows Subsystem for Linux (WSL) Ubuntu distro and macOS. Visual Studio Code is the preferred code editor. However, developers are free to use any compatible equivalent if it aides in their productivity. This guide will be targeted towards the above setups. For Windows, all mention of terminal commands should be run in WSL unless specified otherwise.","title":"Setup local development environment"},{"location":"local-development-setup/#microsoft-intune","text":"Microsoft InTune allows an off network device access to Defra O365 resources such as Outlook, Teams and SharePoint. New InTune enrolments are currently banned which means if a user does not already have an InTune profile, they will not be able to access these resources from their device. There is no alternative at present. InTune requires that the user profile on a device is an organisation account. So either this must be setup when first configuring the device or a new user profile will need to be created on the device. It is not possible to convert an existing user profile to an InTune profile.","title":"Microsoft InTune"},{"location":"local-development-setup/#setup-steps","text":"","title":"Setup steps"},{"location":"local-development-setup/#windows","text":"Install Windows Subsystem for Linux (WSL) Mount Windows drives in WSL Add user to sudoers file (optional)","title":"Windows"},{"location":"local-development-setup/#macos","text":"Setup command line tools","title":"macOS"},{"location":"local-development-setup/#common","text":"Install Docker Desktop Install IDE Install StandardJs Install SonarLint Setup commit signing Install Docker Compose Install Detect Secrets Install Node Version Manager (NVM) Install .NET SDK Install kubectl Install Helm Install Azure CLI Install Snyk CLI Install GitHub CLI Install OpenVPN","title":"Common"},{"location":"local-development-setup/install-azure-cli/","text":"Install Azure CLI Azure CLI enables interaction with Azure services from the command line. Installation Follow Microsoft's setup guide Login to Azure tenant az login --tenant TENANT.onmicrosoft.com Follow the instructions displayed and sign in using your a- Microsoft administration account.","title":"Install Azure CLI"},{"location":"local-development-setup/install-azure-cli/#install-azure-cli","text":"Azure CLI enables interaction with Azure services from the command line.","title":"Install Azure CLI"},{"location":"local-development-setup/install-azure-cli/#installation","text":"Follow Microsoft's setup guide","title":"Installation"},{"location":"local-development-setup/install-azure-cli/#login-to-azure-tenant","text":"az login --tenant TENANT.onmicrosoft.com Follow the instructions displayed and sign in using your a- Microsoft administration account.","title":"Login to Azure tenant"},{"location":"local-development-setup/install-detect-secrets/","text":"Install Detect Secrets Prevent committing passwords and other sensitive information to git repositories. The detect-secrets tool provides out-of-the-box support for scanning git commits for different types of credentials including keywords (e.g. 'password' or 'secret'), private SSH keys, and base64 high entropy string. Installation Install prerequisites detect-secrets is written in Python and will require Python version 3 and pip (the package installer for Python) to be installed on your system. Python Install python version 3, download the latest version for your operating system - Windows - Mac OS Once downloaded, run the .exe (windows) or .pkg (macOS) file. Follow the on screen prompts, after successful installation run the following commands to confirm Python was successfully installed. - Windows - Open Powershell/Command Prompt and type python --verison , it should report the version - Mac Os - Open a Terminal and type python \u2013version , it should report the version WSL If using WSL2 with Docker Desktop then python should already be installed. However, if needed it can be added with the following guide: - WSL Once successfully installed, run the command python3 \u2013-version , it should report the version pip Install pip using the get-pip.py script following these instructions NOTE: pip is already installed if you are using Python 2 >=2.7.9 or Python 3 >=3.4 downloaded from python.org or if you are working in a Virtual Environment created by virtualenv or pyvenv. Just make sure to upgrade pip. Install detect-secrets and pre-commit detect-secrets harnesses the pre-commit tool to set-up the git pre-commit hook that runs detect-secrets on the contents of the commit. Install pre-commit by following the instructions for your system Install detect-secrets by running: pip install detect-secrets Configuration A pre-commit configuration file should exist in every FFC git repository that contains the necessary information to run detect-secrets . See the guide for creating FFC git repositories . Your system will need to be configured to set up the git hooks for both currently cloned, and future cloned, FFC repositories. Currently cloned FFC git repositories Set up the pre-commit git hook to run detect-secrets by running the following command in every FFC repository you have cloned on your system : pre-commit install All future cloned FFC git repositories To automatically set up the pre-commit git hook to run detect-secrets for newly cloned repositories, set up a global template: git config --global init.templateDir ~/.git-template pre-commit init-templatedir ~/.git-template Using with other git hooks managers pre-commit installs a single git hook to .git/hooks/pre-commit By default git only allows a single script to be run for each hook. If a repository is using a git hooks manager such as husky , additional configuration will be required in order to run git hooks created by husky and git hooks created by git-secrets. The solution for running multiple scripts from a single hook is out of scope of this document. However, husky provides options and this Stack Overflow post discusses another. Usage Refer to the secrets management guide for details on dealing with detect-secrets false positives and excludes.","title":"Install Detect Secrets"},{"location":"local-development-setup/install-detect-secrets/#install-detect-secrets","text":"Prevent committing passwords and other sensitive information to git repositories. The detect-secrets tool provides out-of-the-box support for scanning git commits for different types of credentials including keywords (e.g. 'password' or 'secret'), private SSH keys, and base64 high entropy string.","title":"Install Detect Secrets"},{"location":"local-development-setup/install-detect-secrets/#installation","text":"","title":"Installation"},{"location":"local-development-setup/install-detect-secrets/#install-prerequisites","text":"detect-secrets is written in Python and will require Python version 3 and pip (the package installer for Python) to be installed on your system.","title":"Install prerequisites"},{"location":"local-development-setup/install-detect-secrets/#python","text":"Install python version 3, download the latest version for your operating system - Windows - Mac OS Once downloaded, run the .exe (windows) or .pkg (macOS) file. Follow the on screen prompts, after successful installation run the following commands to confirm Python was successfully installed. - Windows - Open Powershell/Command Prompt and type python --verison , it should report the version - Mac Os - Open a Terminal and type python \u2013version , it should report the version","title":"Python"},{"location":"local-development-setup/install-detect-secrets/#wsl","text":"If using WSL2 with Docker Desktop then python should already be installed. However, if needed it can be added with the following guide: - WSL Once successfully installed, run the command python3 \u2013-version , it should report the version","title":"WSL"},{"location":"local-development-setup/install-detect-secrets/#pip","text":"Install pip using the get-pip.py script following these instructions NOTE: pip is already installed if you are using Python 2 >=2.7.9 or Python 3 >=3.4 downloaded from python.org or if you are working in a Virtual Environment created by virtualenv or pyvenv. Just make sure to upgrade pip.","title":"pip"},{"location":"local-development-setup/install-detect-secrets/#install-detect-secrets-and-pre-commit","text":"detect-secrets harnesses the pre-commit tool to set-up the git pre-commit hook that runs detect-secrets on the contents of the commit. Install pre-commit by following the instructions for your system Install detect-secrets by running: pip install detect-secrets","title":"Install detect-secrets and pre-commit"},{"location":"local-development-setup/install-detect-secrets/#configuration","text":"A pre-commit configuration file should exist in every FFC git repository that contains the necessary information to run detect-secrets . See the guide for creating FFC git repositories . Your system will need to be configured to set up the git hooks for both currently cloned, and future cloned, FFC repositories.","title":"Configuration"},{"location":"local-development-setup/install-detect-secrets/#currently-cloned-ffc-git-repositories","text":"Set up the pre-commit git hook to run detect-secrets by running the following command in every FFC repository you have cloned on your system : pre-commit install","title":"Currently cloned FFC git repositories"},{"location":"local-development-setup/install-detect-secrets/#all-future-cloned-ffc-git-repositories","text":"To automatically set up the pre-commit git hook to run detect-secrets for newly cloned repositories, set up a global template: git config --global init.templateDir ~/.git-template pre-commit init-templatedir ~/.git-template","title":"All future cloned FFC git repositories"},{"location":"local-development-setup/install-detect-secrets/#using-with-other-git-hooks-managers","text":"pre-commit installs a single git hook to .git/hooks/pre-commit By default git only allows a single script to be run for each hook. If a repository is using a git hooks manager such as husky , additional configuration will be required in order to run git hooks created by husky and git hooks created by git-secrets. The solution for running multiple scripts from a single hook is out of scope of this document. However, husky provides options and this Stack Overflow post discusses another.","title":"Using with other git hooks managers"},{"location":"local-development-setup/install-detect-secrets/#usage","text":"Refer to the secrets management guide for details on dealing with detect-secrets false positives and excludes.","title":"Usage"},{"location":"local-development-setup/install-docker-compose/","text":"Install Docker Compose Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Then, with a single command, you create and start all the services from your configuration. When installing Docker Desktop, Compose is automatically added to WSL. Compose provides all the capability of Docker Compose and is intended to be the long term replacement, avoiding the need for the below installation as most docker-compose commands are included. However, it is to be noted that Compose has some differences detailed in this document It is also to be noted that Compose will not work with any volume mount ending with a trailing / . For example, this will work: - volumes: - ./app:/home/node/app This will not: - volumes: - ./app/:/home/node/app/ Installation Only follow this if your current installation of Docker Desktop does not include Compose. Follow the installation guide provided by Docker for your OS.","title":"Install Docker Compose"},{"location":"local-development-setup/install-docker-compose/#install-docker-compose","text":"Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Then, with a single command, you create and start all the services from your configuration. When installing Docker Desktop, Compose is automatically added to WSL. Compose provides all the capability of Docker Compose and is intended to be the long term replacement, avoiding the need for the below installation as most docker-compose commands are included. However, it is to be noted that Compose has some differences detailed in this document It is also to be noted that Compose will not work with any volume mount ending with a trailing / . For example, this will work: - volumes: - ./app:/home/node/app This will not: - volumes: - ./app/:/home/node/app/","title":"Install Docker Compose"},{"location":"local-development-setup/install-docker-compose/#installation","text":"Only follow this if your current installation of Docker Desktop does not include Compose. Follow the installation guide provided by Docker for your OS.","title":"Installation"},{"location":"local-development-setup/install-docker-desktop/","text":"Install Docker Desktop Docker Desktop is a tool for MacOS and Windows machines for the building and sharing of containerized applications and microservices. It is the fastest and easiest way to get started with Docker on your local machine. Installation If not already installed as part of the Windows Subsystem for Linux (WSL) setup, follow the below steps to install Docker Desktop. Download the Docker Desktop installer for your operating system. Run the installer and follow the prompts to install Docker Desktop. WSL Ensure that Docker Desktop is set to use the WSL2 backend. This can be done by right clicking the Docker Desktop icon in the system tray and selecting Settings . From there, select General and ensure Use the WSL 2 based engine is checked. Ensure that Docker Desktop is available to your WSL distro. This can be done by right clicking the Docker Desktop icon in the system tray and selecting Settings . From there, select Resources , WSL Integration and ensure that the distro you are using is checked.","title":"Install Docker Desktop"},{"location":"local-development-setup/install-docker-desktop/#install-docker-desktop","text":"Docker Desktop is a tool for MacOS and Windows machines for the building and sharing of containerized applications and microservices. It is the fastest and easiest way to get started with Docker on your local machine.","title":"Install Docker Desktop"},{"location":"local-development-setup/install-docker-desktop/#installation","text":"If not already installed as part of the Windows Subsystem for Linux (WSL) setup, follow the below steps to install Docker Desktop. Download the Docker Desktop installer for your operating system. Run the installer and follow the prompts to install Docker Desktop.","title":"Installation"},{"location":"local-development-setup/install-docker-desktop/#wsl","text":"Ensure that Docker Desktop is set to use the WSL2 backend. This can be done by right clicking the Docker Desktop icon in the system tray and selecting Settings . From there, select General and ensure Use the WSL 2 based engine is checked. Ensure that Docker Desktop is available to your WSL distro. This can be done by right clicking the Docker Desktop icon in the system tray and selecting Settings . From there, select Resources , WSL Integration and ensure that the distro you are using is checked.","title":"WSL"},{"location":"local-development-setup/install-dotnet-sdk/","text":"Install .NET SDK The .NET SDK is a set of libraries and tools that allow developers to create .NET applications and libraries. Choose version (or use LTS) Select the version required then either a package manager or binary for your OS from Microsoft Note On macOS, creation of symbolic links is missing from the installer after installation. To create them manually run ln -s /usr/local/share/dotnet/dotnet /usr/local/bin/ Verify installation Check your installation was successful with dotnet --version Install .NET tools As per Microsoft's setup guide the Entity Framework tools allow the creation and application of code first migrations can be installed with the command dotnet tool install --global dotnet-ef","title":"Install .NET SDK"},{"location":"local-development-setup/install-dotnet-sdk/#install-net-sdk","text":"The .NET SDK is a set of libraries and tools that allow developers to create .NET applications and libraries.","title":"Install .NET SDK"},{"location":"local-development-setup/install-dotnet-sdk/#choose-version-or-use-lts","text":"Select the version required then either a package manager or binary for your OS from Microsoft Note On macOS, creation of symbolic links is missing from the installer after installation. To create them manually run ln -s /usr/local/share/dotnet/dotnet /usr/local/bin/","title":"Choose version (or use LTS)"},{"location":"local-development-setup/install-dotnet-sdk/#verify-installation","text":"Check your installation was successful with dotnet --version","title":"Verify installation"},{"location":"local-development-setup/install-dotnet-sdk/#install-net-tools","text":"As per Microsoft's setup guide the Entity Framework tools allow the creation and application of code first migrations can be installed with the command dotnet tool install --global dotnet-ef","title":"Install .NET tools"},{"location":"local-development-setup/install-github/","text":"Install GitHub CLI GitHub allows interaction with GitHub via the command line. Simplifying creation of remote branches and associated Pull Requests. For example, a new local branch can automatically be pushed to GitHub with a draft Pull Request with: gh pr create -d Installation Follow the appropriate guide for your OS: Install guide Linux (including WSL) guide","title":"Install GitHub CLI"},{"location":"local-development-setup/install-github/#install-github-cli","text":"GitHub allows interaction with GitHub via the command line. Simplifying creation of remote branches and associated Pull Requests. For example, a new local branch can automatically be pushed to GitHub with a draft Pull Request with: gh pr create -d","title":"Install GitHub CLI"},{"location":"local-development-setup/install-github/#installation","text":"Follow the appropriate guide for your OS: Install guide Linux (including WSL) guide","title":"Installation"},{"location":"local-development-setup/install-kubectl/","text":"Install kubectl kubectl is a command line tool for interacting with Kubernetes clusters. Installation Follow the Kubernetes documentation Note For WSL use the 'Install on Linux' instructions Setup shell autocompletion (optional) sudo sh -c 'kubectl completion bash > /etc/bash_completion.d/kubectl' Notes: 1. assumes you are using bash shell 1. you will need to reload your shell for the change to be picked up Reference https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion Add command alias for kubectl (optional) To add the frequently used alias k for kubectl add the following lines to your .bashrc file (the 2nd line adds autocomplete for the alias): alias k=kubectl complete -o default -F __start_kubectl k Add further aliases for Kubectl (optional) Download the following file and save to your home directory. https://github.com/ahmetb/kubectl-aliases/blob/0533366d8e3e3b3987cc1b7b07a7e8fcfb69f93c/.kubectl_aliases Update your .bashrc file with the below to enable autocomplete on all aliases in the file. # Kubectl [ -f ~/.kubectl_aliases ] && source ~/.kubectl_aliases source <(kubectl completion bash) for a in $(sed '/^alias /!d;s/^alias //;s/=.*$//' ~/.kubectl_aliases); do complete -F _complete_alias \"$a\" done For quick switching of Kubernetes contexts and namespaces, it may be beneficial to append the following lines to the kubectl_aliases file. alias kns='kubectl config set-context --current --namespace' alias kc='kubectl config use-context' Full details are available in this blog post","title":"Install kubectl"},{"location":"local-development-setup/install-kubectl/#install-kubectl","text":"kubectl is a command line tool for interacting with Kubernetes clusters.","title":"Install kubectl"},{"location":"local-development-setup/install-kubectl/#installation","text":"Follow the Kubernetes documentation Note For WSL use the 'Install on Linux' instructions","title":"Installation"},{"location":"local-development-setup/install-kubectl/#setup-shell-autocompletion-optional","text":"sudo sh -c 'kubectl completion bash > /etc/bash_completion.d/kubectl' Notes: 1. assumes you are using bash shell 1. you will need to reload your shell for the change to be picked up Reference https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion","title":"Setup shell autocompletion (optional)"},{"location":"local-development-setup/install-kubectl/#add-command-alias-for-kubectl-optional","text":"To add the frequently used alias k for kubectl add the following lines to your .bashrc file (the 2nd line adds autocomplete for the alias): alias k=kubectl complete -o default -F __start_kubectl k","title":"Add command alias for kubectl (optional)"},{"location":"local-development-setup/install-kubectl/#add-further-aliases-for-kubectl-optional","text":"Download the following file and save to your home directory. https://github.com/ahmetb/kubectl-aliases/blob/0533366d8e3e3b3987cc1b7b07a7e8fcfb69f93c/.kubectl_aliases Update your .bashrc file with the below to enable autocomplete on all aliases in the file. # Kubectl [ -f ~/.kubectl_aliases ] && source ~/.kubectl_aliases source <(kubectl completion bash) for a in $(sed '/^alias /!d;s/^alias //;s/=.*$//' ~/.kubectl_aliases); do complete -F _complete_alias \"$a\" done For quick switching of Kubernetes contexts and namespaces, it may be beneficial to append the following lines to the kubectl_aliases file. alias kns='kubectl config set-context --current --namespace' alias kc='kubectl config use-context' Full details are available in this blog post","title":"Add further aliases for Kubectl (optional)"},{"location":"local-development-setup/install-node-version-manager/","text":"Install Node Version Manager Node Version Manager allows a developer to easily install and switch between multiple Node versions on a single operating system. Installation Follow the installation guide","title":"Install Node Version Manager"},{"location":"local-development-setup/install-node-version-manager/#install-node-version-manager","text":"Node Version Manager allows a developer to easily install and switch between multiple Node versions on a single operating system.","title":"Install Node Version Manager"},{"location":"local-development-setup/install-node-version-manager/#installation","text":"Follow the installation guide","title":"Installation"},{"location":"local-development-setup/install-openvpn/","text":"Install OpenVPN Access to some cloud resources is restricted to VPN access only. Install client Install the client following the OpenVPN documentation Configure client Cloud Centre of Excellence (CCoE) will provide full client configuration instructions when setting up access for a new FCP developer.","title":"Install OpenVPN"},{"location":"local-development-setup/install-openvpn/#install-openvpn","text":"Access to some cloud resources is restricted to VPN access only.","title":"Install OpenVPN"},{"location":"local-development-setup/install-openvpn/#install-client","text":"Install the client following the OpenVPN documentation","title":"Install client"},{"location":"local-development-setup/install-openvpn/#configure-client","text":"Cloud Centre of Excellence (CCoE) will provide full client configuration instructions when setting up access for a new FCP developer.","title":"Configure client"},{"location":"local-development-setup/install-snyk/","text":"Install Snyk CLI Installation Follow the setup guide in the official Snyk documentation .","title":"Install Snyk CLI"},{"location":"local-development-setup/install-snyk/#install-snyk-cli","text":"","title":"Install Snyk CLI"},{"location":"local-development-setup/install-snyk/#installation","text":"Follow the setup guide in the official Snyk documentation .","title":"Installation"},{"location":"local-development-setup/install-sonarlint/","text":"Install SonarLint SonarLint is an IDE extension that identifies code quality issues as you code. It can be configured to run in connected mode which allows rules configured in a SonarCloud instance to be applied to a local workspace, flagging up issues as the developer writes code. This guide will demonstrate how to install the SonarLint extension in VS Code. For other IDEs, refer to the SonarLint documentation . Dependencies Java Runtime Environment v17+ With Ubuntu, the open source version of the Java Runtime Environment (JRE) can be installed using the following command. sudo apt-get install openjdk-17-jre SonarLint Installation (VS Code) install SonarLint extension set location of JRE in VS Code settings. The below example is the install location of the above command json \"sonarlint.ls.javaHome\": \"/usr/lib/jvm/java-11-openjdk-amd64\" Note to WSL users ensure that you update the remote settings.json file if you wish to use SonarLint in your linux environment. This will give you Sonar code analysis using default quality gates for languages supported by SonarLint. If you wish to use connected mode to sync your local workspace with a SonarCloud project, follow the below steps. within the SonarCloud UI, navigate to your account's security settings enter a token name and generate a token, noting the token value securely within VS Code, add a SonarCloud server connection to settings.json to enable connected mode json \"sonarlint.connectedMode.connections.sonarcloud\": [{ \"organizationKey\": \"defra\", \"token\": \"MY_TOKEN\" }] within each project workspace, create or edit your workspace settings.json file to include a project link json \"sonarlint.connectedMode.project\": { \"projectKey\": \"ffc-demo-payment-web\" } Note if you have multiple SonarCloud instances then a connectionId property can be added to the two code snippets above to correctly bind a project to the correct SonarCloud instance. update project bindings by selecting Update all project bindings to SonarCloud from the VS Code Command Palette . ( ctrl + shift + p to open) Rules, quality gates, exclusions etc set in the SonarCloud project will now be applied to local workspace. Configure Sonar for C The SonarLint extension for VS Code does not currently support C#, but we can still get Sonar linting rules set up in VS Code by adding the Sonar analyser to Omnisharp. Add the Sonar Analyzer NuGet package if not already present in the csproj file: dotnet add <PROJECT_NAME> package SonarAnalyzer.CSharp Enable analysers in VS Code: Search for Omnisharp in settings and select Enable Roslyn Analysers . Alternative configuration is available outside of VS Code. When VS Code analyses C# code, it will now check against the standard set of Sonar rules for C#. The Sonar rule IDs are prefixed with S when displayed in VS Code. For more details on the rules you can consult the Sonar rules documentation .","title":"Install SonarLint"},{"location":"local-development-setup/install-sonarlint/#install-sonarlint","text":"SonarLint is an IDE extension that identifies code quality issues as you code. It can be configured to run in connected mode which allows rules configured in a SonarCloud instance to be applied to a local workspace, flagging up issues as the developer writes code. This guide will demonstrate how to install the SonarLint extension in VS Code. For other IDEs, refer to the SonarLint documentation .","title":"Install SonarLint"},{"location":"local-development-setup/install-sonarlint/#dependencies","text":"Java Runtime Environment v17+ With Ubuntu, the open source version of the Java Runtime Environment (JRE) can be installed using the following command. sudo apt-get install openjdk-17-jre","title":"Dependencies"},{"location":"local-development-setup/install-sonarlint/#sonarlint-installation-vs-code","text":"install SonarLint extension set location of JRE in VS Code settings. The below example is the install location of the above command json \"sonarlint.ls.javaHome\": \"/usr/lib/jvm/java-11-openjdk-amd64\" Note to WSL users ensure that you update the remote settings.json file if you wish to use SonarLint in your linux environment. This will give you Sonar code analysis using default quality gates for languages supported by SonarLint. If you wish to use connected mode to sync your local workspace with a SonarCloud project, follow the below steps. within the SonarCloud UI, navigate to your account's security settings enter a token name and generate a token, noting the token value securely within VS Code, add a SonarCloud server connection to settings.json to enable connected mode json \"sonarlint.connectedMode.connections.sonarcloud\": [{ \"organizationKey\": \"defra\", \"token\": \"MY_TOKEN\" }] within each project workspace, create or edit your workspace settings.json file to include a project link json \"sonarlint.connectedMode.project\": { \"projectKey\": \"ffc-demo-payment-web\" } Note if you have multiple SonarCloud instances then a connectionId property can be added to the two code snippets above to correctly bind a project to the correct SonarCloud instance. update project bindings by selecting Update all project bindings to SonarCloud from the VS Code Command Palette . ( ctrl + shift + p to open) Rules, quality gates, exclusions etc set in the SonarCloud project will now be applied to local workspace.","title":"SonarLint Installation (VS Code)"},{"location":"local-development-setup/install-sonarlint/#configure-sonar-for-c","text":"The SonarLint extension for VS Code does not currently support C#, but we can still get Sonar linting rules set up in VS Code by adding the Sonar analyser to Omnisharp. Add the Sonar Analyzer NuGet package if not already present in the csproj file: dotnet add <PROJECT_NAME> package SonarAnalyzer.CSharp Enable analysers in VS Code: Search for Omnisharp in settings and select Enable Roslyn Analysers . Alternative configuration is available outside of VS Code. When VS Code analyses C# code, it will now check against the standard set of Sonar rules for C#. The Sonar rule IDs are prefixed with S when displayed in VS Code. For more details on the rules you can consult the Sonar rules documentation .","title":"Configure Sonar for C"},{"location":"local-development-setup/install-standard-js/","text":"Install StandardJs In line with Defra's digital standards for JavaScript, StandardJS coding standards should be used. Installation Run the following command to install StandardJS globally: npm install -g standard","title":"Install StandardJs"},{"location":"local-development-setup/install-standard-js/#install-standardjs","text":"In line with Defra's digital standards for JavaScript, StandardJS coding standards should be used.","title":"Install StandardJs"},{"location":"local-development-setup/install-standard-js/#installation","text":"Run the following command to install StandardJS globally: npm install -g standard","title":"Installation"},{"location":"local-development-setup/install-vs-code/","text":"Install IDE Developers are free to use their own IDE/text editor of choice. However, this guide will assume Visual Studio Code will be used as that is the most commonly used tool in FCP and Defra. If not using VS Code, this guide should still be read to ensure that the equivalent installation steps are applied. Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and macOS. It includes support for debugging, embedded Git control and GitHub, syntax highlighting, intelligent code completion, snippets, and code refactoring. Installation VS Code Install for your operating system following Microsoft's guidance VS Code Integration Visual Studio has an extension to validate syntax for StandardJS, Defra's linting standard. Install StandardJS extension Disable VS Code JavaScript Validation by adding \"javascript.validate.enable\": false to settings.json Set auto fix on save by adding \"standard.autoFixOnSave\": true to settings.json Install C# Extension Add C# extension to enable .Net development from the Marketplace Configure autosave If you prefer your code to autosave, add the following to settings.json. \"files.autoSave\": \"afterDelay\" \"files.autoSaveDelay\": 600 Indentation VS Code supports multiple indentation settings for each language by updating the settings.json file. The below example sets a default indentation to two spaces, but uses four spaces for C# files as per Microsoft standards. \"editor.tabSize\": 2, \"[csharp]\":{ \"editor.tabSize\": 4, }, Note JS standard will automatically correct JavaScript files to two space indentation. WSL Configuration Install Remote WSL extension The Remote - WSL allows VS Code installed in Windows to be used in WSL. This means when code is source controlled in WSL, VS Code can read, debug and interact with it without workarounds. In WSL2 it is recommended to always use the remote WSL approach. A current directory can be opened in VS Code with code . . With WSL1, as source code will be in Windows, depending on the development activity it may be easier to use VS Code in Windows. Turn on signed commits To turn on signed commits (see section on signed commits for more information) in the UI make sure the Enable commit signing with GPG or X.509 is selected. For the JSON version of the settings use \"git.enableCommitSigning\": true","title":"Install IDE"},{"location":"local-development-setup/install-vs-code/#install-ide","text":"Developers are free to use their own IDE/text editor of choice. However, this guide will assume Visual Studio Code will be used as that is the most commonly used tool in FCP and Defra. If not using VS Code, this guide should still be read to ensure that the equivalent installation steps are applied. Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and macOS. It includes support for debugging, embedded Git control and GitHub, syntax highlighting, intelligent code completion, snippets, and code refactoring.","title":"Install IDE"},{"location":"local-development-setup/install-vs-code/#installation-vs-code","text":"Install for your operating system following Microsoft's guidance","title":"Installation VS Code"},{"location":"local-development-setup/install-vs-code/#vs-code-integration","text":"Visual Studio has an extension to validate syntax for StandardJS, Defra's linting standard. Install StandardJS extension Disable VS Code JavaScript Validation by adding \"javascript.validate.enable\": false to settings.json Set auto fix on save by adding \"standard.autoFixOnSave\": true to settings.json","title":"VS Code Integration"},{"location":"local-development-setup/install-vs-code/#install-c-extension","text":"Add C# extension to enable .Net development from the Marketplace","title":"Install C# Extension"},{"location":"local-development-setup/install-vs-code/#configure-autosave","text":"If you prefer your code to autosave, add the following to settings.json. \"files.autoSave\": \"afterDelay\" \"files.autoSaveDelay\": 600","title":"Configure autosave"},{"location":"local-development-setup/install-vs-code/#indentation","text":"VS Code supports multiple indentation settings for each language by updating the settings.json file. The below example sets a default indentation to two spaces, but uses four spaces for C# files as per Microsoft standards. \"editor.tabSize\": 2, \"[csharp]\":{ \"editor.tabSize\": 4, }, Note JS standard will automatically correct JavaScript files to two space indentation.","title":"Indentation"},{"location":"local-development-setup/install-vs-code/#wsl-configuration","text":"","title":"WSL Configuration"},{"location":"local-development-setup/install-vs-code/#install-remote-wsl-extension","text":"The Remote - WSL allows VS Code installed in Windows to be used in WSL. This means when code is source controlled in WSL, VS Code can read, debug and interact with it without workarounds. In WSL2 it is recommended to always use the remote WSL approach. A current directory can be opened in VS Code with code . . With WSL1, as source code will be in Windows, depending on the development activity it may be easier to use VS Code in Windows.","title":"Install Remote WSL extension"},{"location":"local-development-setup/install-vs-code/#turn-on-signed-commits","text":"To turn on signed commits (see section on signed commits for more information) in the UI make sure the Enable commit signing with GPG or X.509 is selected. For the JSON version of the settings use \"git.enableCommitSigning\": true","title":"Turn on signed commits"},{"location":"local-development-setup/install-wsl/","text":"Install Windows Subsystem for Linux (WSL) The Windows Subsystem for Linux (WSL) allows developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a virtual machine. Official Documentation WSL1 or WSL2 WSL2 is built using a full Linux kernal and is optimised for size and performance. It also solves many of the networking and Docker integration challenges that were present in WSL1. WSL1 requires significantly more manual setup. The remainder of this guide will assume WSL2 is being used. For further information on the differences between WSL1 and WSL2, see the Microsoft documentation . Installation Follow the below Microsoft guide to install the distro of your choice. The Ubuntu distro is recommended. Installation Guide Within the guide there is also a recommended setup of a development environment using WSL. This guide should also be followed as it includes setup of Docker Desktop, Git, Windows Terminal and VS Code. Quick links Setup Docker Desktop Setup Git Setup Visual Studio Code Setup Windows Terminal","title":"Install Windows Subsystem for Linux (WSL)"},{"location":"local-development-setup/install-wsl/#install-windows-subsystem-for-linux-wsl","text":"The Windows Subsystem for Linux (WSL) allows developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a virtual machine. Official Documentation","title":"Install Windows Subsystem for Linux (WSL)"},{"location":"local-development-setup/install-wsl/#wsl1-or-wsl2","text":"WSL2 is built using a full Linux kernal and is optimised for size and performance. It also solves many of the networking and Docker integration challenges that were present in WSL1. WSL1 requires significantly more manual setup. The remainder of this guide will assume WSL2 is being used. For further information on the differences between WSL1 and WSL2, see the Microsoft documentation .","title":"WSL1 or WSL2"},{"location":"local-development-setup/install-wsl/#installation","text":"Follow the below Microsoft guide to install the distro of your choice. The Ubuntu distro is recommended. Installation Guide Within the guide there is also a recommended setup of a development environment using WSL. This guide should also be followed as it includes setup of Docker Desktop, Git, Windows Terminal and VS Code.","title":"Installation"},{"location":"local-development-setup/install-wsl/#quick-links","text":"Setup Docker Desktop Setup Git Setup Visual Studio Code Setup Windows Terminal","title":"Quick links"},{"location":"local-development-setup/installing-helm/","text":"Install Helm Helm is a package manager for Kubernetes. Helm Charts are the package definitions which help you install and upgrade Kubernetes applications. Helm 3 was released in November 2019 . Until then, Helm 2 had been the default version of Helm. There are significant differences between the two versions, one of the most substantial being the removal of Tiller . This has resulted in a much easier installation process along with improving the management experience. All charts created by FCP are done so using Helm 3. Installation Installation instructions vary based on OS. Refer to the official docs for details .","title":"Install Helm"},{"location":"local-development-setup/installing-helm/#install-helm","text":"Helm is a package manager for Kubernetes. Helm Charts are the package definitions which help you install and upgrade Kubernetes applications. Helm 3 was released in November 2019 . Until then, Helm 2 had been the default version of Helm. There are significant differences between the two versions, one of the most substantial being the removal of Tiller . This has resulted in a much easier installation process along with improving the management experience. All charts created by FCP are done so using Helm 3.","title":"Install Helm"},{"location":"local-development-setup/installing-helm/#installation","text":"Installation instructions vary based on OS. Refer to the official docs for details .","title":"Installation"},{"location":"local-development-setup/mount-windows-drives-in-wsl/","text":"Mount Windows drives in WSL WSL will automatically mount all Windows drives under the /mnt/ directory followed by the drive letter. For example, the C: drive will be accessible on /mnt/c/ . However, in order to allow WSL to change Windows file permissions, the drives need to be mounted with the metadata option. Setup 1 Create or edit WSL config file by entering the below terminal command. sudo nano /etc/wsl.conf 2 Add the following content to the file, then save and exit. [automount] options = \"metadata\" 3 Reboot machine for changes to take effect.","title":"Mount Windows drives in WSL"},{"location":"local-development-setup/mount-windows-drives-in-wsl/#mount-windows-drives-in-wsl","text":"WSL will automatically mount all Windows drives under the /mnt/ directory followed by the drive letter. For example, the C: drive will be accessible on /mnt/c/ . However, in order to allow WSL to change Windows file permissions, the drives need to be mounted with the metadata option.","title":"Mount Windows drives in WSL"},{"location":"local-development-setup/mount-windows-drives-in-wsl/#setup","text":"1 Create or edit WSL config file by entering the below terminal command. sudo nano /etc/wsl.conf 2 Add the following content to the file, then save and exit. [automount] options = \"metadata\" 3 Reboot machine for changes to take effect.","title":"Setup"},{"location":"local-development-setup/setup-macos-command-line-tools/","text":"Setup command line tools Mac users who prefer to have a more traditional Unix toolkit accessible to them through the Terminal may wish to install the optional Command Line Tools subsection of the Xcode IDE. This is possible installing the entire Xcode package, no developer account is required either. Install command line tools xcode-select --install","title":"Setup command line tools"},{"location":"local-development-setup/setup-macos-command-line-tools/#setup-command-line-tools","text":"Mac users who prefer to have a more traditional Unix toolkit accessible to them through the Terminal may wish to install the optional Command Line Tools subsection of the Xcode IDE. This is possible installing the entire Xcode package, no developer account is required either.","title":"Setup command line tools"},{"location":"local-development-setup/setup-macos-command-line-tools/#install-command-line-tools","text":"xcode-select --install","title":"Install command line tools"},{"location":"local-development-setup/setup-sudoers/","text":"Add user to sudoers file sudo is a command line programme that allows trusted users to execute commands as root user. Running a sudo command will prompt for a password. For local development in WSL, developers may prefer to add their WSL user to the sudoers file to avoid the need for a password. Update sudoers file Open sudoers file with root permission to edit with sudo nano /etc/sudoers Append username ALL=(ALL) NOPASSWD:ALL to bottom of file replacing username with your WSL username Save and exit the file","title":"Add user to sudoers file"},{"location":"local-development-setup/setup-sudoers/#add-user-to-sudoers-file","text":"sudo is a command line programme that allows trusted users to execute commands as root user. Running a sudo command will prompt for a password. For local development in WSL, developers may prefer to add their WSL user to the sudoers file to avoid the need for a password.","title":"Add user to sudoers file"},{"location":"local-development-setup/setup-sudoers/#update-sudoers-file","text":"Open sudoers file with root permission to edit with sudo nano /etc/sudoers Append username ALL=(ALL) NOPASSWD:ALL to bottom of file replacing username with your WSL username Save and exit the file","title":"Update sudoers file"},{"location":"local-development-setup/sign-commits/","text":"Setup commit signing You can sign commits and tags locally, so other people can verify that your work comes from a trusted source. If a commit or tag has a GPG or S/MIME signature that is cryptographically verifiable, GitHub marks the commit or tag as verified. If a commit or tag has a signature that cannot be verified, GitHub marks the commit or tag as unverified. You can check the verification status of your signed commits or tags on GitHub and view why your commit signatures might be unverified. Setup Follow GitHub's instructions for setting up and configuring a GPG key. Creating a new GPG key Adding a new GPG key to your GitHub account Setup Git to use new GPG key Signing commits via VS code To turn on signed commits in the VS Code UI, make sure the Enable commit signing with GPG or X.509 is ticked. For the JSON version of the settings use \"git.enableCommitSigning\": true","title":"Setup commit signing"},{"location":"local-development-setup/sign-commits/#setup-commit-signing","text":"You can sign commits and tags locally, so other people can verify that your work comes from a trusted source. If a commit or tag has a GPG or S/MIME signature that is cryptographically verifiable, GitHub marks the commit or tag as verified. If a commit or tag has a signature that cannot be verified, GitHub marks the commit or tag as unverified. You can check the verification status of your signed commits or tags on GitHub and view why your commit signatures might be unverified.","title":"Setup commit signing"},{"location":"local-development-setup/sign-commits/#setup","text":"Follow GitHub's instructions for setting up and configuring a GPG key. Creating a new GPG key Adding a new GPG key to your GitHub account Setup Git to use new GPG key","title":"Setup"},{"location":"local-development-setup/sign-commits/#signing-commits-via-vs-code","text":"To turn on signed commits in the VS Code UI, make sure the Enable commit signing with GPG or X.509 is ticked. For the JSON version of the settings use \"git.enableCommitSigning\": true","title":"Signing commits via VS code"},{"location":"z_content-to-be-revised/guides/","text":"Guides These guides help provide additional context to support our standards. Contents Accounts Application Insights Availability Alerts Backlog management Collaboration Debug .NET Core in a Linux container Demo service Developing and debugging in a container Google Tag Manager Jenkins Joiners, movers and leavers Kubernetes AAD Pod Identity Configure NGINX Ingress Controller Creating a workstream namespace Install Kubernetes Dashboard Interact with cluster Probes Managing vulnerabilities Microservice resource provisioning CI pipeline Managed Identity Postgres database Release pipeline Service Bus queues, topics and subscriptions Pact Broker Redis caching in Hapi Secrets management Shared assets VS Code and WSL1 WAF waivers Troubleshooting guide","title":"Guides"},{"location":"z_content-to-be-revised/guides/#guides","text":"These guides help provide additional context to support our standards.","title":"Guides"},{"location":"z_content-to-be-revised/guides/#contents","text":"Accounts Application Insights Availability Alerts Backlog management Collaboration Debug .NET Core in a Linux container Demo service Developing and debugging in a container Google Tag Manager Jenkins Joiners, movers and leavers Kubernetes AAD Pod Identity Configure NGINX Ingress Controller Creating a workstream namespace Install Kubernetes Dashboard Interact with cluster Probes Managing vulnerabilities Microservice resource provisioning CI pipeline Managed Identity Postgres database Release pipeline Service Bus queues, topics and subscriptions Pact Broker Redis caching in Hapi Secrets management Shared assets VS Code and WSL1 WAF waivers Troubleshooting guide","title":"Contents"},{"location":"z_content-to-be-revised/guides/accounts/","text":"Accounts Azure Portal To access Azure portal use the account prefixed with a- and suffixed with defra.onmicrosoft.com Jenkins To access Jenkins use the account prefixed with a- and suffixed with dtz.local Open VPN OpenVPN can be setup with either of the above or your AD account, ie defra.gov.uk","title":"Accounts"},{"location":"z_content-to-be-revised/guides/accounts/#accounts","text":"","title":"Accounts"},{"location":"z_content-to-be-revised/guides/accounts/#azure-portal","text":"To access Azure portal use the account prefixed with a- and suffixed with defra.onmicrosoft.com","title":"Azure Portal"},{"location":"z_content-to-be-revised/guides/accounts/#jenkins","text":"To access Jenkins use the account prefixed with a- and suffixed with dtz.local","title":"Jenkins"},{"location":"z_content-to-be-revised/guides/accounts/#open-vpn","text":"OpenVPN can be setup with either of the above or your AD account, ie defra.gov.uk","title":"Open VPN"},{"location":"z_content-to-be-revised/guides/availability-alerts/","text":"Availability Alert Setup Guide Availability alerts send web request to an application at regular intervals from various different Azure region around the world. It will alert a set Action Group if an application isn't responding, or if it doesn't respond within a given timescale. The simplest test is a URL ping. A URL ping relys on a public accessible URL. How to add an availability alert The availability alerts are setup through the Azure Portal on the subscription where alerting is required. Select an Application Insight resource Click on the \"Availibility\" option under \"Investigate\" within the navigation pane. Within the \"Create Test\" pane 3.1. Within \"Test name\" add a name for the test 3.2. Select \"URL ping test\" for the \"Test type\" 3.3. Within URL add the URL for the web page you would like to test 3.4. Select Parse dependent requests. This will include any dependencies (images, scripts, style files etc...) and test will fail if any these resources cannot be found. The response time will also include the loading of these dependecies. 3.5. Select \"Enable retries for availability test failures\". This will ensure if a test fails, it is retried after a short interval. 3.6. Select a \"Test Frequency\" 3.7. Use the default 5 location(s) for the \"Test locations\". These are the Azure regions where the ping tests will send web requests from. 3.8. Add the required \"Success criteria\" 3.9. Click \"Create\" After a few minutes, click \"Refreh\" to see your test results To setup \"Action Groups\" for your alerts. Click on the elipseat the side of the test, to open the menu Click on \"Open Rules (Alerts) page Within the \"Rules Management\" screen, Click on the Rule for the Availability Alert Under \"Actions\" Click on \"Manage action groups\" Within the \"Select an action group...\" pane, Click \"Create action group\" 9.1. Under \"Basics\" and \"Instance Details\" add an \"Action name\" and a \"Display name\" 9.2. Click \"Next:Notifications > \" 9.3. Under \"Notifications\" Select \"Email/SMS message/Push/Voice\". Within the pane, select \"Email\" and enter an email address. Click \"Ok\". 9.4. Click \"Review + create\" 9.5. Click \"Create\" Reference https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability","title":"Availability Alert Setup Guide"},{"location":"z_content-to-be-revised/guides/availability-alerts/#availability-alert-setup-guide","text":"Availability alerts send web request to an application at regular intervals from various different Azure region around the world. It will alert a set Action Group if an application isn't responding, or if it doesn't respond within a given timescale. The simplest test is a URL ping. A URL ping relys on a public accessible URL.","title":"Availability Alert Setup Guide"},{"location":"z_content-to-be-revised/guides/availability-alerts/#how-to-add-an-availability-alert","text":"The availability alerts are setup through the Azure Portal on the subscription where alerting is required. Select an Application Insight resource Click on the \"Availibility\" option under \"Investigate\" within the navigation pane. Within the \"Create Test\" pane 3.1. Within \"Test name\" add a name for the test 3.2. Select \"URL ping test\" for the \"Test type\" 3.3. Within URL add the URL for the web page you would like to test 3.4. Select Parse dependent requests. This will include any dependencies (images, scripts, style files etc...) and test will fail if any these resources cannot be found. The response time will also include the loading of these dependecies. 3.5. Select \"Enable retries for availability test failures\". This will ensure if a test fails, it is retried after a short interval. 3.6. Select a \"Test Frequency\" 3.7. Use the default 5 location(s) for the \"Test locations\". These are the Azure regions where the ping tests will send web requests from. 3.8. Add the required \"Success criteria\" 3.9. Click \"Create\" After a few minutes, click \"Refreh\" to see your test results To setup \"Action Groups\" for your alerts. Click on the elipseat the side of the test, to open the menu Click on \"Open Rules (Alerts) page Within the \"Rules Management\" screen, Click on the Rule for the Availability Alert Under \"Actions\" Click on \"Manage action groups\" Within the \"Select an action group...\" pane, Click \"Create action group\" 9.1. Under \"Basics\" and \"Instance Details\" add an \"Action name\" and a \"Display name\" 9.2. Click \"Next:Notifications > \" 9.3. Under \"Notifications\" Select \"Email/SMS message/Push/Voice\". Within the pane, select \"Email\" and enter an email address. Click \"Ok\". 9.4. Click \"Review + create\" 9.5. Click \"Create\"","title":"How to add an availability alert"},{"location":"z_content-to-be-revised/guides/availability-alerts/#reference","text":"https://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability","title":"Reference"},{"location":"z_content-to-be-revised/guides/backlog-management/","text":"Backlog management Sizing tickets Teams may find it beneficial to score tickets to help plan their sprints and calculate delivery estimates. There are several techniques that could be applied to aide with this including: - Scrum Poker - T-shirt sizing - Large, uncertain, small - The Bucket system Board format Whether following a Scrum or Kanban approach, teams should have a board to support agile delivery. A board helps team visualise, prioritise and plan their work. As well as a virtual board in Jira it would be advantageous to have a physical board mirroring the content. The columns within the board should be decided by the team and integrated upon if opportunities to improve delivery are identified. The below represents a good starting position for a board. Backlog Issues that need refinement. Scrum teams may find it better to omit this column and only include issues that are in the current sprint on the board. To do Refined issues with full description and acceptance criteria. The team have agreed that these stories are a current priority and are ready to be worked on. In progress Issues currently being worked on. These issues should be assigned to a team member. Team members should work on one story at a time where possible to manage Work in Progress (WIP). Kanban teams may impose a WIP limit to control the throughput of work. Review Issues that have been completed by the team and are ready for review. The review be undertaken by another member of the team who confirms that all acceptance criteria has been met and the issue is done. For code based issues, this review is likely to be a code review . It is the responsibility of the person who worked on the task to find a reviewer. The issue should be reassigned to the reviewer who becomes the new owner. Teams should not pick up new issues if there are outstanding issues to review to prevent a bottleneck. Done Issues that have been delivered and successfully reviewed.","title":"Backlog management"},{"location":"z_content-to-be-revised/guides/backlog-management/#backlog-management","text":"","title":"Backlog management"},{"location":"z_content-to-be-revised/guides/backlog-management/#sizing-tickets","text":"Teams may find it beneficial to score tickets to help plan their sprints and calculate delivery estimates. There are several techniques that could be applied to aide with this including: - Scrum Poker - T-shirt sizing - Large, uncertain, small - The Bucket system","title":"Sizing tickets"},{"location":"z_content-to-be-revised/guides/backlog-management/#board-format","text":"Whether following a Scrum or Kanban approach, teams should have a board to support agile delivery. A board helps team visualise, prioritise and plan their work. As well as a virtual board in Jira it would be advantageous to have a physical board mirroring the content. The columns within the board should be decided by the team and integrated upon if opportunities to improve delivery are identified. The below represents a good starting position for a board.","title":"Board format"},{"location":"z_content-to-be-revised/guides/backlog-management/#backlog","text":"Issues that need refinement. Scrum teams may find it better to omit this column and only include issues that are in the current sprint on the board.","title":"Backlog"},{"location":"z_content-to-be-revised/guides/backlog-management/#to-do","text":"Refined issues with full description and acceptance criteria. The team have agreed that these stories are a current priority and are ready to be worked on.","title":"To do"},{"location":"z_content-to-be-revised/guides/backlog-management/#in-progress","text":"Issues currently being worked on. These issues should be assigned to a team member. Team members should work on one story at a time where possible to manage Work in Progress (WIP). Kanban teams may impose a WIP limit to control the throughput of work.","title":"In progress"},{"location":"z_content-to-be-revised/guides/backlog-management/#review","text":"Issues that have been completed by the team and are ready for review. The review be undertaken by another member of the team who confirms that all acceptance criteria has been met and the issue is done. For code based issues, this review is likely to be a code review . It is the responsibility of the person who worked on the task to find a reviewer. The issue should be reassigned to the reviewer who becomes the new owner. Teams should not pick up new issues if there are outstanding issues to review to prevent a bottleneck.","title":"Review"},{"location":"z_content-to-be-revised/guides/backlog-management/#done","text":"Issues that have been delivered and successfully reviewed.","title":"Done"},{"location":"z_content-to-be-revised/guides/debug-dotnet-container/","text":"Debugging .NET Core in a Linux container .NET Core services can be developed using VS Code or Visual Studio. As all FFC services are designed to be developed and run in Linux containers, debugging them requires the attachement of a debugger from the running container. In the case of .NET Core, this is dependent on a remote debugger being present in the container image. All FFC services based on the Defra .NET Core development image include the vsdbg remote debugger. Attaching to the remote debugger VS Code add your breakpoint start the container with docker-compose up --build from the repository root directory in VS Code create a launch.json configuration similar to the below substituting the name of the container, ffc-demo-payment-service-core { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \".NET Core Docker Attach\", \"type\": \"coreclr\", \"request\": \"attach\", \"processId\": \"${command:pickRemoteProcess}\", \"pipeTransport\": { \"pipeProgram\": \"docker\", \"pipeArgs\": [ \"exec\", \"-i\", \"ffc-demo-payment-service-core\" ], \"debuggerPath\": \"/vsdbg/vsdbg\", \"pipeCwd\": \"${workspaceRoot}\", \"quoteArgs\": false }, \"sourceFileMap\": { \"/home/dotnet\": \"${workspaceFolder}\" } } ] } start the VS Code debugger using this launch configuration in the context menu, select the process matching the running application, eg. FFCDemoPaymentService the breakpoint can now be hit within VS Code Visual Studio Visual Studio does not integrate with the WSL filesystem, so WSL users must clone the repository in Windows to debug using Visual Studio. It is important that the following git configuration setting is present to ensure that cloning in Windows does not alter existing line endings git config --global core.autocrlf input For services which require environment variables to be read from the host, it is recommended to store these in a .env file in the repository as Docker Compose will automatically read this file when running the container. This file must be excluded from source control. This process has a prerequisite of the user having Docker Desktop installed which includes Docker Compose by default. add your break point using Powershell, start the container with docker-compose up --build from the repository root directory in Visual Studio, select Debug -> Attach to process select Docker (Linux Container) for Connection type type the name of the container in Connection target , eg. ffc-demo-payment-service click Refresh select process matching running application, eg FFCDemoPaymentService click Attach select Managed (.NET Core for Unix) code type click Ok the breakpoint can now be hit within Visual Studio Note volume mounts do not appear to work with this approach, so for changes to be picked up, the container will need to be recreated.","title":"Debugging .NET Core in a Linux container"},{"location":"z_content-to-be-revised/guides/debug-dotnet-container/#debugging-net-core-in-a-linux-container","text":".NET Core services can be developed using VS Code or Visual Studio. As all FFC services are designed to be developed and run in Linux containers, debugging them requires the attachement of a debugger from the running container. In the case of .NET Core, this is dependent on a remote debugger being present in the container image. All FFC services based on the Defra .NET Core development image include the vsdbg remote debugger.","title":"Debugging .NET Core in a Linux container"},{"location":"z_content-to-be-revised/guides/debug-dotnet-container/#attaching-to-the-remote-debugger","text":"","title":"Attaching to the remote debugger"},{"location":"z_content-to-be-revised/guides/debug-dotnet-container/#vs-code","text":"add your breakpoint start the container with docker-compose up --build from the repository root directory in VS Code create a launch.json configuration similar to the below substituting the name of the container, ffc-demo-payment-service-core { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \".NET Core Docker Attach\", \"type\": \"coreclr\", \"request\": \"attach\", \"processId\": \"${command:pickRemoteProcess}\", \"pipeTransport\": { \"pipeProgram\": \"docker\", \"pipeArgs\": [ \"exec\", \"-i\", \"ffc-demo-payment-service-core\" ], \"debuggerPath\": \"/vsdbg/vsdbg\", \"pipeCwd\": \"${workspaceRoot}\", \"quoteArgs\": false }, \"sourceFileMap\": { \"/home/dotnet\": \"${workspaceFolder}\" } } ] } start the VS Code debugger using this launch configuration in the context menu, select the process matching the running application, eg. FFCDemoPaymentService the breakpoint can now be hit within VS Code","title":"VS Code"},{"location":"z_content-to-be-revised/guides/debug-dotnet-container/#visual-studio","text":"Visual Studio does not integrate with the WSL filesystem, so WSL users must clone the repository in Windows to debug using Visual Studio. It is important that the following git configuration setting is present to ensure that cloning in Windows does not alter existing line endings git config --global core.autocrlf input For services which require environment variables to be read from the host, it is recommended to store these in a .env file in the repository as Docker Compose will automatically read this file when running the container. This file must be excluded from source control. This process has a prerequisite of the user having Docker Desktop installed which includes Docker Compose by default. add your break point using Powershell, start the container with docker-compose up --build from the repository root directory in Visual Studio, select Debug -> Attach to process select Docker (Linux Container) for Connection type type the name of the container in Connection target , eg. ffc-demo-payment-service click Refresh select process matching running application, eg FFCDemoPaymentService click Attach select Managed (.NET Core for Unix) code type click Ok the breakpoint can now be hit within Visual Studio Note volume mounts do not appear to work with this approach, so for changes to be picked up, the container will need to be recreated.","title":"Visual Studio"},{"location":"z_content-to-be-revised/guides/demo-service/","text":"Demo service The FFC Demo service is a mock digital service to both prove Platform capability and act as an examplar service. This allows delivery teams see working examples of patterns and standards in practice and replicate them within their own services. Service context The demo service is built around the premise of claiming financial aid in the event property subsides into a mine shaft. This is an entirely fictional scenario and no Defra service providing this capabililty exists. The demo service is made up of seven microservices, five Node.js and one .NET Core. There is also a respository that supports local development across all seven microservices. Web service Front end web application where user follows a claim creation journey and submits a new claim for payment. Claim data is cached using Redis between questions and submitted to an Azure Service Bus queue for the Claim service. Source code Claim service Receives a new claim submission and persists claim data in a PostgreSQL database. Publishes new claim validated message to two Azure Service Bus queues for the Payment and Calculation services. Source code Calculation service Receives a valid claim message and calculates a monetary value based on claim data. Publishes claim value to Azure Service Bus queue for Payment Service. Source code Payment service Generates a payment schedule from valid claim message and persists in a PostgreSQL database. Also persists associated payment value received from calcuated message. Both a Node.js and .NET Core version of this service exist. Source code (Node.js) Source code (.NET Core) Payment web service Front end web application for internal users. Requests payment schedule data from Payment service via HTTP (Node.js only). Source code Collector service Consumes payment update events from Azure Event Hub and persists in a PostgreSQL database. Source code Local development orchestration Easy running and debugging of all demo services locally within single Docker network. Source code Patterns and standards The demo service is built following FFC standards and patterns. However, for reasons of time constraint, not every service has every pattern in place. Below is a reference of where patterns and standards can be found. Directory structure All Tests Unit Web Payment (Node.js) Payment (.NET Core) Calculation Payment web Collector Narrow integration Web Payment (Node.js) Calculation Payment web Local integration Web - Redis, Azure Service Bus Claim - Azure Service Bus, PostgreSQL Payment (Node.js) - Azure Service Bus, PostgreSQL Calculation - Azure Service Bus Collector - Azure Event Hub Contract - Provider Async Claim Contract - Consumer Async Payment (Node.js) Contract - Provider HTTP Payment (Node.js) Contract - Consumer HTTP Payment web Acceptance Web Security Web Payment web Accessibility Web .NET Core Snyk setup Payment (.NET Core) Docker All Docker Compose All FFC CI Pipeline Jenkinsfile All Liquibase database migrations Claim Payment (Node.js) Payment (.NET Core) Collector Messaging Send message to queue Web Claim Calculation Receive message from queue Claim Calculation Payment (Node.js) Payment (.NET Core) GOV.UK Notify Claim Outbox pattern Claim Dead lettering Calcuation Completing message Claim Calculation Payment (Node.js) Payment (.NET Core) Abandon message Claim Calculation Payment (Node.js) Payment (.NET Core) Webpack Web Payment web Application Insights All Horizontal pod autoscaling Calculation Events Publishing event Payment Consuming event Collector","title":"Demo service"},{"location":"z_content-to-be-revised/guides/demo-service/#demo-service","text":"The FFC Demo service is a mock digital service to both prove Platform capability and act as an examplar service. This allows delivery teams see working examples of patterns and standards in practice and replicate them within their own services.","title":"Demo service"},{"location":"z_content-to-be-revised/guides/demo-service/#service-context","text":"The demo service is built around the premise of claiming financial aid in the event property subsides into a mine shaft. This is an entirely fictional scenario and no Defra service providing this capabililty exists. The demo service is made up of seven microservices, five Node.js and one .NET Core. There is also a respository that supports local development across all seven microservices.","title":"Service context"},{"location":"z_content-to-be-revised/guides/demo-service/#web-service","text":"Front end web application where user follows a claim creation journey and submits a new claim for payment. Claim data is cached using Redis between questions and submitted to an Azure Service Bus queue for the Claim service. Source code","title":"Web service"},{"location":"z_content-to-be-revised/guides/demo-service/#claim-service","text":"Receives a new claim submission and persists claim data in a PostgreSQL database. Publishes new claim validated message to two Azure Service Bus queues for the Payment and Calculation services. Source code","title":"Claim service"},{"location":"z_content-to-be-revised/guides/demo-service/#calculation-service","text":"Receives a valid claim message and calculates a monetary value based on claim data. Publishes claim value to Azure Service Bus queue for Payment Service. Source code","title":"Calculation service"},{"location":"z_content-to-be-revised/guides/demo-service/#payment-service","text":"Generates a payment schedule from valid claim message and persists in a PostgreSQL database. Also persists associated payment value received from calcuated message. Both a Node.js and .NET Core version of this service exist. Source code (Node.js) Source code (.NET Core)","title":"Payment service"},{"location":"z_content-to-be-revised/guides/demo-service/#payment-web-service","text":"Front end web application for internal users. Requests payment schedule data from Payment service via HTTP (Node.js only). Source code","title":"Payment web service"},{"location":"z_content-to-be-revised/guides/demo-service/#collector-service","text":"Consumes payment update events from Azure Event Hub and persists in a PostgreSQL database. Source code","title":"Collector service"},{"location":"z_content-to-be-revised/guides/demo-service/#local-development-orchestration","text":"Easy running and debugging of all demo services locally within single Docker network. Source code","title":"Local development orchestration"},{"location":"z_content-to-be-revised/guides/demo-service/#patterns-and-standards","text":"The demo service is built following FFC standards and patterns. However, for reasons of time constraint, not every service has every pattern in place. Below is a reference of where patterns and standards can be found.","title":"Patterns and standards"},{"location":"z_content-to-be-revised/guides/demo-service/#directory-structure","text":"All","title":"Directory structure"},{"location":"z_content-to-be-revised/guides/demo-service/#tests","text":"","title":"Tests"},{"location":"z_content-to-be-revised/guides/demo-service/#unit","text":"Web Payment (Node.js) Payment (.NET Core) Calculation Payment web Collector","title":"Unit"},{"location":"z_content-to-be-revised/guides/demo-service/#narrow-integration","text":"Web Payment (Node.js) Calculation Payment web","title":"Narrow integration"},{"location":"z_content-to-be-revised/guides/demo-service/#local-integration","text":"Web - Redis, Azure Service Bus Claim - Azure Service Bus, PostgreSQL Payment (Node.js) - Azure Service Bus, PostgreSQL Calculation - Azure Service Bus Collector - Azure Event Hub","title":"Local integration"},{"location":"z_content-to-be-revised/guides/demo-service/#contract-provider-async","text":"Claim","title":"Contract - Provider Async"},{"location":"z_content-to-be-revised/guides/demo-service/#contract-consumer-async","text":"Payment (Node.js)","title":"Contract - Consumer Async"},{"location":"z_content-to-be-revised/guides/demo-service/#contract-provider-http","text":"Payment (Node.js)","title":"Contract - Provider HTTP"},{"location":"z_content-to-be-revised/guides/demo-service/#contract-consumer-http","text":"Payment web","title":"Contract - Consumer HTTP"},{"location":"z_content-to-be-revised/guides/demo-service/#acceptance","text":"Web","title":"Acceptance"},{"location":"z_content-to-be-revised/guides/demo-service/#security","text":"Web Payment web","title":"Security"},{"location":"z_content-to-be-revised/guides/demo-service/#accessibility","text":"Web","title":"Accessibility"},{"location":"z_content-to-be-revised/guides/demo-service/#net-core-snyk-setup","text":"Payment (.NET Core)","title":".NET Core Snyk setup"},{"location":"z_content-to-be-revised/guides/demo-service/#docker","text":"All","title":"Docker"},{"location":"z_content-to-be-revised/guides/demo-service/#docker-compose","text":"All","title":"Docker Compose"},{"location":"z_content-to-be-revised/guides/demo-service/#ffc-ci-pipeline-jenkinsfile","text":"All","title":"FFC CI Pipeline Jenkinsfile"},{"location":"z_content-to-be-revised/guides/demo-service/#liquibase-database-migrations","text":"Claim Payment (Node.js) Payment (.NET Core) Collector","title":"Liquibase database migrations"},{"location":"z_content-to-be-revised/guides/demo-service/#messaging","text":"","title":"Messaging"},{"location":"z_content-to-be-revised/guides/demo-service/#send-message-to-queue","text":"Web Claim Calculation","title":"Send message to queue"},{"location":"z_content-to-be-revised/guides/demo-service/#receive-message-from-queue","text":"Claim Calculation Payment (Node.js) Payment (.NET Core)","title":"Receive message from queue"},{"location":"z_content-to-be-revised/guides/demo-service/#govuk-notify","text":"Claim","title":"GOV.UK Notify"},{"location":"z_content-to-be-revised/guides/demo-service/#outbox-pattern","text":"Claim","title":"Outbox pattern"},{"location":"z_content-to-be-revised/guides/demo-service/#dead-lettering","text":"Calcuation","title":"Dead lettering"},{"location":"z_content-to-be-revised/guides/demo-service/#completing-message","text":"Claim Calculation Payment (Node.js) Payment (.NET Core)","title":"Completing message"},{"location":"z_content-to-be-revised/guides/demo-service/#abandon-message","text":"Claim Calculation Payment (Node.js) Payment (.NET Core)","title":"Abandon message"},{"location":"z_content-to-be-revised/guides/demo-service/#webpack","text":"Web Payment web","title":"Webpack"},{"location":"z_content-to-be-revised/guides/demo-service/#application-insights","text":"All","title":"Application Insights"},{"location":"z_content-to-be-revised/guides/demo-service/#horizontal-pod-autoscaling","text":"Calculation","title":"Horizontal pod autoscaling"},{"location":"z_content-to-be-revised/guides/demo-service/#events","text":"","title":"Events"},{"location":"z_content-to-be-revised/guides/demo-service/#publishing-event","text":"Payment","title":"Publishing event"},{"location":"z_content-to-be-revised/guides/demo-service/#consuming-event","text":"Collector","title":"Consuming event"},{"location":"z_content-to-be-revised/guides/developing-in-container/","text":"Developing in a container The FFC Architecture Vision prescribes a containerised microservice ecosystem. Docker is the containerisation technology used within Defra. Containers are lightweight and fast. One of their main benefits for developers is that it is simple to replicate an application\u2019s environment and dependencies locally consistently. Crucially, they enable a workflow for your code that allows you to develop and test locally, push to upstream, and be confident that what you have built locally will work in CI and any environment. The following guide has a strong focus on Node.js as it is the primary technology choice of FFC and Defra. However, details of debugging .NET containers is included in another guide . Docker All FFC microservices are built from supported Defra parent images for Node.js and .NET. A Dockerfile will be included in each microservice repository containing a multi-stage build definition referencing these images. All microservice repositories created from the FFC Node template will include this Dockerfile already configured. Docker Compose Docker Compose is a tool for defining and running multi-container Docker applications using yaml configuration files. All microservice repositories created from the FFC Node template include a pre-configured set of Docker Compose yaml files to support local development and testing as well as some being a prerequisite for CI capability. An example Node.js Dockerfile showing a multi-stage build for both development and production. Note that the development image runs the application in watch mode to support local development and testing, whilst production simply runs the application. development is dependent on the local package.json including a watch script. More on this below. ARG PARENT_VERSION=1.2.9-node14.17.6 ARG PORT=3000 ARG PORT_DEBUG=9229 # Development FROM defradigital/node-development:${PARENT_VERSION} AS development ARG PARENT_VERSION LABEL uk.gov.defra.ffc.parent-image=defradigital/node-development:${PARENT_VERSION} ARG PORT ARG PORT_DEBUG ENV PORT ${PORT} EXPOSE ${PORT} ${PORT_DEBUG} COPY --chown=node:node package*.json ./ RUN npm install COPY --chown=node:node . . CMD [ \"npm\", \"run\", \"start:watch\" ] # Production FROM defradigital/node:${PARENT_VERSION} AS production ARG PARENT_VERSION LABEL uk.gov.defra.ffc.parent-image=defradigital/node:${PARENT_VERSION} ARG PORT ENV PORT ${PORT} EXPOSE ${PORT} COPY --from=development /home/node/app/ ./app/ COPY --from=development /home/node/package*.json ./ RUN npm ci CMD [ \"node\", \"app\" ] docker-compose.yaml Used to define creation of the production image locally and in CI. This file should include all configuration needed to create a clean production image. Port and local volume mapping should be avoided in this file. The template repository will set a container_name property in this file so that containers created have shorter and more predictable names to support local development. However, if local scaling of container instances is required, then this property should be removed as container names will need to be dynamic in that scenario. To avoid duplication, other dependent container images can be defined in this file such as PostgreSQL or Redis, but no volume or port bindings for those dependencies should be included. docker-compose.override.yaml Used to apply overrides to docker-compose.yaml to support local development. This is where port and volume mappings should be declared. If dependencies such as PostgreSQL or Redis are used, this is the file where volume and port bindings should be declared for those dependencies. This image will build the development image which typically is the same as production but will run the code in watch mode so changes made to the code locally are automatically picked up in the container and restart the application. Avoiding port conflicts When binding container ports to localhost, it is important to consider any conflicts that may occur with other services developers may wish to run locally. For example, if a service is made up of two microservices, both running on port 3000 . Then both cannot be mapped to localhost:3000 without a conflict. In this scenario, to successfully run both services on the same device with port binding, one of the services should bind the container's port 3000 to a different localhost port. The same consideration should be given to the debug port exposed to avoid a conflict on port 9229 , the default Node debug port. # service 1 docker-compose.override.yaml ports: - \"3000:3000\" - \"9229:9229\" # service 2 docker-compose.override.yaml ports: - \"3001:3000\" - \"9230:9229\" This equally applies when binding dependency images such as PostgreSQL and Redis. docker-compose.debug.yaml Used to start application in debug mode. This is only required if you wish the application to wait for the debugger before starting the application. If you just wish to attach a debugger to an already running instance, the override file is sufficient. docker-compose.test.yaml Used to run all tests in the repository. This is a dependency of the FFC CI pipeline. Port bindings should be avoided in this file to avoid conflicts between running builds. docker-compose.test.watch.yaml Used as an override to docker-compose.test.yaml to run tests in watch mode to support Test Driven Development (TDD). Changes to either application or test code will automatically trigger re-running of affected tests. All Node.js FFC microservices use Jest . Jest has powerful capability to support multiple watch scenarios such as running individual tests, only tests affected by changes, only failed tests, filtering by regular expression as well as running the full suite. In order to understand which code has changed, Jest uses the local .git directory. This means when running tests in a container, the local .git folder must be mounted to the container in docker-compose.watch.yaml . volumes: - ./.git:/home/node/.git docker-compose.test.debug.yaml Used to run Jest in watch mode but support debugging of tests. This allows developers to have all the capability of watch but with the added bonus of being able to attach a debugger. Details of debugging tests are below. package.json scripts To enable the capability provided by the above Docker Compose files, package.json needs to be configured to support the scripts referenced in the command . Below is an extract of the default package.json file provided by FFC Node template . \"scripts\": { \"pretest\": \"npm run test:lint\", \"test\": \"jest --runInBand --forceExit\", \"test:watch\": \"jest --coverage=false --onlyChanged --watch --runInBand\", \"test:debug\": \"node --inspect-brk=0.0.0.0 ./node_modules/jest/bin/jest.js --coverage=false --onlyChanged --watch --runInBand --no-cache\", \"test:lint\": \"standard\", \"start:watch\": \"nodemon --inspect=0.0.0.0 --ext js --legacy-watch app/index.js\", \"start:debug\": \"nodemon --inspect-brk=0.0.0.0 --ext js --legacy-watch app/index.js\" pretest This will automatically run before the test script and will lint all JavaScript files in according with StandardJs standards. test This will run all Jest tests within the repository with no watch mode enabled and will output code coverage results on test completion. This is primary used for CI, but can be run locally as a quick check of test status. --runInBand will ensure that tests run sequentially rather than parallel. Although this will result in slower running overall, it means that integration tests spanning containers have connections that are cleanly and predictably open and closed to avoid test disruption. --forceExit will force Jest close a test with open connections 1 second after completion of the test. Ideally this would not be needed, however in some scenarios Jest is unable to determine whether a Hapi server is still running even if it is cleanly shut down in the test. test:watch This will run tests in watch mode and is the most commonly used by developers to support TDD. --coverage=false - as typically only running a subset of tests, there is little value displaying a test coverage summary. Disabling it also reduces lines written to the console to support developer focus. --onlyChanged - start by only running tests that are affected by code changes. Accuracy of this is dependent on the .git folder being mounted to the volume as described above as well as the folders containing test and application code. --watch - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run. Developers have the option to change the behaviour of watch mode. test:watch This will run tests in watch mode and is the most commonly used by developers to support TDD. --coverage=false - as typically only running a subset of tests, there is little value displaying a test coverage summary. Disabling it also reduces lines written to the console to support developer focus. --onlyChanged - start by only running tests that are affected by code changes. Accuracy of this is dependent on the .git folder being mounted to the volume as described above. --watch - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run. Developers have the option to change the behaviour of watch mode. test:debug This runs tests in watch mode and has all the same behaviour and running options as test:watch . However, the key difference it this will wait for a debugger to be attached before starting test execution. This enables developers to apply breakpoints in the test and application code to debug troublesome tests. An example Visual Studio Code debugging profile to use this script is provided below. --no-cache - Jest caches files between test runs which can result in some breakpoints not being hit. This disables that behaviour. test:lint Run linting with StandardJs only. start:watch Starts the application in watch mode. This is typically how developers will run all applications locally. As code is changed, the running application in the container automatically identifies the changes and restarts the running application. Nodemon is used to orchestrate restarting of the application. This is dependent on the application code folder having a volume binding to the container. start:debug This has the same behaviour as start:watch but like test:debug will wait for a debugger to be attached before executing any code. Convenience scripts Repositories created from FFC Node template will include two convenience scripts to support developers easily running the application utilising the above setup. ./scripts/start Run the application using Docker Compose. Typically this is just a simple abstraction over docker-compose up however, is can be extended to ensure container runs in a specific container network or run database migrations prior to starting the application for example. ./scripts/test Run tests. Without any arguments provided will run the test script in package.json . To ensure clean running, all test containers are recreated. Note that this will not affect data persisted in development databases for example if the above setup is followed. Optional arguments -h - shows all available arguments. -w - runs tests in watch mode using test:watch script -d - runs tests in debug mode using test:debug script Debugging code running in a container If the above setup is followed, then everything is in place to support debugging of applications and tests in containers. Developers are free to use their own choice of IDE, however, all example debug configurations within this guide will assume Visual Studio Code is used. These debug configurations should all be included in a launch.json file in the .vscode folder at the root of the repository. This folder should be excluded from source control. Application debugging profiles Attach to an already running container { \"name\": \"Docker: Attach\", \"type\": \"node\", \"request\": \"attach\", \"restart\": true, \"port\": 9229, \"remoteRoot\": \"/home/node\", \"skipFiles\": [ \"<node_internals>/**\", \"**/node_modules/**\" ] } This will attach to the node process exposed by the debug port. Note that this uses the localhost port not the container port. So if port 9229 is bound to a different port locally, then this value should be changed to match here. restart - will ensure that as code is changed and the application restarted, the debugger is automatically reattached. remoteRoot - must match the location of the code that matches the local workspace structure. When using the Node.js Defra Docker base images the location will always be home/node . skipFiles - an array of locations where debugging such skip. Typically this would be internal Node.js code as well as those from third party npm modules. Start an application in debug mode { \"name\": \"Docker: Attach Launch\", \"type\": \"node\", \"request\": \"attach\", \"remoteRoot\": \"/home/node\", \"restart\": true, \"port\": 9229, \"skipFiles\": [ \"<node_internals>/**\", \"**/node_modules/**\" ], \"preLaunchTask\": \"compose-debug-up\", \"postDebugTask\": \"compose-debug-down\" }, This will start a new container in debug mode using the start:debug package.json script. The application will wait for a debugger before running any code. This is dependent on preLaunchTask and postDebugTask being defined in a .vscode/tasks.json file. An example of this is below. { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"compose-debug-up\", \"type\": \"shell\", \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml up -d\" }, { \"label\": \"compose-debug-down\", \"type\": \"shell\", \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml down\" } ] } Test debugging { \"name\": \"Docker: Jest Attach\", \"type\": \"node\", \"request\": \"attach\", \"port\": 9229, \"restart\": true, \"timeout\": 10000, \"remoteRoot\": \"/home/node\", \"disableOptimisticBPs\": true, \"continueOnAttach\": true, \"skipFiles\": [ \"<node_internals>/**\", \"**/node_modules/**\" ] } This assumes that the ./script/test -d command referenced above has already been run and the test suit is waiting for the debugger to attach. This profile will attach that debugger. disableOptimisticBPs - this needs to be set as true as Jest takes a copy of all test files and uses these copies for execution. If this is not disabled then this process can result in breakpoints not being mapped to their correct location. continueOnAttach - instruct the pending test execution to continue once the debugger is attached. Other debugging profiles A range of different debugging profiles can be found in this repository as well as a test application setup to the above standards to test them. Other useful Docker development guides Defra has well documented standards and guidance on developing with containers, that provides further examples of good local development practice.","title":"Developing in a container"},{"location":"z_content-to-be-revised/guides/developing-in-container/#developing-in-a-container","text":"The FFC Architecture Vision prescribes a containerised microservice ecosystem. Docker is the containerisation technology used within Defra. Containers are lightweight and fast. One of their main benefits for developers is that it is simple to replicate an application\u2019s environment and dependencies locally consistently. Crucially, they enable a workflow for your code that allows you to develop and test locally, push to upstream, and be confident that what you have built locally will work in CI and any environment. The following guide has a strong focus on Node.js as it is the primary technology choice of FFC and Defra. However, details of debugging .NET containers is included in another guide .","title":"Developing in a container"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker","text":"All FFC microservices are built from supported Defra parent images for Node.js and .NET. A Dockerfile will be included in each microservice repository containing a multi-stage build definition referencing these images. All microservice repositories created from the FFC Node template will include this Dockerfile already configured.","title":"Docker"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-compose","text":"Docker Compose is a tool for defining and running multi-container Docker applications using yaml configuration files. All microservice repositories created from the FFC Node template include a pre-configured set of Docker Compose yaml files to support local development and testing as well as some being a prerequisite for CI capability. An example Node.js Dockerfile showing a multi-stage build for both development and production. Note that the development image runs the application in watch mode to support local development and testing, whilst production simply runs the application. development is dependent on the local package.json including a watch script. More on this below. ARG PARENT_VERSION=1.2.9-node14.17.6 ARG PORT=3000 ARG PORT_DEBUG=9229 # Development FROM defradigital/node-development:${PARENT_VERSION} AS development ARG PARENT_VERSION LABEL uk.gov.defra.ffc.parent-image=defradigital/node-development:${PARENT_VERSION} ARG PORT ARG PORT_DEBUG ENV PORT ${PORT} EXPOSE ${PORT} ${PORT_DEBUG} COPY --chown=node:node package*.json ./ RUN npm install COPY --chown=node:node . . CMD [ \"npm\", \"run\", \"start:watch\" ] # Production FROM defradigital/node:${PARENT_VERSION} AS production ARG PARENT_VERSION LABEL uk.gov.defra.ffc.parent-image=defradigital/node:${PARENT_VERSION} ARG PORT ENV PORT ${PORT} EXPOSE ${PORT} COPY --from=development /home/node/app/ ./app/ COPY --from=development /home/node/package*.json ./ RUN npm ci CMD [ \"node\", \"app\" ]","title":"Docker Compose"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-composeyaml","text":"Used to define creation of the production image locally and in CI. This file should include all configuration needed to create a clean production image. Port and local volume mapping should be avoided in this file. The template repository will set a container_name property in this file so that containers created have shorter and more predictable names to support local development. However, if local scaling of container instances is required, then this property should be removed as container names will need to be dynamic in that scenario. To avoid duplication, other dependent container images can be defined in this file such as PostgreSQL or Redis, but no volume or port bindings for those dependencies should be included.","title":"docker-compose.yaml"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-composeoverrideyaml","text":"Used to apply overrides to docker-compose.yaml to support local development. This is where port and volume mappings should be declared. If dependencies such as PostgreSQL or Redis are used, this is the file where volume and port bindings should be declared for those dependencies. This image will build the development image which typically is the same as production but will run the code in watch mode so changes made to the code locally are automatically picked up in the container and restart the application.","title":"docker-compose.override.yaml"},{"location":"z_content-to-be-revised/guides/developing-in-container/#avoiding-port-conflicts","text":"When binding container ports to localhost, it is important to consider any conflicts that may occur with other services developers may wish to run locally. For example, if a service is made up of two microservices, both running on port 3000 . Then both cannot be mapped to localhost:3000 without a conflict. In this scenario, to successfully run both services on the same device with port binding, one of the services should bind the container's port 3000 to a different localhost port. The same consideration should be given to the debug port exposed to avoid a conflict on port 9229 , the default Node debug port. # service 1 docker-compose.override.yaml ports: - \"3000:3000\" - \"9229:9229\" # service 2 docker-compose.override.yaml ports: - \"3001:3000\" - \"9230:9229\" This equally applies when binding dependency images such as PostgreSQL and Redis.","title":"Avoiding port conflicts"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-composedebugyaml","text":"Used to start application in debug mode. This is only required if you wish the application to wait for the debugger before starting the application. If you just wish to attach a debugger to an already running instance, the override file is sufficient.","title":"docker-compose.debug.yaml"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-composetestyaml","text":"Used to run all tests in the repository. This is a dependency of the FFC CI pipeline. Port bindings should be avoided in this file to avoid conflicts between running builds.","title":"docker-compose.test.yaml"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-composetestwatchyaml","text":"Used as an override to docker-compose.test.yaml to run tests in watch mode to support Test Driven Development (TDD). Changes to either application or test code will automatically trigger re-running of affected tests. All Node.js FFC microservices use Jest . Jest has powerful capability to support multiple watch scenarios such as running individual tests, only tests affected by changes, only failed tests, filtering by regular expression as well as running the full suite. In order to understand which code has changed, Jest uses the local .git directory. This means when running tests in a container, the local .git folder must be mounted to the container in docker-compose.watch.yaml . volumes: - ./.git:/home/node/.git","title":"docker-compose.test.watch.yaml"},{"location":"z_content-to-be-revised/guides/developing-in-container/#docker-composetestdebugyaml","text":"Used to run Jest in watch mode but support debugging of tests. This allows developers to have all the capability of watch but with the added bonus of being able to attach a debugger. Details of debugging tests are below.","title":"docker-compose.test.debug.yaml"},{"location":"z_content-to-be-revised/guides/developing-in-container/#packagejson-scripts","text":"To enable the capability provided by the above Docker Compose files, package.json needs to be configured to support the scripts referenced in the command . Below is an extract of the default package.json file provided by FFC Node template . \"scripts\": { \"pretest\": \"npm run test:lint\", \"test\": \"jest --runInBand --forceExit\", \"test:watch\": \"jest --coverage=false --onlyChanged --watch --runInBand\", \"test:debug\": \"node --inspect-brk=0.0.0.0 ./node_modules/jest/bin/jest.js --coverage=false --onlyChanged --watch --runInBand --no-cache\", \"test:lint\": \"standard\", \"start:watch\": \"nodemon --inspect=0.0.0.0 --ext js --legacy-watch app/index.js\", \"start:debug\": \"nodemon --inspect-brk=0.0.0.0 --ext js --legacy-watch app/index.js\"","title":"package.json scripts"},{"location":"z_content-to-be-revised/guides/developing-in-container/#pretest","text":"This will automatically run before the test script and will lint all JavaScript files in according with StandardJs standards.","title":"pretest"},{"location":"z_content-to-be-revised/guides/developing-in-container/#test","text":"This will run all Jest tests within the repository with no watch mode enabled and will output code coverage results on test completion. This is primary used for CI, but can be run locally as a quick check of test status. --runInBand will ensure that tests run sequentially rather than parallel. Although this will result in slower running overall, it means that integration tests spanning containers have connections that are cleanly and predictably open and closed to avoid test disruption. --forceExit will force Jest close a test with open connections 1 second after completion of the test. Ideally this would not be needed, however in some scenarios Jest is unable to determine whether a Hapi server is still running even if it is cleanly shut down in the test.","title":"test"},{"location":"z_content-to-be-revised/guides/developing-in-container/#testwatch","text":"This will run tests in watch mode and is the most commonly used by developers to support TDD. --coverage=false - as typically only running a subset of tests, there is little value displaying a test coverage summary. Disabling it also reduces lines written to the console to support developer focus. --onlyChanged - start by only running tests that are affected by code changes. Accuracy of this is dependent on the .git folder being mounted to the volume as described above as well as the folders containing test and application code. --watch - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run. Developers have the option to change the behaviour of watch mode.","title":"test:watch"},{"location":"z_content-to-be-revised/guides/developing-in-container/#testwatch_1","text":"This will run tests in watch mode and is the most commonly used by developers to support TDD. --coverage=false - as typically only running a subset of tests, there is little value displaying a test coverage summary. Disabling it also reduces lines written to the console to support developer focus. --onlyChanged - start by only running tests that are affected by code changes. Accuracy of this is dependent on the .git folder being mounted to the volume as described above. --watch - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run. Developers have the option to change the behaviour of watch mode.","title":"test:watch"},{"location":"z_content-to-be-revised/guides/developing-in-container/#testdebug","text":"This runs tests in watch mode and has all the same behaviour and running options as test:watch . However, the key difference it this will wait for a debugger to be attached before starting test execution. This enables developers to apply breakpoints in the test and application code to debug troublesome tests. An example Visual Studio Code debugging profile to use this script is provided below. --no-cache - Jest caches files between test runs which can result in some breakpoints not being hit. This disables that behaviour.","title":"test:debug"},{"location":"z_content-to-be-revised/guides/developing-in-container/#testlint","text":"Run linting with StandardJs only.","title":"test:lint"},{"location":"z_content-to-be-revised/guides/developing-in-container/#startwatch","text":"Starts the application in watch mode. This is typically how developers will run all applications locally. As code is changed, the running application in the container automatically identifies the changes and restarts the running application. Nodemon is used to orchestrate restarting of the application. This is dependent on the application code folder having a volume binding to the container.","title":"start:watch"},{"location":"z_content-to-be-revised/guides/developing-in-container/#startdebug","text":"This has the same behaviour as start:watch but like test:debug will wait for a debugger to be attached before executing any code.","title":"start:debug"},{"location":"z_content-to-be-revised/guides/developing-in-container/#convenience-scripts","text":"Repositories created from FFC Node template will include two convenience scripts to support developers easily running the application utilising the above setup.","title":"Convenience scripts"},{"location":"z_content-to-be-revised/guides/developing-in-container/#scriptsstart","text":"Run the application using Docker Compose. Typically this is just a simple abstraction over docker-compose up however, is can be extended to ensure container runs in a specific container network or run database migrations prior to starting the application for example.","title":"./scripts/start"},{"location":"z_content-to-be-revised/guides/developing-in-container/#scriptstest","text":"Run tests. Without any arguments provided will run the test script in package.json . To ensure clean running, all test containers are recreated. Note that this will not affect data persisted in development databases for example if the above setup is followed.","title":"./scripts/test"},{"location":"z_content-to-be-revised/guides/developing-in-container/#optional-arguments","text":"-h - shows all available arguments. -w - runs tests in watch mode using test:watch script -d - runs tests in debug mode using test:debug script","title":"Optional arguments"},{"location":"z_content-to-be-revised/guides/developing-in-container/#debugging-code-running-in-a-container","text":"If the above setup is followed, then everything is in place to support debugging of applications and tests in containers. Developers are free to use their own choice of IDE, however, all example debug configurations within this guide will assume Visual Studio Code is used. These debug configurations should all be included in a launch.json file in the .vscode folder at the root of the repository. This folder should be excluded from source control.","title":"Debugging code running in a container"},{"location":"z_content-to-be-revised/guides/developing-in-container/#application-debugging-profiles","text":"","title":"Application debugging profiles"},{"location":"z_content-to-be-revised/guides/developing-in-container/#attach-to-an-already-running-container","text":"{ \"name\": \"Docker: Attach\", \"type\": \"node\", \"request\": \"attach\", \"restart\": true, \"port\": 9229, \"remoteRoot\": \"/home/node\", \"skipFiles\": [ \"<node_internals>/**\", \"**/node_modules/**\" ] } This will attach to the node process exposed by the debug port. Note that this uses the localhost port not the container port. So if port 9229 is bound to a different port locally, then this value should be changed to match here. restart - will ensure that as code is changed and the application restarted, the debugger is automatically reattached. remoteRoot - must match the location of the code that matches the local workspace structure. When using the Node.js Defra Docker base images the location will always be home/node . skipFiles - an array of locations where debugging such skip. Typically this would be internal Node.js code as well as those from third party npm modules.","title":"Attach to an already running container"},{"location":"z_content-to-be-revised/guides/developing-in-container/#start-an-application-in-debug-mode","text":"{ \"name\": \"Docker: Attach Launch\", \"type\": \"node\", \"request\": \"attach\", \"remoteRoot\": \"/home/node\", \"restart\": true, \"port\": 9229, \"skipFiles\": [ \"<node_internals>/**\", \"**/node_modules/**\" ], \"preLaunchTask\": \"compose-debug-up\", \"postDebugTask\": \"compose-debug-down\" }, This will start a new container in debug mode using the start:debug package.json script. The application will wait for a debugger before running any code. This is dependent on preLaunchTask and postDebugTask being defined in a .vscode/tasks.json file. An example of this is below. { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"compose-debug-up\", \"type\": \"shell\", \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml up -d\" }, { \"label\": \"compose-debug-down\", \"type\": \"shell\", \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml down\" } ] }","title":"Start an application in debug mode"},{"location":"z_content-to-be-revised/guides/developing-in-container/#test-debugging","text":"{ \"name\": \"Docker: Jest Attach\", \"type\": \"node\", \"request\": \"attach\", \"port\": 9229, \"restart\": true, \"timeout\": 10000, \"remoteRoot\": \"/home/node\", \"disableOptimisticBPs\": true, \"continueOnAttach\": true, \"skipFiles\": [ \"<node_internals>/**\", \"**/node_modules/**\" ] } This assumes that the ./script/test -d command referenced above has already been run and the test suit is waiting for the debugger to attach. This profile will attach that debugger. disableOptimisticBPs - this needs to be set as true as Jest takes a copy of all test files and uses these copies for execution. If this is not disabled then this process can result in breakpoints not being mapped to their correct location. continueOnAttach - instruct the pending test execution to continue once the debugger is attached.","title":"Test debugging"},{"location":"z_content-to-be-revised/guides/developing-in-container/#other-debugging-profiles","text":"A range of different debugging profiles can be found in this repository as well as a test application setup to the above standards to test them.","title":"Other debugging profiles"},{"location":"z_content-to-be-revised/guides/developing-in-container/#other-useful-docker-development-guides","text":"Defra has well documented standards and guidance on developing with containers, that provides further examples of good local development practice.","title":"Other useful Docker development guides"},{"location":"z_content-to-be-revised/guides/google-tag-manager/","text":"Google Tag Manager Google Tag Manager integration into a future ffc service requires contacting the < TBD > team. They will create the necessary Google Tag Manager containers needed to be setup for the new ffc service for Non-Production and Production. The developer will need to create a google account using their DEFRA email address. The developer can create a Google account without using Gmail here - https://accounts.google.com/SignUpWithoutGmail. Once the < TBD > team have setup up the relevant containers, they will give the developer a GTM code, e.g. GTM-M5YK7JL. They will provide a code for Non-Production and Production environments (containers). They may give the full javascipt code snippets similar to the following: (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M5YK7JL'); The developer only needs to take to GTM code provided by the < TBD > team and replace in the following files for the new ffc service: - helm/< helm chart name >/values.yaml (see example for ffc-demo-web ) * in section - container: * variable - googleTagManagerKey: - docker-compose.yaml (see example for ffc-demo-web ) * in section services: -> < service name >: -> environment: * variable - GOOGLE_TAG_MANAGER_KEY If javascript code snippets are provided by the < TBD > team. Then extract the GTM code from them and replace in the above files. Once the GTM code has been replaced the developer can load and start interaction with the website. Then the developer needs to login to Google Tag Manager with their DEFRA email (created in previous steps). The interactions can then be viewed in Google Analytics and Data Studio to verify everything is working correctly. See DEFRA documentation for more information on Digital Analytics Standards. NOTE: < TBD > references a team who the developer will need to contact, at the moment its still to be determined. So the current point of contact is Daniel Gilbert (email: Daniel.Gilbert@defra.gov.uk)","title":"Google Tag Manager"},{"location":"z_content-to-be-revised/guides/google-tag-manager/#google-tag-manager","text":"Google Tag Manager integration into a future ffc service requires contacting the < TBD > team. They will create the necessary Google Tag Manager containers needed to be setup for the new ffc service for Non-Production and Production. The developer will need to create a google account using their DEFRA email address. The developer can create a Google account without using Gmail here - https://accounts.google.com/SignUpWithoutGmail. Once the < TBD > team have setup up the relevant containers, they will give the developer a GTM code, e.g. GTM-M5YK7JL. They will provide a code for Non-Production and Production environments (containers). They may give the full javascipt code snippets similar to the following: (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0], j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src= 'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f); })(window,document,'script','dataLayer','GTM-M5YK7JL'); The developer only needs to take to GTM code provided by the < TBD > team and replace in the following files for the new ffc service: - helm/< helm chart name >/values.yaml (see example for ffc-demo-web ) * in section - container: * variable - googleTagManagerKey: - docker-compose.yaml (see example for ffc-demo-web ) * in section services: -> < service name >: -> environment: * variable - GOOGLE_TAG_MANAGER_KEY If javascript code snippets are provided by the < TBD > team. Then extract the GTM code from them and replace in the above files. Once the GTM code has been replaced the developer can load and start interaction with the website. Then the developer needs to login to Google Tag Manager with their DEFRA email (created in previous steps). The interactions can then be viewed in Google Analytics and Data Studio to verify everything is working correctly. See DEFRA documentation for more information on Digital Analytics Standards. NOTE: < TBD > references a team who the developer will need to contact, at the moment its still to be determined. So the current point of contact is Daniel Gilbert (email: Daniel.Gilbert@defra.gov.uk)","title":"Google Tag Manager"},{"location":"z_content-to-be-revised/guides/jenkins/","text":"Jenkins Using Jenkins locally Installation It is recommended that Jenkins is run locally within the Blue Ocean Docker image. The Blue Ocean image is recommended by Jenkins as it comes with Blue Ocean already configured. Blue Ocean provides a more user friendly experience which may suit developers new to Jenkins. The below command will pull the image and run the container. docker run \\ -u root \\ --rm \\ -d \\ -p 8080:8080 \\ -p 50000:50000 \\ -v jenkins-data:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v <PATH_TO_LOCAL_GIT_REPOS>:/repos:ro \\ jenkinsci/blueocean -v <PATH_TO_LOCAL_GIT_REPOS>:repos:ro The above line should be amended to specify the file path of your local git repositories. This will allow you to run builds from your local repository. An example could be -v /c/users/ddts_220606/source/repos:\\repos:ro This official guide includes instructions for how to setup access credentials to the Jenkins container. Run build from local repository The following assumes you have an existing repository containing a Jenkinsfile. select New Item from the main menu enter a project name and select Pipeline within the Pipeline section, select Pipeline script from SCM as the Definition enter Git as the SCM enter the local repository path into the Repository URL using the mount configured above, for example file:////repos/repo1 Set the Script Path to where the Jenkinsfile can be found in the repo save the project select Build Now Troubleshooting If an exception of type Jenkins CI Pipeline Scripts not permitted to use method groovy.lang.GroovyObject is thrown, then complete the following steps. select Manage Jenkins from the main menu select In-process Script Approval select Approve for Groovy","title":"Jenkins"},{"location":"z_content-to-be-revised/guides/jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"z_content-to-be-revised/guides/jenkins/#using-jenkins-locally","text":"","title":"Using Jenkins locally"},{"location":"z_content-to-be-revised/guides/jenkins/#installation","text":"It is recommended that Jenkins is run locally within the Blue Ocean Docker image. The Blue Ocean image is recommended by Jenkins as it comes with Blue Ocean already configured. Blue Ocean provides a more user friendly experience which may suit developers new to Jenkins. The below command will pull the image and run the container. docker run \\ -u root \\ --rm \\ -d \\ -p 8080:8080 \\ -p 50000:50000 \\ -v jenkins-data:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v <PATH_TO_LOCAL_GIT_REPOS>:/repos:ro \\ jenkinsci/blueocean -v <PATH_TO_LOCAL_GIT_REPOS>:repos:ro The above line should be amended to specify the file path of your local git repositories. This will allow you to run builds from your local repository. An example could be -v /c/users/ddts_220606/source/repos:\\repos:ro This official guide includes instructions for how to setup access credentials to the Jenkins container.","title":"Installation"},{"location":"z_content-to-be-revised/guides/jenkins/#run-build-from-local-repository","text":"The following assumes you have an existing repository containing a Jenkinsfile. select New Item from the main menu enter a project name and select Pipeline within the Pipeline section, select Pipeline script from SCM as the Definition enter Git as the SCM enter the local repository path into the Repository URL using the mount configured above, for example file:////repos/repo1 Set the Script Path to where the Jenkinsfile can be found in the repo save the project select Build Now","title":"Run build from local repository"},{"location":"z_content-to-be-revised/guides/jenkins/#troubleshooting","text":"If an exception of type Jenkins CI Pipeline Scripts not permitted to use method groovy.lang.GroovyObject is thrown, then complete the following steps. select Manage Jenkins from the main menu select In-process Script Approval select Approve for Groovy","title":"Troubleshooting"},{"location":"z_content-to-be-revised/guides/jlm/","text":"Joiners, movers and leavers Joining the FFC programme [ ] complete Onboard New Users/Off board existings Users request with Azure CCoE request portal requesting: [ ] OpenVPN certificate [ ] dtz account for Jenkins [ ] accesss to all FFC cloud services [ ] raise a myIT catalogue request for: [ ] O365/AzureAD Platform Admin , specifically an a- Microsoft adminstration account for Azure Portal [ ] Microsoft InTune [ ] request a full set of developer Azure Service Bus queues suffixed with developer initials [ ] join Defra FFC Snyk organisation [ ] join all relevant collaboration channels, see here [ ] request addition to the FFC GitHub team [ ] request addition to the FFC Azure DevOps team [ ] follow development laptop setup guide [ ] take time to understand Defra's software development standards [ ] take time to understand FFC standards in this repository Moving to another team within FFC [ ] ensure you have all relevant Azure Service Bus queues for the new service Leaving the FFC programme [ ] complete Onboard New Users/Off board existings Users request with Azure CCoE request portal requesting removal from all FFC specific resources [ ] request removal of all Azure Service Bus developer queues [ ] leave Defra FFC Snyk organisation [ ] leave all collaboration channels [ ] leave FFC developer team in GitHub [ ] leave FFC developer team in Azure DevOps","title":"Joiners, movers and leavers"},{"location":"z_content-to-be-revised/guides/jlm/#joiners-movers-and-leavers","text":"","title":"Joiners, movers and leavers"},{"location":"z_content-to-be-revised/guides/jlm/#joining-the-ffc-programme","text":"[ ] complete Onboard New Users/Off board existings Users request with Azure CCoE request portal requesting: [ ] OpenVPN certificate [ ] dtz account for Jenkins [ ] accesss to all FFC cloud services [ ] raise a myIT catalogue request for: [ ] O365/AzureAD Platform Admin , specifically an a- Microsoft adminstration account for Azure Portal [ ] Microsoft InTune [ ] request a full set of developer Azure Service Bus queues suffixed with developer initials [ ] join Defra FFC Snyk organisation [ ] join all relevant collaboration channels, see here [ ] request addition to the FFC GitHub team [ ] request addition to the FFC Azure DevOps team [ ] follow development laptop setup guide [ ] take time to understand Defra's software development standards [ ] take time to understand FFC standards in this repository","title":"Joining the FFC programme"},{"location":"z_content-to-be-revised/guides/jlm/#moving-to-another-team-within-ffc","text":"[ ] ensure you have all relevant Azure Service Bus queues for the new service","title":"Moving to another team within FFC"},{"location":"z_content-to-be-revised/guides/jlm/#leaving-the-ffc-programme","text":"[ ] complete Onboard New Users/Off board existings Users request with Azure CCoE request portal requesting removal from all FFC specific resources [ ] request removal of all Azure Service Bus developer queues [ ] leave Defra FFC Snyk organisation [ ] leave all collaboration channels [ ] leave FFC developer team in GitHub [ ] leave FFC developer team in Azure DevOps","title":"Leaving the FFC programme"},{"location":"z_content-to-be-revised/guides/pact-broker/","text":"Pact Broker The Pact Broker is an application for sharing of consumer driven contracts and verification results. It is optimised for use with \"pacts\" (contracts created by the Pact framework), but can be used for any type of contract that can be serialized to JSON. https://github.com/pact-foundation/pact_broker To use the Pact-Broker locally, this repository contains a Dockerized version of the Pact Broker: https://github.com/pact-foundation/pact-broker-docker Pact Broker CLI To install the Pact Broker CLI follow the instructions in the Developer set up guide Webhooks Documentation: https://docs.pact.io/pact_broker/webhooks Through the Pact-Broker CLI: Open a command window and navigate to the bin folder of the pact-cli and run the following command replacing the varibles for your deployment. For more information: https://docs.pact.io/pact_broker/client_cli/readme pact-broker create-webhook \"https://listeners.jenkins-sb.savagebeast.com/job/listeners-acceptance/job/graphql/job/DEVTOOLS-610-test-pact-broker-webhooks/buildWithParameters?os_authType=basic&environment=shared&graphqlHost=shared.graphql.docker.savagebeast.com\" --request=POST --broker-base-url=http://localhost:9292 --description='Test 3 Webhook CLI' --contract-content-changed --consumer=\"Example App\" --provider=\"Example API\" --user=username:password --header=\"accept:application/json\" Through the Pact-Broker UI: Under the column called \"Webhook status\" click on \"Create\" Under the heading \"Links\" Locate \"pb:create\" in the column called \"NON-GET\" Click on the \" ! \" symbol. A new window will open to allow the use of HTTP verbs POST, DELETE. To create the webhook use POST with following body: Jenkins Example { \"description\": \"Trigger ffc-demo-claim-service Acceptance Test Job on contract_content_changed event from ffc-demo-payment-service\", \"events\": [{ \"name\": \"contract_content_changed\" }], \"request\": { \"method\": \"POST\", \"url\": \"\", \"headers\": { \"authorization\": \"Basic \" } } } Slack Example { \"description\": \"Trigger to send Slack message on provider-verification-failed event from ffc-demo-payment-service\", \"events\": [ { \"name\": \"provider_verification_failed\" } ], \"request\": { \"method\": \"POST\", \"url\": \"\", \"headers\": { \"Content-Type\": \"application/json\" }, \"body\": { \"channel\": \"#generalbuildfailures\", \"username\": \"webhookbot\", \"text\": \"Pact verification failed for ${pactbroker.consumerName}/${pactbroker.providerName}: ${pactbroker.pactUrl}\", \"icon_emoji\": \":(\" } } } To delete a Webhook use the DELETE Verb with an empty body. To test the webhook: Select the webhook using \"GET\" Under the heading \"Links\" Locate \"pb:execute\" Note As of writing this guide, there is an issue with the default url used within the UI for executing the API calls. The current default url uses the internal IP address. This causes the following error: violates the following Content Security Policy directive: \"default-src 'self'\". Note that 'connect-src' was not explicitly set, so 'default-src' is used as a fallback. to resolve the issue, the IP address needs replacing with the DNS domain name.","title":"Pact Broker"},{"location":"z_content-to-be-revised/guides/pact-broker/#pact-broker","text":"The Pact Broker is an application for sharing of consumer driven contracts and verification results. It is optimised for use with \"pacts\" (contracts created by the Pact framework), but can be used for any type of contract that can be serialized to JSON. https://github.com/pact-foundation/pact_broker To use the Pact-Broker locally, this repository contains a Dockerized version of the Pact Broker: https://github.com/pact-foundation/pact-broker-docker","title":"Pact Broker"},{"location":"z_content-to-be-revised/guides/pact-broker/#pact-broker-cli","text":"To install the Pact Broker CLI follow the instructions in the Developer set up guide","title":"Pact Broker CLI"},{"location":"z_content-to-be-revised/guides/pact-broker/#webhooks","text":"Documentation: https://docs.pact.io/pact_broker/webhooks Through the Pact-Broker CLI: Open a command window and navigate to the bin folder of the pact-cli and run the following command replacing the varibles for your deployment. For more information: https://docs.pact.io/pact_broker/client_cli/readme pact-broker create-webhook \"https://listeners.jenkins-sb.savagebeast.com/job/listeners-acceptance/job/graphql/job/DEVTOOLS-610-test-pact-broker-webhooks/buildWithParameters?os_authType=basic&environment=shared&graphqlHost=shared.graphql.docker.savagebeast.com\" --request=POST --broker-base-url=http://localhost:9292 --description='Test 3 Webhook CLI' --contract-content-changed --consumer=\"Example App\" --provider=\"Example API\" --user=username:password --header=\"accept:application/json\" Through the Pact-Broker UI: Under the column called \"Webhook status\" click on \"Create\" Under the heading \"Links\" Locate \"pb:create\" in the column called \"NON-GET\" Click on the \" ! \" symbol. A new window will open to allow the use of HTTP verbs POST, DELETE. To create the webhook use POST with following body:","title":"Webhooks"},{"location":"z_content-to-be-revised/guides/pact-broker/#jenkins-example","text":"{ \"description\": \"Trigger ffc-demo-claim-service Acceptance Test Job on contract_content_changed event from ffc-demo-payment-service\", \"events\": [{ \"name\": \"contract_content_changed\" }], \"request\": { \"method\": \"POST\", \"url\": \"\", \"headers\": { \"authorization\": \"Basic \" } } }","title":"Jenkins Example"},{"location":"z_content-to-be-revised/guides/pact-broker/#slack-example","text":"{ \"description\": \"Trigger to send Slack message on provider-verification-failed event from ffc-demo-payment-service\", \"events\": [ { \"name\": \"provider_verification_failed\" } ], \"request\": { \"method\": \"POST\", \"url\": \"\", \"headers\": { \"Content-Type\": \"application/json\" }, \"body\": { \"channel\": \"#generalbuildfailures\", \"username\": \"webhookbot\", \"text\": \"Pact verification failed for ${pactbroker.consumerName}/${pactbroker.providerName}: ${pactbroker.pactUrl}\", \"icon_emoji\": \":(\" } } } To delete a Webhook use the DELETE Verb with an empty body. To test the webhook: Select the webhook using \"GET\" Under the heading \"Links\" Locate \"pb:execute\"","title":"Slack Example"},{"location":"z_content-to-be-revised/guides/pact-broker/#note","text":"As of writing this guide, there is an issue with the default url used within the UI for executing the API calls. The current default url uses the internal IP address. This causes the following error: violates the following Content Security Policy directive: \"default-src 'self'\". Note that 'connect-src' was not explicitly set, so 'default-src' is used as a fallback. to resolve the issue, the IP address needs replacing with the DNS domain name.","title":"Note"},{"location":"z_content-to-be-revised/guides/redis-caching/","text":"Redis Caching in Hapi Hapi caching is described in their official documentation. The following guide specifically describes how server-side Redis caching can be added to a Hapi based microservice. Server-side caching in Hapi Hapi server-side caching uses the catbox interface to abstract away the underlying caching technology being used (e.g. memory, Redis, Memcached). There are three main concepts to Hapi server-side caching: * The cache strategy (or provider): is the underlying caching technology being employed. Here catbox-redis , the Redis adapter for catbox, is the strategy. * The cache client : is the low-level cache abstraction, and is initialised using a cache stragegy (e.g. memory or Redis). Hapi initialises an in-memory cache client by default , and you can create additional cache clients using the same or different strategies (e.g. you can have one in-memory cache, and one Redis cache). * The cache policy : is a higher-level cache abstraction that sets a policy on the storage within the cache (e.g. expiry times). The cache policy also provides additional segmentation within the cache client. Typically the cache policy is how you would interact with cache values via the get and set menthods. Configuring the default cache client As mentioned above, Hapi initialises an in-memory cache client by default. If you wish you can make the default cache client use a different strategy. For example, the following would make the default cache strategy use Redis or memory depending on the value of config.useRedis : const catbox = config.useRedis ? require('@hapi/catbox-redis') : require('@hapi/catbox-memory') const catboxOptions = config.useRedis ? { host: process.env.REDIS_HOSTNAME, port: process.env.REDIS_PORT, password: process.env.REDIS_PASSWORD, partition: process.env.REDIS_PARTITION, tls: process.env.NODE_ENV === 'production' ? {} : undefined } : {} const server = hapi.server({ port: config.port, cache: [{ provider: { constructor: catbox, options: catboxOptions } }] } Configuring new cache clients Additional cache clients can be created when initialising the Hapi server by adding new definitions to the cache array. Additional caches are required to be given a name . For example, the following will create a new Redis cache client called session. const catbox = require('@hapi/catbox-redis') const catboxOptions = { host: process.env.REDIS_HOSTNAME, port: process.env.REDIS_PORT, password: process.env.REDIS_PASSWORD, partition: process.env.REDIS_PARTITION, tls: process.env.NODE_ENV === 'production' ? {} : undefined } const server = hapi.server({ port: config.port, cache: [{ name: 'session', provider: { constructor: catbox, options: catboxOptions } }] } NOTE 1 : This example will create two cache clients, the default in-memory cache client and a new cache client called session that uses Redis NOTE 2 : Hapi will always use the default in-memory cache client unless you specify the name when using it (either directly or via the cache policy, see below) Creating and using a cache policy Lastly we create the cache policy, which is typically how we interact with the cache (see the catbox policy documentation for more details and how set and get data in the cache). When creating a cache policy, if you don't explictily provide the name of a cache client (via the cache property), it will use the default cache client. To create a cache policy using a segement within the default cache client: myCache = server.cache({ expiresIn: 36000, segment: 'mySegment' // ... any other configuration }) To create a cache policy using a segement within a named cache client (in this case session ): myCache = server.cache({ cache: 'session' expiresIn: 36000, segment: 'mySegment' // ... any other configuration }) Integration with yar session cookies Hapi yar is a plugin that adds unauthenticatd session support (state across multiple browser request) to Hapi. By default it tries to fit session data into a session cookie, but will use server-side storage via the Hapi cache interface if the sesion data is greater than the max size specified when registering the plugin. Combining Hapi yar with Redis caching is one way to allow multiple replicates of a web server microservice to share server-side user session data. Example configuration using the default cache client: server.register({ plugin: require('@hapi/yar'), options: { cache: { expiresIn: 36000 }, maxCookieSize: 0 // this will force server-side caching // ... other config here } }) Example configuration using a named cache client: server.register({ plugin: require('@hapi/yar'), options: { cache: { cache: 'session' expiresIn: 36000 }, maxCookieSize: 0 // this will force server-side caching // ... other config here } })","title":"Redis Caching in Hapi"},{"location":"z_content-to-be-revised/guides/redis-caching/#redis-caching-in-hapi","text":"Hapi caching is described in their official documentation. The following guide specifically describes how server-side Redis caching can be added to a Hapi based microservice.","title":"Redis Caching in Hapi"},{"location":"z_content-to-be-revised/guides/redis-caching/#server-side-caching-in-hapi","text":"Hapi server-side caching uses the catbox interface to abstract away the underlying caching technology being used (e.g. memory, Redis, Memcached). There are three main concepts to Hapi server-side caching: * The cache strategy (or provider): is the underlying caching technology being employed. Here catbox-redis , the Redis adapter for catbox, is the strategy. * The cache client : is the low-level cache abstraction, and is initialised using a cache stragegy (e.g. memory or Redis). Hapi initialises an in-memory cache client by default , and you can create additional cache clients using the same or different strategies (e.g. you can have one in-memory cache, and one Redis cache). * The cache policy : is a higher-level cache abstraction that sets a policy on the storage within the cache (e.g. expiry times). The cache policy also provides additional segmentation within the cache client. Typically the cache policy is how you would interact with cache values via the get and set menthods.","title":"Server-side caching in Hapi"},{"location":"z_content-to-be-revised/guides/redis-caching/#configuring-the-default-cache-client","text":"As mentioned above, Hapi initialises an in-memory cache client by default. If you wish you can make the default cache client use a different strategy. For example, the following would make the default cache strategy use Redis or memory depending on the value of config.useRedis : const catbox = config.useRedis ? require('@hapi/catbox-redis') : require('@hapi/catbox-memory') const catboxOptions = config.useRedis ? { host: process.env.REDIS_HOSTNAME, port: process.env.REDIS_PORT, password: process.env.REDIS_PASSWORD, partition: process.env.REDIS_PARTITION, tls: process.env.NODE_ENV === 'production' ? {} : undefined } : {} const server = hapi.server({ port: config.port, cache: [{ provider: { constructor: catbox, options: catboxOptions } }] }","title":"Configuring the default cache client"},{"location":"z_content-to-be-revised/guides/redis-caching/#configuring-new-cache-clients","text":"Additional cache clients can be created when initialising the Hapi server by adding new definitions to the cache array. Additional caches are required to be given a name . For example, the following will create a new Redis cache client called session. const catbox = require('@hapi/catbox-redis') const catboxOptions = { host: process.env.REDIS_HOSTNAME, port: process.env.REDIS_PORT, password: process.env.REDIS_PASSWORD, partition: process.env.REDIS_PARTITION, tls: process.env.NODE_ENV === 'production' ? {} : undefined } const server = hapi.server({ port: config.port, cache: [{ name: 'session', provider: { constructor: catbox, options: catboxOptions } }] } NOTE 1 : This example will create two cache clients, the default in-memory cache client and a new cache client called session that uses Redis NOTE 2 : Hapi will always use the default in-memory cache client unless you specify the name when using it (either directly or via the cache policy, see below)","title":"Configuring new cache clients"},{"location":"z_content-to-be-revised/guides/redis-caching/#creating-and-using-a-cache-policy","text":"Lastly we create the cache policy, which is typically how we interact with the cache (see the catbox policy documentation for more details and how set and get data in the cache). When creating a cache policy, if you don't explictily provide the name of a cache client (via the cache property), it will use the default cache client. To create a cache policy using a segement within the default cache client: myCache = server.cache({ expiresIn: 36000, segment: 'mySegment' // ... any other configuration }) To create a cache policy using a segement within a named cache client (in this case session ): myCache = server.cache({ cache: 'session' expiresIn: 36000, segment: 'mySegment' // ... any other configuration })","title":"Creating and using a cache policy"},{"location":"z_content-to-be-revised/guides/redis-caching/#integration-with-yar-session-cookies","text":"Hapi yar is a plugin that adds unauthenticatd session support (state across multiple browser request) to Hapi. By default it tries to fit session data into a session cookie, but will use server-side storage via the Hapi cache interface if the sesion data is greater than the max size specified when registering the plugin. Combining Hapi yar with Redis caching is one way to allow multiple replicates of a web server microservice to share server-side user session data. Example configuration using the default cache client: server.register({ plugin: require('@hapi/yar'), options: { cache: { expiresIn: 36000 }, maxCookieSize: 0 // this will force server-side caching // ... other config here } }) Example configuration using a named cache client: server.register({ plugin: require('@hapi/yar'), options: { cache: { cache: 'session' expiresIn: 36000 }, maxCookieSize: 0 // this will force server-side caching // ... other config here } })","title":"Integration with yar session cookies"},{"location":"z_content-to-be-revised/guides/secrets-management/","text":"Secrets Management Setting up a new FFC git repository The standards for creating FFC git repositories contains details for configuring new repos to work with the client-side detect-secrets tool. Dealing with false positives detect-secrets often identifies false positives (something it thinks is a secret, but is not), which will stop the developer from committing their changes. We have two strategies for dealing with false positives. For one-time false positives, they can be overridden by commiting with the --no-verify flag. This will commit the change, but any future commits with changes to the file containing the false positive will result in it being detected again. False positives can be permanently ignored by adding them to the secrets baseline. Run the following command and commit the updated .secrets.baseline file: detect-secrets scan --update .secrets.baseline Excluding files and directories The pre-commit configuration file detailed in the guide for creating FFC git repositories contains a regex for exlcuding files from the detect-secrets scan. This can be updated to exlcude additional files or directories. Update with caution.","title":"Secrets Management"},{"location":"z_content-to-be-revised/guides/secrets-management/#secrets-management","text":"","title":"Secrets Management"},{"location":"z_content-to-be-revised/guides/secrets-management/#setting-up-a-new-ffc-git-repository","text":"The standards for creating FFC git repositories contains details for configuring new repos to work with the client-side detect-secrets tool.","title":"Setting up a new FFC git repository"},{"location":"z_content-to-be-revised/guides/secrets-management/#dealing-with-false-positives","text":"detect-secrets often identifies false positives (something it thinks is a secret, but is not), which will stop the developer from committing their changes. We have two strategies for dealing with false positives. For one-time false positives, they can be overridden by commiting with the --no-verify flag. This will commit the change, but any future commits with changes to the file containing the false positive will result in it being detected again. False positives can be permanently ignored by adding them to the secrets baseline. Run the following command and commit the updated .secrets.baseline file: detect-secrets scan --update .secrets.baseline","title":"Dealing with false positives"},{"location":"z_content-to-be-revised/guides/secrets-management/#excluding-files-and-directories","text":"The pre-commit configuration file detailed in the guide for creating FFC git repositories contains a regex for exlcuding files from the detect-secrets scan. This can be updated to exlcude additional files or directories. Update with caution.","title":"Excluding files and directories"},{"location":"z_content-to-be-revised/guides/shared-assets/","text":"Shared assets To better enable developer agility, the FFC Platform team have created several shared assets to reduce the effort needed to deliver a microservice ecosystem within FFC. Teams are expected to use these shared assets. Development guide The development guide is a library of FFC standards and guides useful to support developers within the FFC programme. GitHub Development guide Docker parent images Docker parent images have been created for Node.js and .NET Core. These parent images prevent duplication in each microservice by abstracting common layers away from each microservice. DockerHub Node.js Node.js development .NET Core .NET Core development GitHub Node.js .NET Core Jenkins library The Jenkins shared library abstracts common CI stages away from each microservice repository and ensures that all adequate assurance steps are completed. The library supports the addition of custom steps at key points in the pipeline. GitHub Jenkins library Helm chart library The Helm chart library keeps microservice Helm charts dry by abstracting common resource definitions away from each microservice repository. The packaged chart is hosted in a GitHub repository. GitHub Helm chart library Helm chart repository Microservice template A GitHub template repository has been created to allow teams to quickly get started with new microservice. The template includes all common setup steps such as Docker, Helm charts and Jenkins. GitHub Node.js .NET Core SonarCloud scanning Analysing containerised .NET Core microservices with SonarCloud in CI is challenging. To simplify this process a Docker image has been created to abstract this complexity away from both microservices and the Jenkins library. DockerHub .NET Core SonarCloud Analysis GitHub .NET Core SonarCloud Analysis Kubernetes configuration To support creation of new Kubernetes clusters and workstream namespaces, a repostory containing setup defininitions has been created. GitHub Kubernetes configuration Pact broker The Pact Broker is a repository for all Pact contracts across all environments for the programme. It is used as a tool to aide contract testing in both build and deployment phases. GitHub Pact broker Jenkins agent The Jenkins agent is a Docker image to support a Jenkins instance running in a Kubernetes cluster GitHub Jenkins agent Secret scanning All FFC repositories are regularly scanned for committed secrets. A utility repository has been created in support of that. GitHub Git secret scanning Secret scanning Demo service A demo service has been created to demonstrate FFC's standards and patterns in operation. The demo service allows developers to see an example of how these patterns work in a \"real\" implementation. See Demo service for further details. Microservice reference library A repository of all microservices created by the programme. Confluence Reference library npm publishing Simplify publishing packages to npm, including switching between pre-release and full releases. GitHub npm publish DockerHub npm publish Kafka admin client Simplify viewing and deleting Kafka consumer groups GitHub Kafka admin client Messaging npm package npm package to simplify Azure Service Bus sending and consuming in line with FFC standards GitHub ffc-messaging npm ffc-messaging Events npm package npm package to simplify Azure Event Hubs sending and consuming in line with FFC standards GitHub ffc-events npm ffc-events","title":"Shared assets"},{"location":"z_content-to-be-revised/guides/shared-assets/#shared-assets","text":"To better enable developer agility, the FFC Platform team have created several shared assets to reduce the effort needed to deliver a microservice ecosystem within FFC. Teams are expected to use these shared assets.","title":"Shared assets"},{"location":"z_content-to-be-revised/guides/shared-assets/#development-guide","text":"The development guide is a library of FFC standards and guides useful to support developers within the FFC programme.","title":"Development guide"},{"location":"z_content-to-be-revised/guides/shared-assets/#github","text":"Development guide","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#docker-parent-images","text":"Docker parent images have been created for Node.js and .NET Core. These parent images prevent duplication in each microservice by abstracting common layers away from each microservice.","title":"Docker parent images"},{"location":"z_content-to-be-revised/guides/shared-assets/#dockerhub","text":"Node.js Node.js development .NET Core .NET Core development","title":"DockerHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_1","text":"Node.js .NET Core","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#jenkins-library","text":"The Jenkins shared library abstracts common CI stages away from each microservice repository and ensures that all adequate assurance steps are completed. The library supports the addition of custom steps at key points in the pipeline.","title":"Jenkins library"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_2","text":"Jenkins library","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#helm-chart-library","text":"The Helm chart library keeps microservice Helm charts dry by abstracting common resource definitions away from each microservice repository. The packaged chart is hosted in a GitHub repository.","title":"Helm chart library"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_3","text":"Helm chart library Helm chart repository","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#microservice-template","text":"A GitHub template repository has been created to allow teams to quickly get started with new microservice. The template includes all common setup steps such as Docker, Helm charts and Jenkins.","title":"Microservice template"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_4","text":"Node.js","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#net-core-sonarcloud-scanning","text":"Analysing containerised .NET Core microservices with SonarCloud in CI is challenging. To simplify this process a Docker image has been created to abstract this complexity away from both microservices and the Jenkins library.","title":".NET Core SonarCloud scanning"},{"location":"z_content-to-be-revised/guides/shared-assets/#dockerhub_1","text":".NET Core SonarCloud Analysis","title":"DockerHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_5","text":".NET Core SonarCloud Analysis","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#kubernetes-configuration","text":"To support creation of new Kubernetes clusters and workstream namespaces, a repostory containing setup defininitions has been created.","title":"Kubernetes configuration"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_6","text":"Kubernetes configuration","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#pact-broker","text":"The Pact Broker is a repository for all Pact contracts across all environments for the programme. It is used as a tool to aide contract testing in both build and deployment phases.","title":"Pact broker"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_7","text":"Pact broker","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#jenkins-agent","text":"The Jenkins agent is a Docker image to support a Jenkins instance running in a Kubernetes cluster","title":"Jenkins agent"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_8","text":"Jenkins agent","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#secret-scanning","text":"All FFC repositories are regularly scanned for committed secrets. A utility repository has been created in support of that.","title":"Secret scanning"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_9","text":"Git secret scanning Secret scanning","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#demo-service","text":"A demo service has been created to demonstrate FFC's standards and patterns in operation. The demo service allows developers to see an example of how these patterns work in a \"real\" implementation. See Demo service for further details.","title":"Demo service"},{"location":"z_content-to-be-revised/guides/shared-assets/#microservice-reference-library","text":"A repository of all microservices created by the programme.","title":"Microservice reference library"},{"location":"z_content-to-be-revised/guides/shared-assets/#confluence","text":"Reference library","title":"Confluence"},{"location":"z_content-to-be-revised/guides/shared-assets/#npm-publishing","text":"Simplify publishing packages to npm, including switching between pre-release and full releases.","title":"npm publishing"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_10","text":"npm publish","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#dockerhub_2","text":"npm publish","title":"DockerHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#kafka-admin-client","text":"Simplify viewing and deleting Kafka consumer groups","title":"Kafka admin client"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_11","text":"Kafka admin client","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#messaging-npm-package","text":"npm package to simplify Azure Service Bus sending and consuming in line with FFC standards","title":"Messaging npm package"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_12","text":"ffc-messaging","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#npm","text":"ffc-messaging","title":"npm"},{"location":"z_content-to-be-revised/guides/shared-assets/#events-npm-package","text":"npm package to simplify Azure Event Hubs sending and consuming in line with FFC standards","title":"Events npm package"},{"location":"z_content-to-be-revised/guides/shared-assets/#github_13","text":"ffc-events","title":"GitHub"},{"location":"z_content-to-be-revised/guides/shared-assets/#npm_1","text":"ffc-events","title":"npm"},{"location":"z_content-to-be-revised/guides/troubleshooting/","text":"Troubleshooting Guide A troubleshooting guide for common problems faced by developers on the FFC programme Service bus Application logs message or service timeout What this looks like - In the application logs, data: {\"message\": \"Unable to create the amqp session due to operation timeout.\"} - ServiceBusError: Unable to create the amqp session due to operation timeout at ... code: 'ServiceTimeout' What this means - The application is unable to send a message to its requested topic due to network or configuration issues What is the solution - Ensure the machine is connected to the Azure vNET with a valid FFC subscription via OpenVPN - Check the environment variables for the service bus connection: these are stored locally in either the user's profile file (e.g. .bashrc ) or the repository's .env file - Stop and prune all Docker containers, networks and volumes - Use this testing framework to check outbound requests independently of the application Request does not reach the Azure topic What this looks like - On the Azure portal, the requests chart for the topic has no activity once a request has been sent What this means - The request is not being received within the Azure topic What is the solution - Ensure you are looking at the correct Azure topic within the correct subscription - Check that the request does not timeout - If the application is web-based, try again as webpack was possibly running - If the application is web-based, clear the web-browser's cache or use a private/incognito window - Check Azure platform's health Messages in a topic's Service Bus Subscription(s) are not processed What this looks like - On the Azure portal, the message count for the topic's Service Bus subscription(s) increase once a request has been sent What this means - The messages received from the Azure topic are pushed to the Service Bus subscription(s) but are not being processed What is the solution - Check the subcription is setup for the topic, Azure automatically deletes idle subscriptions after 14 days unless specified to never do so during creation - Usually, another microservice is responsible for ingesting and processing these messages, ensure it is up - Check the environmental subscription variables are correct for the ingesting service (these are usually stored locally within the repository in a .env file) Note: These issues and remedies also apply to Azure's Service Bus queues Docker Tests cannot access database migration data What this looks like - await db.tblName.findAll() returns nothing for a table with insert data in a changelog file(s) - await db.tblName.create(tableRecord).catch(err => { console.log(err) }) returns relation \"public.tableName\" does not exist What this means - The application is attempting to read from an empty database table What is the solution - Check the docker-compose.migrate.yaml file was ran - Confirm the database migration was successful - If using docker compose , ensure the database volume is mapped correctly - If using docker compose , ensure Docker Desktop is configured correctly for V2 by checking/unchecking the Use Docker Compose V2 in Docker Desktop's preferences","title":"Troubleshooting Guide"},{"location":"z_content-to-be-revised/guides/troubleshooting/#troubleshooting-guide","text":"A troubleshooting guide for common problems faced by developers on the FFC programme","title":"Troubleshooting Guide"},{"location":"z_content-to-be-revised/guides/troubleshooting/#service-bus","text":"","title":"Service bus"},{"location":"z_content-to-be-revised/guides/troubleshooting/#application-logs-message-or-service-timeout","text":"What this looks like - In the application logs, data: {\"message\": \"Unable to create the amqp session due to operation timeout.\"} - ServiceBusError: Unable to create the amqp session due to operation timeout at ... code: 'ServiceTimeout' What this means - The application is unable to send a message to its requested topic due to network or configuration issues What is the solution - Ensure the machine is connected to the Azure vNET with a valid FFC subscription via OpenVPN - Check the environment variables for the service bus connection: these are stored locally in either the user's profile file (e.g. .bashrc ) or the repository's .env file - Stop and prune all Docker containers, networks and volumes - Use this testing framework to check outbound requests independently of the application","title":"Application logs message or service timeout"},{"location":"z_content-to-be-revised/guides/troubleshooting/#request-does-not-reach-the-azure-topic","text":"What this looks like - On the Azure portal, the requests chart for the topic has no activity once a request has been sent What this means - The request is not being received within the Azure topic What is the solution - Ensure you are looking at the correct Azure topic within the correct subscription - Check that the request does not timeout - If the application is web-based, try again as webpack was possibly running - If the application is web-based, clear the web-browser's cache or use a private/incognito window - Check Azure platform's health","title":"Request does not reach the Azure topic"},{"location":"z_content-to-be-revised/guides/troubleshooting/#messages-in-a-topics-service-bus-subscriptions-are-not-processed","text":"What this looks like - On the Azure portal, the message count for the topic's Service Bus subscription(s) increase once a request has been sent What this means - The messages received from the Azure topic are pushed to the Service Bus subscription(s) but are not being processed What is the solution - Check the subcription is setup for the topic, Azure automatically deletes idle subscriptions after 14 days unless specified to never do so during creation - Usually, another microservice is responsible for ingesting and processing these messages, ensure it is up - Check the environmental subscription variables are correct for the ingesting service (these are usually stored locally within the repository in a .env file) Note: These issues and remedies also apply to Azure's Service Bus queues","title":"Messages in a topic's Service Bus Subscription(s) are not processed"},{"location":"z_content-to-be-revised/guides/troubleshooting/#docker","text":"","title":"Docker"},{"location":"z_content-to-be-revised/guides/troubleshooting/#tests-cannot-access-database-migration-data","text":"What this looks like - await db.tblName.findAll() returns nothing for a table with insert data in a changelog file(s) - await db.tblName.create(tableRecord).catch(err => { console.log(err) }) returns relation \"public.tableName\" does not exist What this means - The application is attempting to read from an empty database table What is the solution - Check the docker-compose.migrate.yaml file was ran - Confirm the database migration was successful - If using docker compose , ensure the database volume is mapped correctly - If using docker compose , ensure Docker Desktop is configured correctly for V2 by checking/unchecking the Use Docker Compose V2 in Docker Desktop's preferences","title":"Tests cannot access database migration data"},{"location":"z_content-to-be-revised/guides/vs-code-wsl1/","text":"Using Visual Studio Code with WSL1 With WSL1, as source code will be in Windows, depending on the development activity it may be easier to use WSL in Windows rather than using a remote WSL session. Configure terminal to use WSL Download wslgit.exe from https://github.com/andy-5/wslgit Update settings.json with: \"git.path\": \"C:\\\\WINDOWS\\\\System32\\\\wslgit.exe\", \"terminal.integrated.shell.windows\": \"C:\\\\WINDOWS\\\\System32\\\\wsl.exe\" Add or update the Windows environment variable WSLGIT_USE_INTERACTIVE_SHELL to false or 0 Debug configuration With WSL1, you can use the WSL workspace folder and update your debug configuration to use the WSL folder. Examples below. Basic { \"type\": \"node\", \"request\": \"launch\", \"name\": \"WSL - Launch Program\", \"useWSL\": true, \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \"${command:extension.vscode-wsl-workspaceFolder}\", \"program\": \"${workspaceFolder}\\\\index.js\" } App and test { \"type\": \"node\", \"request\": \"launch\", \"name\": \"WSL - Launch Program\", \"useWSL\": true, \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \"${command:extension.vscode-wsl-workspaceFolder}\", \"program\": \"${workspaceFolder}\\\\index.js\", \"env\" : { \"PASSWORD\": \"secret\", \"PORT\": \"3000\", \"WSLENV\": \"PASSWORD:PORT\" } }, { \"type\": \"node\", \"request\": \"launch\", \"name\": \"WSL - Lab Tests\", \"useWSL\": true, \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \"${command:extension.vscode-wsl-workspaceFolder}\", \"program\": \"${workspaceFolder}/node_modules/lab/bin/lab\", \"args\": [ \"-v\", ], \"internalConsoleOptions\": \"openOnSessionStart\" } The env section is used to pass environment variables to the debugger \"internalConsoleOptions\": \"openOnSessionStart\" allows F5 to start debugger.","title":"Using Visual Studio Code with WSL1"},{"location":"z_content-to-be-revised/guides/vs-code-wsl1/#using-visual-studio-code-with-wsl1","text":"With WSL1, as source code will be in Windows, depending on the development activity it may be easier to use WSL in Windows rather than using a remote WSL session.","title":"Using Visual Studio Code with WSL1"},{"location":"z_content-to-be-revised/guides/vs-code-wsl1/#configure-terminal-to-use-wsl","text":"Download wslgit.exe from https://github.com/andy-5/wslgit Update settings.json with: \"git.path\": \"C:\\\\WINDOWS\\\\System32\\\\wslgit.exe\", \"terminal.integrated.shell.windows\": \"C:\\\\WINDOWS\\\\System32\\\\wsl.exe\" Add or update the Windows environment variable WSLGIT_USE_INTERACTIVE_SHELL to false or 0","title":"Configure terminal to use WSL"},{"location":"z_content-to-be-revised/guides/vs-code-wsl1/#debug-configuration","text":"With WSL1, you can use the WSL workspace folder and update your debug configuration to use the WSL folder. Examples below.","title":"Debug configuration"},{"location":"z_content-to-be-revised/guides/vs-code-wsl1/#basic","text":"{ \"type\": \"node\", \"request\": \"launch\", \"name\": \"WSL - Launch Program\", \"useWSL\": true, \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \"${command:extension.vscode-wsl-workspaceFolder}\", \"program\": \"${workspaceFolder}\\\\index.js\" }","title":"Basic"},{"location":"z_content-to-be-revised/guides/vs-code-wsl1/#app-and-test","text":"{ \"type\": \"node\", \"request\": \"launch\", \"name\": \"WSL - Launch Program\", \"useWSL\": true, \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \"${command:extension.vscode-wsl-workspaceFolder}\", \"program\": \"${workspaceFolder}\\\\index.js\", \"env\" : { \"PASSWORD\": \"secret\", \"PORT\": \"3000\", \"WSLENV\": \"PASSWORD:PORT\" } }, { \"type\": \"node\", \"request\": \"launch\", \"name\": \"WSL - Lab Tests\", \"useWSL\": true, \"localRoot\": \"${workspaceFolder}\", \"remoteRoot\": \"${command:extension.vscode-wsl-workspaceFolder}\", \"program\": \"${workspaceFolder}/node_modules/lab/bin/lab\", \"args\": [ \"-v\", ], \"internalConsoleOptions\": \"openOnSessionStart\" } The env section is used to pass environment variables to the debugger \"internalConsoleOptions\": \"openOnSessionStart\" allows F5 to start debugger.","title":"App and test"},{"location":"z_content-to-be-revised/guides/vulnerabilities/","text":"Managing vulnerabilities Package vulnerabilities Vulnerabilties in application packages are identified using Snyk and npm Audit (Node.js only). Resolving vulnerabilities npm Audit Follow official npm documentation Snyk Follow official Snyk documentation","title":"Managing vulnerabilities"},{"location":"z_content-to-be-revised/guides/vulnerabilities/#managing-vulnerabilities","text":"","title":"Managing vulnerabilities"},{"location":"z_content-to-be-revised/guides/vulnerabilities/#package-vulnerabilities","text":"Vulnerabilties in application packages are identified using Snyk and npm Audit (Node.js only).","title":"Package vulnerabilities"},{"location":"z_content-to-be-revised/guides/vulnerabilities/#resolving-vulnerabilities","text":"","title":"Resolving vulnerabilities"},{"location":"z_content-to-be-revised/guides/vulnerabilities/#npm-audit","text":"Follow official npm documentation","title":"npm Audit"},{"location":"z_content-to-be-revised/guides/vulnerabilities/#snyk","text":"Follow official Snyk documentation","title":"Snyk"},{"location":"z_content-to-be-revised/guides/waf-waiver/","text":"WAF waivers Azure Web Application Firewall (WAF) on Azure Application Gateway provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks. WAF on Application Gateway is based on Core Rule Set (CRS) 3.1, 3.0, or 2.2.9 from the Open Web Application Security Project (OWASP). The WAF automatically updates to include protection against new vulnerabilities, with no additional configuration needed. All of the WAF features listed below exist inside of a WAF Policy. You can create multiple policies, and they can be associated with an Application Gateway, to individual listeners, or to path-based routing rules on an Application Gateway. This way, you can have separate policies for each site behind your Application Gateway if needed. See official documentation for more information. If a waiver is required for any policy then this must be approved by Security and implemented by CCoE. Waiver process CCoE provide details of policy breach investigate policy breach take corrective action if policy breach can be avoided if policy breach cannot be avoided, capture details in FFC WAF Waiver log updating both Waiver table and WAF log output add justification for waiver forward to Security for approval if approved, request CCoE update policies","title":"WAF waivers"},{"location":"z_content-to-be-revised/guides/waf-waiver/#waf-waivers","text":"Azure Web Application Firewall (WAF) on Azure Application Gateway provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks. WAF on Application Gateway is based on Core Rule Set (CRS) 3.1, 3.0, or 2.2.9 from the Open Web Application Security Project (OWASP). The WAF automatically updates to include protection against new vulnerabilities, with no additional configuration needed. All of the WAF features listed below exist inside of a WAF Policy. You can create multiple policies, and they can be associated with an Application Gateway, to individual listeners, or to path-based routing rules on an Application Gateway. This way, you can have separate policies for each site behind your Application Gateway if needed. See official documentation for more information. If a waiver is required for any policy then this must be approved by Security and implemented by CCoE.","title":"WAF waivers"},{"location":"z_content-to-be-revised/guides/waf-waiver/#waiver-process","text":"CCoE provide details of policy breach investigate policy breach take corrective action if policy breach can be avoided if policy breach cannot be avoided, capture details in FFC WAF Waiver log updating both Waiver table and WAF log output add justification for waiver forward to Security for approval if approved, request CCoE update policies","title":"Waiver process"},{"location":"z_content-to-be-revised/guides/kubernetes/","text":"Kubernetes Guides supporting the usage of Kubernetes Contents AAD Pod Identity Configure NGINX Ingress Controller Creating a workstream namespace Install Kubernetes Dashboard Interact with cluster Probes","title":"Kubernetes"},{"location":"z_content-to-be-revised/guides/kubernetes/#kubernetes","text":"Guides supporting the usage of Kubernetes","title":"Kubernetes"},{"location":"z_content-to-be-revised/guides/kubernetes/#contents","text":"AAD Pod Identity Configure NGINX Ingress Controller Creating a workstream namespace Install Kubernetes Dashboard Interact with cluster Probes","title":"Contents"},{"location":"z_content-to-be-revised/guides/kubernetes/configure-nginx-ingress-controller/","text":"Configure NGINX Ingress Controller An Ingress controller is an application that runs in a Kubernetes cluster and configures an HTTP load balancer according to Ingress resources. In the case of NGINX, the Ingress controller is deployed in a pod along with the load balancer. Installation The documentation for NGINX's chart includes details on how to install it. TL;DR: helm install stable/nginx-ingress --name nginx-ingress","title":"Configure NGINX Ingress Controller"},{"location":"z_content-to-be-revised/guides/kubernetes/configure-nginx-ingress-controller/#configure-nginx-ingress-controller","text":"An Ingress controller is an application that runs in a Kubernetes cluster and configures an HTTP load balancer according to Ingress resources. In the case of NGINX, the Ingress controller is deployed in a pod along with the load balancer.","title":"Configure NGINX Ingress Controller"},{"location":"z_content-to-be-revised/guides/kubernetes/configure-nginx-ingress-controller/#installation","text":"The documentation for NGINX's chart includes details on how to install it. TL;DR: helm install stable/nginx-ingress --name nginx-ingress","title":"Installation"},{"location":"z_content-to-be-revised/guides/kubernetes/create-namespace/","text":"Creating a workstream namespace Each workstream delivery team in FFC will have their own dedicated namespace in each cluster. This allows logical separation between services as well as the enabling simpler implementation of monitoring, RBAC and stability mechanisms. Requirements The following are the required outcomes of a workstream namespace.# - namespace follows the naming convention ffc-WORKSTREAM eg. ffc-elm - ResourceQuota resource to limit resource usage - RoleBinding resource to restrict interaction to delivery team Process Full instructions are included in the FFC Kubernetes Configuration repository.","title":"Creating a workstream namespace"},{"location":"z_content-to-be-revised/guides/kubernetes/create-namespace/#creating-a-workstream-namespace","text":"Each workstream delivery team in FFC will have their own dedicated namespace in each cluster. This allows logical separation between services as well as the enabling simpler implementation of monitoring, RBAC and stability mechanisms.","title":"Creating a workstream namespace"},{"location":"z_content-to-be-revised/guides/kubernetes/create-namespace/#requirements","text":"The following are the required outcomes of a workstream namespace.# - namespace follows the naming convention ffc-WORKSTREAM eg. ffc-elm - ResourceQuota resource to limit resource usage - RoleBinding resource to restrict interaction to delivery team","title":"Requirements"},{"location":"z_content-to-be-revised/guides/kubernetes/create-namespace/#process","text":"Full instructions are included in the FFC Kubernetes Configuration repository.","title":"Process"},{"location":"z_content-to-be-revised/guides/kubernetes/install-kubernetes-dashboard/","text":"Install Kubernetes dashboard The Kubernetes dashboard is a web-based Kubernetes user interface. Installation Install the dashboard to a Kubernetes cluster using the kubectl apply command specified in the Deploying the dashboard UI section in the below link. https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ Example: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml Create default user and access token Follow the guide in the below link. https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md Run dashboard Run terminal command kubectl proxy Access dashboard at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/","title":"Install Kubernetes dashboard"},{"location":"z_content-to-be-revised/guides/kubernetes/install-kubernetes-dashboard/#install-kubernetes-dashboard","text":"The Kubernetes dashboard is a web-based Kubernetes user interface.","title":"Install Kubernetes dashboard"},{"location":"z_content-to-be-revised/guides/kubernetes/install-kubernetes-dashboard/#installation","text":"Install the dashboard to a Kubernetes cluster using the kubectl apply command specified in the Deploying the dashboard UI section in the below link. https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ Example: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml","title":"Installation"},{"location":"z_content-to-be-revised/guides/kubernetes/install-kubernetes-dashboard/#create-default-user-and-access-token","text":"Follow the guide in the below link. https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md","title":"Create default user and access token"},{"location":"z_content-to-be-revised/guides/kubernetes/install-kubernetes-dashboard/#run-dashboard","text":"Run terminal command kubectl proxy Access dashboard at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/","title":"Run dashboard"},{"location":"z_content-to-be-revised/guides/kubernetes/interaction/","text":"Interact with cluster kubectl is used to interact with the cluster. In order to use kubectl with an FFC cluster, a kubeconfig file for the cluster is required. A cluster can only be accessed when connected to VPN. Acquiring a Kubeconfig file for a cluster To acquire a Kubeconfig, a subscription Id is needed along with the name of the cluster and the resource group in which it resides. This information can be acquired via the Azure Portal or from CCoE. az account set --subscription SUBSCRIPTION_ID az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER --file WHERE_TO_SAVE_KUBECONFIG Note if the file parameter is not passed, the Kubeconfig will be merged with the users default configuration file stored at ~/.kube/config . Productivity Developers may find it more productive to use tools such as Lens k9s or kubectl-aliases to avoid needing to regularly type long terminal commands to interact with the cluster.","title":"Interact with cluster"},{"location":"z_content-to-be-revised/guides/kubernetes/interaction/#interact-with-cluster","text":"kubectl is used to interact with the cluster. In order to use kubectl with an FFC cluster, a kubeconfig file for the cluster is required. A cluster can only be accessed when connected to VPN.","title":"Interact with cluster"},{"location":"z_content-to-be-revised/guides/kubernetes/interaction/#acquiring-a-kubeconfig-file-for-a-cluster","text":"To acquire a Kubeconfig, a subscription Id is needed along with the name of the cluster and the resource group in which it resides. This information can be acquired via the Azure Portal or from CCoE. az account set --subscription SUBSCRIPTION_ID az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER --file WHERE_TO_SAVE_KUBECONFIG Note if the file parameter is not passed, the Kubeconfig will be merged with the users default configuration file stored at ~/.kube/config .","title":"Acquiring a Kubeconfig file for a cluster"},{"location":"z_content-to-be-revised/guides/kubernetes/interaction/#productivity","text":"Developers may find it more productive to use tools such as Lens k9s or kubectl-aliases to avoid needing to regularly type long terminal commands to interact with the cluster.","title":"Productivity"},{"location":"z_content-to-be-revised/guides/kubernetes/pod-identity/","text":"AAD Pod Identity AAD Pod Identity enables Kubernetes applications to access cloud resources securely with Azure Active Directory (AAD). Using Kubernetes primitives, administrators configure identities and bindings to match pods. Then your containerized applications can leverage any resource in the cloud that depends on AAD as an identity provider. Further reading More information is available in Confluence","title":"AAD Pod Identity"},{"location":"z_content-to-be-revised/guides/kubernetes/pod-identity/#aad-pod-identity","text":"AAD Pod Identity enables Kubernetes applications to access cloud resources securely with Azure Active Directory (AAD). Using Kubernetes primitives, administrators configure identities and bindings to match pods. Then your containerized applications can leverage any resource in the cloud that depends on AAD as an identity provider.","title":"AAD Pod Identity"},{"location":"z_content-to-be-revised/guides/kubernetes/pod-identity/#further-reading","text":"More information is available in Confluence","title":"Further reading"},{"location":"z_content-to-be-revised/guides/kubernetes/probes/","text":"Probes Kubernetes has two types of probes, readiness and liveness. Kubernetes uses readiness probes to know when a container is ready to start accepting traffic. Kubernetes uses liveness probes to know when to restart a container. The FFC Helm chart library includes templates for both readiness and liveness probes. Configuring probes Probes can be configured in the Helm chart on a Deployment resource, under the container node. The above is a simple example of an Http readiness and liveness probes. readinessProbe: path: /healthy port: 3000 initialDelaySeconds: 10 periodSeconds: 10 failureThreshold: 3 livenessProbe: path: /healthz port: 3000 initialDelaySeconds: 10 periodSeconds: 10 failureThreshold: 3 In this example, the cluster will wait for 10 seconds after the pod is deployed. It will then poll both the liveness and readiness endpoints on port 3000 every 10 seconds. If it receives three successive status codes other than 200 for the readiness probe it will stop routing traffic to that pod. If it receives three successive status codes other than 200 for the liveness probe it will assume the pod is unresponsive and kill it. Note that a liveness probe works in conjunction with the restartPolicy value. In order to restart the restartPolicy must be set to Always or OnFailure. Values path : the URL route the liveness probe should sent a response to. port : the port on which the service is exposing initialDelaySeconds : how long before the first probe should be sent. This should be safely longer than it takes the pod to start up, otherwise the pod could be stuck in a reboot loop periodSeconds : how often the liveness probe should check the pod is responsive. Recommendation is between 10 and 20 seconds failureThreshold : how many probe failures before the pod is automatically restarted timeoutSeconds : how long to wait for a response before considering the probe failed As well as Http probes, there are also command and TCP based probes, full details can be found in the documentation https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/","title":"Probes"},{"location":"z_content-to-be-revised/guides/kubernetes/probes/#probes","text":"Kubernetes has two types of probes, readiness and liveness. Kubernetes uses readiness probes to know when a container is ready to start accepting traffic. Kubernetes uses liveness probes to know when to restart a container. The FFC Helm chart library includes templates for both readiness and liveness probes.","title":"Probes"},{"location":"z_content-to-be-revised/guides/kubernetes/probes/#configuring-probes","text":"Probes can be configured in the Helm chart on a Deployment resource, under the container node. The above is a simple example of an Http readiness and liveness probes. readinessProbe: path: /healthy port: 3000 initialDelaySeconds: 10 periodSeconds: 10 failureThreshold: 3 livenessProbe: path: /healthz port: 3000 initialDelaySeconds: 10 periodSeconds: 10 failureThreshold: 3 In this example, the cluster will wait for 10 seconds after the pod is deployed. It will then poll both the liveness and readiness endpoints on port 3000 every 10 seconds. If it receives three successive status codes other than 200 for the readiness probe it will stop routing traffic to that pod. If it receives three successive status codes other than 200 for the liveness probe it will assume the pod is unresponsive and kill it. Note that a liveness probe works in conjunction with the restartPolicy value. In order to restart the restartPolicy must be set to Always or OnFailure.","title":"Configuring probes"},{"location":"z_content-to-be-revised/guides/kubernetes/probes/#values","text":"path : the URL route the liveness probe should sent a response to. port : the port on which the service is exposing initialDelaySeconds : how long before the first probe should be sent. This should be safely longer than it takes the pod to start up, otherwise the pod could be stuck in a reboot loop periodSeconds : how often the liveness probe should check the pod is responsive. Recommendation is between 10 and 20 seconds failureThreshold : how many probe failures before the pod is automatically restarted timeoutSeconds : how long to wait for a response before considering the probe failed As well as Http probes, there are also command and TCP based probes, full details can be found in the documentation https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/","title":"Values"},{"location":"z_content-to-be-revised/guides/resource-provisioning/","text":"Resource provisionsing Guides supporting the addition of Azure resources to microservices running on the FFC Platform. Contents CI pipeline Managed Identity Postgres database Release pipeline Service Bus queues, topics and subscriptions","title":"Resource provisionsing"},{"location":"z_content-to-be-revised/guides/resource-provisioning/#resource-provisionsing","text":"Guides supporting the addition of Azure resources to microservices running on the FFC Platform.","title":"Resource provisionsing"},{"location":"z_content-to-be-revised/guides/resource-provisioning/#contents","text":"CI pipeline Managed Identity Postgres database Release pipeline Service Bus queues, topics and subscriptions","title":"Contents"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/","text":"FFC CI Pipeline Create build pipeline for a microservice All Jenkins pipelines are created within a workstream team's Jenkins folder, for example ffc-grants . All pipelines must use the FFC CI pipeline . Configure GitHub repository navigate to Settings -> Webhooks -> Add webhook set Payload URL to be https://jenkins-ffc-api.azure.defra.cloud/github-webhook/ set Content type to be application/json set Secret to be the webhook secret value. This can be retrieved from Azure Key Vault github-webhook-secret value set the following events to trigger the webhook Pull requests Pushes select Add webhook Configure Jenkins navigate to your workstream folder in Jenkins select New Item enter the item name in the format <repository name>-build , for example ffc-demo-web-build select Multibranch Pipeline select Ok enter GitHub as a Branch Source for credentials select the github-token with the ffcplatform user enter your GitHub URL in the HTTPS URL field, for example https://github.com/DEFRA/ffc-demo-web.git set Discover branches to All branches delete Discover pull requests from origin and Discover pull requests from forks set Scan Multibranch Pipeline Triggers -> Periodically if not otherwise run to true with an interval of 1 hour set Pipeline Action Triggers -> Pipeline Delete Event set ffc-housekeeping/cleanup-on-branch-delete set Pipeline Action Triggers -> Include Filter to be * set Pipeline Action Triggers -> Additional Parameter -> Parameter Name to be repoName and Parameter Value to be the name of the repository, for example, ffc-demo-web Configure SonarCloud Note this step should be performed before running the build or you may end up with duplicate projects. navigate to the Defra organisation within SonarCloud select Analyze new project select repository to analyse select Set Up select Adminsitration -> Analysis Method and disable SonarCloud Automatic Analysis select Administration -> Quality Profiles and set all to Sonar way select Administration -> Quality Gate and set to Sonar way select Administration -> Update key and ensure key matches the name of the repository, removing the DEFRA_ prefix, for example, ffc-demo-web Configure Snyk A Snyk scan is run as part of the CI pipeline. By default, a project will be set to private. In order to access unlimited free tests the project needs to be marked as open source. To do so, follow the instructions . Configure repository add this Jenkinsfile to the repository removing either the Node.js or .NET Core line as appropriate. For more information about usage of this Jenkinsfile, see the FFC CI pipeline documentation .","title":"FFC CI Pipeline"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#ffc-ci-pipeline","text":"","title":"FFC CI Pipeline"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#create-build-pipeline-for-a-microservice","text":"All Jenkins pipelines are created within a workstream team's Jenkins folder, for example ffc-grants . All pipelines must use the FFC CI pipeline .","title":"Create build pipeline for a microservice"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#configure-github-repository","text":"navigate to Settings -> Webhooks -> Add webhook set Payload URL to be https://jenkins-ffc-api.azure.defra.cloud/github-webhook/ set Content type to be application/json set Secret to be the webhook secret value. This can be retrieved from Azure Key Vault github-webhook-secret value set the following events to trigger the webhook Pull requests Pushes select Add webhook","title":"Configure GitHub repository"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#configure-jenkins","text":"navigate to your workstream folder in Jenkins select New Item enter the item name in the format <repository name>-build , for example ffc-demo-web-build select Multibranch Pipeline select Ok enter GitHub as a Branch Source for credentials select the github-token with the ffcplatform user enter your GitHub URL in the HTTPS URL field, for example https://github.com/DEFRA/ffc-demo-web.git set Discover branches to All branches delete Discover pull requests from origin and Discover pull requests from forks set Scan Multibranch Pipeline Triggers -> Periodically if not otherwise run to true with an interval of 1 hour set Pipeline Action Triggers -> Pipeline Delete Event set ffc-housekeeping/cleanup-on-branch-delete set Pipeline Action Triggers -> Include Filter to be * set Pipeline Action Triggers -> Additional Parameter -> Parameter Name to be repoName and Parameter Value to be the name of the repository, for example, ffc-demo-web","title":"Configure Jenkins"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#configure-sonarcloud","text":"Note this step should be performed before running the build or you may end up with duplicate projects. navigate to the Defra organisation within SonarCloud select Analyze new project select repository to analyse select Set Up select Adminsitration -> Analysis Method and disable SonarCloud Automatic Analysis select Administration -> Quality Profiles and set all to Sonar way select Administration -> Quality Gate and set to Sonar way select Administration -> Update key and ensure key matches the name of the repository, removing the DEFRA_ prefix, for example, ffc-demo-web","title":"Configure SonarCloud"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#configure-snyk","text":"A Snyk scan is run as part of the CI pipeline. By default, a project will be set to private. In order to access unlimited free tests the project needs to be marked as open source. To do so, follow the instructions .","title":"Configure Snyk"},{"location":"z_content-to-be-revised/guides/resource-provisioning/ci-pipeline/#configure-repository","text":"add this Jenkinsfile to the repository removing either the Node.js or .NET Core line as appropriate. For more information about usage of this Jenkinsfile, see the FFC CI pipeline documentation .","title":"Configure repository"},{"location":"z_content-to-be-revised/guides/resource-provisioning/managed-identity/","text":"Azure Managed Identity This guide describes how to add an Azure Managed Identity to a microservice running within Azure Kubernetes Service. A Managed Identity needs to be bound to the microservice Pod to provide access to additional Azure resources such as Postgres databases and Service Bus message queues . Note: in environments beyond sandpit, managed identities must be created by a pipeline and not manually. Create a Managed Identity Request the creation of an Azure Managed Identity through your usual Cloud Services support channel. The name of the Managed Identity should adhere to the following naming convention: ffc-<cloud-environment>-<workstream>-<service>-role for example ffc-snd-demo-web-role . Add Managed Identity permissions Permissions will need to be associated to the Managed Identity that determine which Azure resources it can access. This step should be carried out when configuring these additional resources. See relevant sections in ServiceBus queue configuration and Postgres database configuration . Update microservice Helm chart Add and configure the AzureIdentity and AzureIdentityBinding Kubernetes templates from the ffc-helm-library to your microservice Helm Chart ( helm/<REPO_NAME>/templates/ ) following the FFC Helm Library guidence . Add Managed Identity values to Azure App Configuration Azure App Configuration stores values required by the Jenkins CI pipelines. For your newly created Managed Identity, create two key-value entries: Key : dev/azureIdentity.clientID ; Value : Client ID of your Managed Identity (found via the Azure Portal); Label : <REPO_NAME> (e.g. ffc-demo-web ) Key : dev/azureIdentity.resourceID ; Value : Resource ID of your Manage Identity (found via the Azure Portal); Label : <REPO_NAME> (e.g. ffc-demo-web )","title":"Azure Managed Identity"},{"location":"z_content-to-be-revised/guides/resource-provisioning/managed-identity/#azure-managed-identity","text":"This guide describes how to add an Azure Managed Identity to a microservice running within Azure Kubernetes Service. A Managed Identity needs to be bound to the microservice Pod to provide access to additional Azure resources such as Postgres databases and Service Bus message queues . Note: in environments beyond sandpit, managed identities must be created by a pipeline and not manually.","title":"Azure Managed Identity"},{"location":"z_content-to-be-revised/guides/resource-provisioning/managed-identity/#create-a-managed-identity","text":"Request the creation of an Azure Managed Identity through your usual Cloud Services support channel. The name of the Managed Identity should adhere to the following naming convention: ffc-<cloud-environment>-<workstream>-<service>-role for example ffc-snd-demo-web-role .","title":"Create a Managed Identity"},{"location":"z_content-to-be-revised/guides/resource-provisioning/managed-identity/#add-managed-identity-permissions","text":"Permissions will need to be associated to the Managed Identity that determine which Azure resources it can access. This step should be carried out when configuring these additional resources. See relevant sections in ServiceBus queue configuration and Postgres database configuration .","title":"Add Managed Identity permissions"},{"location":"z_content-to-be-revised/guides/resource-provisioning/managed-identity/#update-microservice-helm-chart","text":"Add and configure the AzureIdentity and AzureIdentityBinding Kubernetes templates from the ffc-helm-library to your microservice Helm Chart ( helm/<REPO_NAME>/templates/ ) following the FFC Helm Library guidence .","title":"Update microservice Helm chart"},{"location":"z_content-to-be-revised/guides/resource-provisioning/managed-identity/#add-managed-identity-values-to-azure-app-configuration","text":"Azure App Configuration stores values required by the Jenkins CI pipelines. For your newly created Managed Identity, create two key-value entries: Key : dev/azureIdentity.clientID ; Value : Client ID of your Managed Identity (found via the Azure Portal); Label : <REPO_NAME> (e.g. ffc-demo-web ) Key : dev/azureIdentity.resourceID ; Value : Resource ID of your Manage Identity (found via the Azure Portal); Label : <REPO_NAME> (e.g. ffc-demo-web )","title":"Add Managed Identity values to Azure App Configuration"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/","text":"Postgres Database This guide describes how to configure access to an Azure PostgreSQL database from microservices running within Azure Kubernetes Service. Create Managed Identity for your microservice If not already configured add Managed Identity to your microservice Request creation of microservice database Request Cloud Services through your usual support channel to create a database within your Azure Database for PostgreSQL. The name of the database should match the microservice repository name. Microservices should not share a database. For example, a microservice named ffc-demo-claim-service would have a database named ffc_demo_claim_service Note here the use of underscores instead of the normal hyphen convention. Postgres hyphens require escaping with double quote marks so underscores are preferred. Request creation of microservice database role Request Cloud Services to create a database role that is bound to the Managed Identity created for the microservice ( Azure guidence ), for example ffc-snd-demo-claim-role . This identity must also be assigned to the Jenkins VM to ensure that Liquibase migrations can run. Create a Liquibase changelog The FFC Platfrom CI and deployment pipelines support database migrations using Liquibase . Create a Liquibase changelog defining the structure of your database available from the root of your microservice repoository in changelog/db.changelog.xml . Guidence on creating a Liquibase changelog is outside of the scope of this guide, so please check current best practice with the FFC Platform Team. Update Docker Compose files to use Postgres service and environment variables Update docker-compose.yaml , docker-compose.override.yaml , and docker-compose.test.yaml to include a Postgres service and add Postgres environment variables to the microservice. An example Postgres service: services: # Microservice definition here ffc-<workstream>-<service>-postgres: image: postgres:11.4-alpine environment: POSTGRES_DB: ffc_<workstream>_<service> POSTGRES_PASSWORD: ppp POSTGRES_USER: postgres volumes: - postgres_data:/var/lib/postgresql/data ports: - \"5432:5432\" volumes: postgres_data: {} Add dependency on the Postgres service and environment variables the microservice services definition: services: # Microservice definition here depends_on: - ffc-<workstream>-<service>-postgres environment: POSTGRES_DB: ffc_<workstream>_<service> POSTGRES_PASSWORD: ppp POSTGRES_USER: postgres POSTGRES_HOST: ffc-<workstream>-<service>-postgres POSTGRES_PORT: 5432 POSTGRES_SCHEMA_NAME: public Replace <workstream> and <service> as per naming convention described above. Add Docker Compose files to run Liquibase migrations Add a docker-compose.migrate.yaml to the root of the microservice based on the template provided in resources . Replace <workstream> and <service> as per naming convention described above. Update microservice Helm chart Create a Postgres Service Create a Kubernetes template for a Postgres Service in helm/<REPO_NAME>/templates/postgres-service.yaml : {{- if .Values.postgresService.postgresExternalName }} {{- include \"ffc-helm-library.postgres-service\" (list . \"<REPO_NAME>.postgres-service\") -}} {{- end }} {{- define \"<REPO_NAME>.postgres-service\" -}} {{- end -}} replacing <REPO_NAME> with the git repository name. Update the Helm chart values file ( helm/<REPO_NAME>/values.yaml ) with default values for the Postgres service: postgresService: postgresDb: ffc_<workstream>_<service> postgresExternalName: postgresHost: ffc-<workstream>-<service>-postgres postgresPort: 5432 postgresSchema: public postgresUser: postgres replacing <workstream> and <service> as per naming convention described above. Update ConfigMap Update the ConfigMap template of the Helm Chart ( helm/<REPO_NAME>/templates/config-map.yaml ) to include the environment variables for the Postgres database: POSTGRES_DB: {{ quote .Values.postgresService.postgresDb }} POSTGRES_HOST: {{ quote .Values.postgresService.postgresHost }} POSTGRES_PORT: {{ quote .Values.postgresService.postgresPort }} POSTGRES_SCHEMA_NAME: {{ quote .Values.postgresService.postgresSchema }} Create/Update the container Secret Create (or update) the Secret template in helm/<REPO_NAME>/templates/container-secret.yaml : {{- include \"ffc-helm-library.container-secret\" (list . \"<REPO_NAME>.container-secret\") -}} {{- define \"<REPO_NAME>.container-secret\" -}} stringData: POSTGRES_USER: {{ .Values.postgresService.postgresUser | quote }} {{- end -}} replacing <REPO_NAME> with the git repository name. Update the Helm chart values file ( helm/<REPO_NAME>/values.yaml ) with a name for the Secret: containerSecret: name: ffc-<workstream>-<service>-container-secret type: Opaque replacing <workstream> and <service> as per naming convention described above. Add Liquibase migration scripts Copy the scripts from resources to create the following scripts at the root of your microservice: * scripts/migration/database-down * scripts/migration/database-up * scripts/postgres-wait Add values to Azure Key Vault and App Configuration Azure Key Vault is used to store the Postgres username and Azure App Configuration is used to stores values required by the Jenkins CI pipelines. Create the following secret in Azure Key Vault via the Azure Portal: Name : snd-postgres<workstream><service>User ; Value : <managed-identity>@<azure-postgres-instance> (e.g. ffc-snd-demo-web-role@mypostgresserver ) Create the following entries in Azure App Configuraiton via the Azure Portal: A key-value entry where Key : <environment>/postgresService.postgresDb (e.g. dev/postgresService.postgresDb ); Value : ffc_<workstream>_<service> (e.g. ffc_demo_claim_service ); Label : <REPO_NAME> (e.g. ffc-demo-claim-service ) A Key Vault reference entry where Key : dev/postgresService.postgresUser ; Key Vault Secret Key : dev-postgres<workstream><service>User ; Label : <REPO_NAME> (e.g. ffc-demo-claim-service ) where <workstream> and <service> refer to those parts of the queue name described above. Note in environments beyond Sandpit, Azure DevOps will provision databases suffixed with the target environment. The values should be ammended accordingly. e.g. ffc_demo_claim_service_dev Add database code to the microservice Update your microservice code using the relevant Azure authentication SDKs for your language. Patterns for using a Postgres database in microservice code are outside of the scope of this guide. An example is shown below for a Node.js microservice, but please check current best practice with the FFC Platform Team. Node.js example Install the Azure Authentication SDK NPM package: npm install @azure/ms-rest-nodeauth . With the Managed Identity bound to your microservice in the Kubernetes cluster (following the guidence above), you can then access the database using the username <managed-identity>@<azure-postgres-instance> (e.g. ffc-snd-demo-web-role@mypostgresserver ) and an access token as the password: async function example() { const auth = require('@azure/ms-rest-nodeauth') const credentials = await auth.loginWithVmMSI({ resource: 'https://ossrdbms-aad.database.windows.net' }) const databasePassword = await credentials.getToken() // Use databasePassword along with Postgres role bound to Managed Identity to authenticate to your database }","title":"Postgres Database"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#postgres-database","text":"This guide describes how to configure access to an Azure PostgreSQL database from microservices running within Azure Kubernetes Service.","title":"Postgres Database"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#create-managed-identity-for-your-microservice","text":"If not already configured add Managed Identity to your microservice","title":"Create Managed Identity for your microservice"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#request-creation-of-microservice-database","text":"Request Cloud Services through your usual support channel to create a database within your Azure Database for PostgreSQL. The name of the database should match the microservice repository name. Microservices should not share a database. For example, a microservice named ffc-demo-claim-service would have a database named ffc_demo_claim_service Note here the use of underscores instead of the normal hyphen convention. Postgres hyphens require escaping with double quote marks so underscores are preferred.","title":"Request creation of microservice database"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#request-creation-of-microservice-database-role","text":"Request Cloud Services to create a database role that is bound to the Managed Identity created for the microservice ( Azure guidence ), for example ffc-snd-demo-claim-role . This identity must also be assigned to the Jenkins VM to ensure that Liquibase migrations can run.","title":"Request creation of microservice database role"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#create-a-liquibase-changelog","text":"The FFC Platfrom CI and deployment pipelines support database migrations using Liquibase . Create a Liquibase changelog defining the structure of your database available from the root of your microservice repoository in changelog/db.changelog.xml . Guidence on creating a Liquibase changelog is outside of the scope of this guide, so please check current best practice with the FFC Platform Team.","title":"Create a Liquibase changelog"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#update-docker-compose-files-to-use-postgres-service-and-environment-variables","text":"Update docker-compose.yaml , docker-compose.override.yaml , and docker-compose.test.yaml to include a Postgres service and add Postgres environment variables to the microservice. An example Postgres service: services: # Microservice definition here ffc-<workstream>-<service>-postgres: image: postgres:11.4-alpine environment: POSTGRES_DB: ffc_<workstream>_<service> POSTGRES_PASSWORD: ppp POSTGRES_USER: postgres volumes: - postgres_data:/var/lib/postgresql/data ports: - \"5432:5432\" volumes: postgres_data: {} Add dependency on the Postgres service and environment variables the microservice services definition: services: # Microservice definition here depends_on: - ffc-<workstream>-<service>-postgres environment: POSTGRES_DB: ffc_<workstream>_<service> POSTGRES_PASSWORD: ppp POSTGRES_USER: postgres POSTGRES_HOST: ffc-<workstream>-<service>-postgres POSTGRES_PORT: 5432 POSTGRES_SCHEMA_NAME: public Replace <workstream> and <service> as per naming convention described above.","title":"Update Docker Compose files to use Postgres service and environment variables"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#add-docker-compose-files-to-run-liquibase-migrations","text":"Add a docker-compose.migrate.yaml to the root of the microservice based on the template provided in resources . Replace <workstream> and <service> as per naming convention described above.","title":"Add Docker Compose files to run Liquibase migrations"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#update-microservice-helm-chart","text":"","title":"Update microservice Helm chart"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#create-a-postgres-service","text":"Create a Kubernetes template for a Postgres Service in helm/<REPO_NAME>/templates/postgres-service.yaml : {{- if .Values.postgresService.postgresExternalName }} {{- include \"ffc-helm-library.postgres-service\" (list . \"<REPO_NAME>.postgres-service\") -}} {{- end }} {{- define \"<REPO_NAME>.postgres-service\" -}} {{- end -}} replacing <REPO_NAME> with the git repository name. Update the Helm chart values file ( helm/<REPO_NAME>/values.yaml ) with default values for the Postgres service: postgresService: postgresDb: ffc_<workstream>_<service> postgresExternalName: postgresHost: ffc-<workstream>-<service>-postgres postgresPort: 5432 postgresSchema: public postgresUser: postgres replacing <workstream> and <service> as per naming convention described above.","title":"Create a Postgres Service"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#update-configmap","text":"Update the ConfigMap template of the Helm Chart ( helm/<REPO_NAME>/templates/config-map.yaml ) to include the environment variables for the Postgres database: POSTGRES_DB: {{ quote .Values.postgresService.postgresDb }} POSTGRES_HOST: {{ quote .Values.postgresService.postgresHost }} POSTGRES_PORT: {{ quote .Values.postgresService.postgresPort }} POSTGRES_SCHEMA_NAME: {{ quote .Values.postgresService.postgresSchema }}","title":"Update ConfigMap"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#createupdate-the-container-secret","text":"Create (or update) the Secret template in helm/<REPO_NAME>/templates/container-secret.yaml : {{- include \"ffc-helm-library.container-secret\" (list . \"<REPO_NAME>.container-secret\") -}} {{- define \"<REPO_NAME>.container-secret\" -}} stringData: POSTGRES_USER: {{ .Values.postgresService.postgresUser | quote }} {{- end -}} replacing <REPO_NAME> with the git repository name. Update the Helm chart values file ( helm/<REPO_NAME>/values.yaml ) with a name for the Secret: containerSecret: name: ffc-<workstream>-<service>-container-secret type: Opaque replacing <workstream> and <service> as per naming convention described above.","title":"Create/Update the container Secret"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#add-liquibase-migration-scripts","text":"Copy the scripts from resources to create the following scripts at the root of your microservice: * scripts/migration/database-down * scripts/migration/database-up * scripts/postgres-wait","title":"Add Liquibase migration scripts"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#add-values-to-azure-key-vault-and-app-configuration","text":"Azure Key Vault is used to store the Postgres username and Azure App Configuration is used to stores values required by the Jenkins CI pipelines. Create the following secret in Azure Key Vault via the Azure Portal: Name : snd-postgres<workstream><service>User ; Value : <managed-identity>@<azure-postgres-instance> (e.g. ffc-snd-demo-web-role@mypostgresserver ) Create the following entries in Azure App Configuraiton via the Azure Portal: A key-value entry where Key : <environment>/postgresService.postgresDb (e.g. dev/postgresService.postgresDb ); Value : ffc_<workstream>_<service> (e.g. ffc_demo_claim_service ); Label : <REPO_NAME> (e.g. ffc-demo-claim-service ) A Key Vault reference entry where Key : dev/postgresService.postgresUser ; Key Vault Secret Key : dev-postgres<workstream><service>User ; Label : <REPO_NAME> (e.g. ffc-demo-claim-service ) where <workstream> and <service> refer to those parts of the queue name described above. Note in environments beyond Sandpit, Azure DevOps will provision databases suffixed with the target environment. The values should be ammended accordingly. e.g. ffc_demo_claim_service_dev","title":"Add values to Azure Key Vault and App Configuration"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#add-database-code-to-the-microservice","text":"Update your microservice code using the relevant Azure authentication SDKs for your language. Patterns for using a Postgres database in microservice code are outside of the scope of this guide. An example is shown below for a Node.js microservice, but please check current best practice with the FFC Platform Team.","title":"Add database code to the microservice"},{"location":"z_content-to-be-revised/guides/resource-provisioning/postgres-database/#nodejs-example","text":"Install the Azure Authentication SDK NPM package: npm install @azure/ms-rest-nodeauth . With the Managed Identity bound to your microservice in the Kubernetes cluster (following the guidence above), you can then access the database using the username <managed-identity>@<azure-postgres-instance> (e.g. ffc-snd-demo-web-role@mypostgresserver ) and an access token as the password: async function example() { const auth = require('@azure/ms-rest-nodeauth') const credentials = await auth.loginWithVmMSI({ resource: 'https://ossrdbms-aad.database.windows.net' }) const databasePassword = await credentials.getToken() // Use databasePassword along with Postgres role bound to Managed Identity to authenticate to your database }","title":"Node.js example"},{"location":"z_content-to-be-revised/guides/resource-provisioning/release-pipeline/","text":"Release pipelines Sandpit Sandpit release pipelines are run from Jenkins and a deployment to the sandpit environment is automatically triggered by the CI pipeline if building from a main branch. Configure sandpit release pipeline navigate to your workstream folder in Jenkins creating a new credential is optional and if not done, the value of default-deploy-token should be used select Credentials -> (global) -> Add Credentials select Secret text type and set both Description and Id to <respository name>-deploy-token , for example ffc-demo-web-deploy-token set the secret value to be a unique string, this value will be used to authenticate the CI pipeline when triggering a release navigate to your workspace folder and select New Item enter the item name in the format <repository name>-deploy , for example ffc-demo-web-deploy select Pipeline select Ok set This project is parameterised to true and add the following string parameters Name Default Value Description chartVersion 1.0.0 Version of service to deploy chartName <repository name> for example, ffc-demo-web Service to deploy namespace ffc-<service name> for example, ffc-grants Cluster namespace to deploy to environment dev Cluster environment to deploy to helmChartRepoType acr Location of Helm charts set Trigger builds remotely to true enter unique deploy token created above add inline pipeline script with the following content ``` @Library('defra-library@v-9') _ deployToCluster environment: params.environment, namespace: params.namespace, chartName: params.chartName, chartVersion: params.chartVersion, helmChartRepoType: params.helmChartRepoType `` - select Save` Development, Test, PreProduction and Production release pipelines Release pipelines to higher environments are run from Azure DevOps using a common microservice pipeline. A deployment to the development and test environments environment are automatically triggered following a build from a main branch. These pipelines can also be run manually by delivery teams. Deployments to PreProd require a ticket raised with CCoE. Deployments to Production require an RFC raised in myIT to authorise CCoE to deploy. Preparations for higher environments Teams should ensure they have completed the following before deploying to higher environments. Update Azure Application Configuration is updated with all required Helm chart configurations Share any new databases created with the Platform team. The release pipelines are currently dependent on knowing which services have databases in advance. This is expected to change in the near future. Ensure all managed identities are created Ensure all Service Bus queues are created","title":"Release pipelines"},{"location":"z_content-to-be-revised/guides/resource-provisioning/release-pipeline/#release-pipelines","text":"","title":"Release pipelines"},{"location":"z_content-to-be-revised/guides/resource-provisioning/release-pipeline/#sandpit","text":"Sandpit release pipelines are run from Jenkins and a deployment to the sandpit environment is automatically triggered by the CI pipeline if building from a main branch.","title":"Sandpit"},{"location":"z_content-to-be-revised/guides/resource-provisioning/release-pipeline/#configure-sandpit-release-pipeline","text":"navigate to your workstream folder in Jenkins creating a new credential is optional and if not done, the value of default-deploy-token should be used select Credentials -> (global) -> Add Credentials select Secret text type and set both Description and Id to <respository name>-deploy-token , for example ffc-demo-web-deploy-token set the secret value to be a unique string, this value will be used to authenticate the CI pipeline when triggering a release navigate to your workspace folder and select New Item enter the item name in the format <repository name>-deploy , for example ffc-demo-web-deploy select Pipeline select Ok set This project is parameterised to true and add the following string parameters Name Default Value Description chartVersion 1.0.0 Version of service to deploy chartName <repository name> for example, ffc-demo-web Service to deploy namespace ffc-<service name> for example, ffc-grants Cluster namespace to deploy to environment dev Cluster environment to deploy to helmChartRepoType acr Location of Helm charts set Trigger builds remotely to true enter unique deploy token created above add inline pipeline script with the following content ``` @Library('defra-library@v-9') _ deployToCluster environment: params.environment, namespace: params.namespace, chartName: params.chartName, chartVersion: params.chartVersion, helmChartRepoType: params.helmChartRepoType `` - select Save`","title":"Configure sandpit release pipeline"},{"location":"z_content-to-be-revised/guides/resource-provisioning/release-pipeline/#development-test-preproduction-and-production-release-pipelines","text":"Release pipelines to higher environments are run from Azure DevOps using a common microservice pipeline. A deployment to the development and test environments environment are automatically triggered following a build from a main branch. These pipelines can also be run manually by delivery teams. Deployments to PreProd require a ticket raised with CCoE. Deployments to Production require an RFC raised in myIT to authorise CCoE to deploy.","title":"Development, Test, PreProduction and Production release pipelines"},{"location":"z_content-to-be-revised/guides/resource-provisioning/release-pipeline/#preparations-for-higher-environments","text":"Teams should ensure they have completed the following before deploying to higher environments. Update Azure Application Configuration is updated with all required Helm chart configurations Share any new databases created with the Platform team. The release pipelines are currently dependent on knowing which services have databases in advance. This is expected to change in the near future. Ensure all managed identities are created Ensure all Service Bus queues are created","title":"Preparations for higher environments"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/","text":"Azure Service Bus Queues, Topics and Subscriptions This guide describes how to configure access to Azure Service Bus from microservices running within Azure Kubernetes Service (AKS). Create a microservice Managed Identity If not already configured, add an Azure Managed Identity to your microservice . Request creation of Service Bus queues, topics and subscriptions For point-to-point queues For each queue required by your microservices, you will need: * A Service Bus queue for each environment * A Service Bus queue for each of the developers on the workstream in the Sandpit environment to use for local development Queues should follow the following naming convention: ffc-<workstream>-<identifier>-<purpose> where <purpose> either denotes the environment e.g. ffc-demo-payment-snd , or the initials of a developer for the local development queues e.g. ffc-demo-payment-jw . Request the creation of the required queues through your usual Cloud Services support channel. For topics and subscriptions For each topic, you will need: * A Service Bus topic for each environment * A Service Bus topic for each of the developers on the workstream in the Sandpit environment to use for local development Subscriptions are created on a topic. For every subscription, you will also need: * A Service Bus subscription to the topic for each environment * A Service Bus subscription to each developer topic in the Sandpit environment to use for local development Topics and subscriptions should follow the following naming convention: ffc-<workstream>-<identifier>-<purpose> where <purpose> either denotes the environment e.g. ffc-demo-reporting-snd , or the initials of a developer for the local development queues e.g. ffc-demo-reporting-jw . Request the creation of the required topics and subscriptions through your usual Cloud Services support channel. Update Managed Identity permissions For each environment queue, topic and subscription created, read and/or write permissions should be added to the Managed Identites of the microservices that will be using it. Request Cloud Services to add the relevant permissions to the Managed Identities. Permissions do not need to be added to Managed Identities for the local development queues. PR queue provisioning Queues, topics and subscriptions can be automatically provisioned for each microservice PR by the Jenkins CI pipleine to ensure encapsulation of infrastructure. Create a provision.azure.yaml file in the root directory of your microservice, and populate with the Service Bus infrastructure that need provisioning: resources: queues: - name: <queue_identifier> topics: - name: <topic_identifier> where the <queue_identifier> and/or <topic_identifier> relates to the part of the queue or topic name described above. For example for the queue ffc-demo-payment-dev , <queue_identifier> would be replaced with payment . For every topic specified in provision.azure.yaml , the CI pipeline will also create a subscription, so subscriptions do not need to be explictly spcecified. Add a name entry in the provision.azure.yaml for each required queue and topic/subscription. Update local development environment Configure your development environment so that the following environment variables are available for local development: MESSAGE_QUEUE_HOST=<INSERT_VALUE_FROM_AZURE_PORTAL> MESSAGE_QUEUE_PASSWORD=<INSERT_VALUE_FROM_AZURE_PORTAL> MESSAGE_QUEUE_USER=RootManageSharedAccessKey Values for MESSAGE_QUEUE_HOST and MESSAGE_QUEUE_PASSWORD will be found in the Azure Portal. Also create a variable for each of your developer queues, topics and subscriptions: <IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS=ffc-<workstream>-<identifier>-<purpose> where <IDENTIFIER> is the same as the <identifier> part of the queue/topic/subscription name. Options for storing these environment variables include: * A .env file in the root of the microservice that uses Service bus, making sure you Do not commit the .env file to the git repository (add it to the .gitignore ) * Your shell rc file e.g. .bashrc or .zshrc Update Docker Compose to use Service Bus environment variables Add the following environment variables to the microserive environment section of the docker-compose.yaml : MESSAGE_QUEUE_HOST: ${MESSAGE_QUEUE_HOST:-notset} MESSAGE_QUEUE_PASSWORD: ${MESSAGE_QUEUE_PASSWORD:-notset} MESSAGE_QUEUE_USER: ${MESSAGE_QUEUE_USER:-notset} And for every queue, topic and subscription you have created also add: <IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS: ${<IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS:-notset} Update microservice Helm chart Update the ConfigMap template of the Helm Chart ( helm/<REPO_NAME>/templates/config-map.yaml ) to include the environment variables for the message queue host and every queue, topic and subscription you have created, for example: MESSAGE_QUEUE_HOST: {{ quote .Values.container.messageQueueHost }} <IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS: {{ quote .Values.container.<identifier><Queue|Topic|Subscription>Address }} Then create default values for these in the container section of the Helm Chart values file ( helm/<REPO_NAME>/values.yaml ): container: messageQueueHost: dummy <identifier><Queue|Topic|Subscription>Address: <identifier> Add queue, topic and subscription names to Azure App Configuration Azure App Configuration stores values required by the Jenkins CI pipelines. For each queue, topic and subscription create a key-value entry for each environment via the Azure Portal: Key : dev/container.<identifier><Queue|Topic|Subscription>Address ; Value : ffc-<workstream>-<identifier>-<environment> where <workstream> and <identifier> refer to those parts of the queue name described above, and <environment> denotes the environment e.g. ffc-demo-payment-dev Do not add a label to these App Configuration entries. Add messaging code to the microservice Update your microservice code using the relevant Azure authentication and Service Bus SDKs for your language. The easiest way to do this is by using the ffc-messaging module .","title":"Azure Service Bus Queues, Topics and Subscriptions"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#azure-service-bus-queues-topics-and-subscriptions","text":"This guide describes how to configure access to Azure Service Bus from microservices running within Azure Kubernetes Service (AKS).","title":"Azure Service Bus Queues, Topics and Subscriptions"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#create-a-microservice-managed-identity","text":"If not already configured, add an Azure Managed Identity to your microservice .","title":"Create a microservice Managed Identity"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#request-creation-of-service-bus-queues-topics-and-subscriptions","text":"","title":"Request creation of Service Bus queues, topics and subscriptions"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#for-point-to-point-queues","text":"For each queue required by your microservices, you will need: * A Service Bus queue for each environment * A Service Bus queue for each of the developers on the workstream in the Sandpit environment to use for local development Queues should follow the following naming convention: ffc-<workstream>-<identifier>-<purpose> where <purpose> either denotes the environment e.g. ffc-demo-payment-snd , or the initials of a developer for the local development queues e.g. ffc-demo-payment-jw . Request the creation of the required queues through your usual Cloud Services support channel.","title":"For point-to-point queues"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#for-topics-and-subscriptions","text":"For each topic, you will need: * A Service Bus topic for each environment * A Service Bus topic for each of the developers on the workstream in the Sandpit environment to use for local development Subscriptions are created on a topic. For every subscription, you will also need: * A Service Bus subscription to the topic for each environment * A Service Bus subscription to each developer topic in the Sandpit environment to use for local development Topics and subscriptions should follow the following naming convention: ffc-<workstream>-<identifier>-<purpose> where <purpose> either denotes the environment e.g. ffc-demo-reporting-snd , or the initials of a developer for the local development queues e.g. ffc-demo-reporting-jw . Request the creation of the required topics and subscriptions through your usual Cloud Services support channel.","title":"For topics and subscriptions"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#update-managed-identity-permissions","text":"For each environment queue, topic and subscription created, read and/or write permissions should be added to the Managed Identites of the microservices that will be using it. Request Cloud Services to add the relevant permissions to the Managed Identities. Permissions do not need to be added to Managed Identities for the local development queues.","title":"Update Managed Identity permissions"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#pr-queue-provisioning","text":"Queues, topics and subscriptions can be automatically provisioned for each microservice PR by the Jenkins CI pipleine to ensure encapsulation of infrastructure. Create a provision.azure.yaml file in the root directory of your microservice, and populate with the Service Bus infrastructure that need provisioning: resources: queues: - name: <queue_identifier> topics: - name: <topic_identifier> where the <queue_identifier> and/or <topic_identifier> relates to the part of the queue or topic name described above. For example for the queue ffc-demo-payment-dev , <queue_identifier> would be replaced with payment . For every topic specified in provision.azure.yaml , the CI pipeline will also create a subscription, so subscriptions do not need to be explictly spcecified. Add a name entry in the provision.azure.yaml for each required queue and topic/subscription.","title":"PR queue provisioning"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#update-local-development-environment","text":"Configure your development environment so that the following environment variables are available for local development: MESSAGE_QUEUE_HOST=<INSERT_VALUE_FROM_AZURE_PORTAL> MESSAGE_QUEUE_PASSWORD=<INSERT_VALUE_FROM_AZURE_PORTAL> MESSAGE_QUEUE_USER=RootManageSharedAccessKey Values for MESSAGE_QUEUE_HOST and MESSAGE_QUEUE_PASSWORD will be found in the Azure Portal. Also create a variable for each of your developer queues, topics and subscriptions: <IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS=ffc-<workstream>-<identifier>-<purpose> where <IDENTIFIER> is the same as the <identifier> part of the queue/topic/subscription name. Options for storing these environment variables include: * A .env file in the root of the microservice that uses Service bus, making sure you Do not commit the .env file to the git repository (add it to the .gitignore ) * Your shell rc file e.g. .bashrc or .zshrc","title":"Update local development environment"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#update-docker-compose-to-use-service-bus-environment-variables","text":"Add the following environment variables to the microserive environment section of the docker-compose.yaml : MESSAGE_QUEUE_HOST: ${MESSAGE_QUEUE_HOST:-notset} MESSAGE_QUEUE_PASSWORD: ${MESSAGE_QUEUE_PASSWORD:-notset} MESSAGE_QUEUE_USER: ${MESSAGE_QUEUE_USER:-notset} And for every queue, topic and subscription you have created also add: <IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS: ${<IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS:-notset}","title":"Update Docker Compose to use Service Bus environment variables"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#update-microservice-helm-chart","text":"Update the ConfigMap template of the Helm Chart ( helm/<REPO_NAME>/templates/config-map.yaml ) to include the environment variables for the message queue host and every queue, topic and subscription you have created, for example: MESSAGE_QUEUE_HOST: {{ quote .Values.container.messageQueueHost }} <IDENTIFIER>_<QUEUE|TOPIC|SUBSCRIPTION>_ADDRESS: {{ quote .Values.container.<identifier><Queue|Topic|Subscription>Address }} Then create default values for these in the container section of the Helm Chart values file ( helm/<REPO_NAME>/values.yaml ): container: messageQueueHost: dummy <identifier><Queue|Topic|Subscription>Address: <identifier>","title":"Update microservice Helm chart"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#add-queue-topic-and-subscription-names-to-azure-app-configuration","text":"Azure App Configuration stores values required by the Jenkins CI pipelines. For each queue, topic and subscription create a key-value entry for each environment via the Azure Portal: Key : dev/container.<identifier><Queue|Topic|Subscription>Address ; Value : ffc-<workstream>-<identifier>-<environment> where <workstream> and <identifier> refer to those parts of the queue name described above, and <environment> denotes the environment e.g. ffc-demo-payment-dev Do not add a label to these App Configuration entries.","title":"Add queue, topic and subscription names to Azure App Configuration"},{"location":"z_content-to-be-revised/guides/resource-provisioning/servicebus-queues/#add-messaging-code-to-the-microservice","text":"Update your microservice code using the relevant Azure authentication and Service Bus SDKs for your language. The easiest way to do this is by using the ffc-messaging module .","title":"Add messaging code to the microservice"},{"location":"z_content-to-be-revised/standards/","text":"Standards These standards must be adhered to by all FCP developers. Contents APIs Backlog management Code reviews Configuration management Documentation Events and messages FFC CI pipeline Jenkins Kubernetes Configuration Helm charts Labels Pod priority Probes Role Based Access Control (RBAC) Resource management Secrets Quick reference Release notes Route to live Secrets management Source code Technology stack Testing Version control","title":"Standards"},{"location":"z_content-to-be-revised/standards/#standards","text":"These standards must be adhered to by all FCP developers.","title":"Standards"},{"location":"z_content-to-be-revised/standards/#contents","text":"APIs Backlog management Code reviews Configuration management Documentation Events and messages FFC CI pipeline Jenkins Kubernetes Configuration Helm charts Labels Pod priority Probes Role Based Access Control (RBAC) Resource management Secrets Quick reference Release notes Route to live Secrets management Source code Technology stack Testing Version control","title":"Contents"},{"location":"z_content-to-be-revised/standards/api/","text":"API Standards Teams must document the API of all microservices they develop. Both REST APIs such as HTTP and Asynchronous APIs such as messages and events. Links to the documents mentioned below should also be included in the Microservice Reference Library REST Teams need to document their API using OpenAPI 3.0 specification which can be found here . This approach is supported by GDS technical standards which states \"The open standards board recommends that government organisations use OpenAPI version 3 to describe RESTful APIs\" for more information you can read The OpenAPI file should be kept in a docs folder in the top level of the folder and named openapi.yaml unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named openapi-<version>.yaml . This should be documented in the API section of the Microservice Reference Library . An example of how to do this can be found in ffc-demo-payment-service . Asynchronous Teams need to document their API using AsyncAPI specification which can be found here The AsyncAPI file should be kept in a docs folder in the top level of the folder and named asyncapi.yaml unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named asyncapi-<version>.yaml . This should be documented in the Events section of the Microservice Reference Library . An example of how to do this can be found in ffc-demo-payment-service .","title":"API Standards"},{"location":"z_content-to-be-revised/standards/api/#api-standards","text":"Teams must document the API of all microservices they develop. Both REST APIs such as HTTP and Asynchronous APIs such as messages and events. Links to the documents mentioned below should also be included in the Microservice Reference Library","title":"API Standards"},{"location":"z_content-to-be-revised/standards/api/#rest","text":"Teams need to document their API using OpenAPI 3.0 specification which can be found here . This approach is supported by GDS technical standards which states \"The open standards board recommends that government organisations use OpenAPI version 3 to describe RESTful APIs\" for more information you can read The OpenAPI file should be kept in a docs folder in the top level of the folder and named openapi.yaml unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named openapi-<version>.yaml . This should be documented in the API section of the Microservice Reference Library . An example of how to do this can be found in ffc-demo-payment-service .","title":"REST"},{"location":"z_content-to-be-revised/standards/api/#asynchronous","text":"Teams need to document their API using AsyncAPI specification which can be found here The AsyncAPI file should be kept in a docs folder in the top level of the folder and named asyncapi.yaml unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named asyncapi-<version>.yaml . This should be documented in the Events section of the Microservice Reference Library . An example of how to do this can be found in ffc-demo-payment-service .","title":"Asynchronous"},{"location":"z_content-to-be-revised/standards/backlog-management/","text":"Backlog management All FFC teams follow an agile delivery approach. Teams should use whichever agile methodology best fits their delivery, for example Scrum, Kanban etc. The GDS Service manual provides guidance for agile delivery. If teams are unclear which methodology to adopt, they should start with vanilla Scrum and iterate as appropriate. Teams looking for help getting started with Scrum can follow this training series . All teams should manage their backlog in Azure DevOps within the DEFRA-FFC project. The backlog should be managed as a single backlog for the team, not a backlog per product.","title":"Backlog management"},{"location":"z_content-to-be-revised/standards/backlog-management/#backlog-management","text":"All FFC teams follow an agile delivery approach. Teams should use whichever agile methodology best fits their delivery, for example Scrum, Kanban etc. The GDS Service manual provides guidance for agile delivery. If teams are unclear which methodology to adopt, they should start with vanilla Scrum and iterate as appropriate. Teams looking for help getting started with Scrum can follow this training series . All teams should manage their backlog in Azure DevOps within the DEFRA-FFC project. The backlog should be managed as a single backlog for the team, not a backlog per product.","title":"Backlog management"},{"location":"z_content-to-be-revised/standards/ci-pipeline/","text":"FFC CI pipeline The FFC CI pipeline is a shared Jenkins library. The purpose of which is to abstract the complexity of building, testing and deploying applications away from consuming microservices. All FFC microservices will use the FFC CI pipeline. Supported technologies Node.js .NET Helm charts Azure Functions Docker images npm packages Pipeline trigger The pipeline will trigger automatically on any commit to a branch with an open Pull Request (PR) or the main branch. Assuming the repository is configured in line with Source control standards . Build steps The steps for a PR or main build are more or less the same. The most significant difference is there is no deployment to a dedicated PR Kubernetes namespace with a main build. Instead additional build assets are created. The pipeline supports context specific flexibility by allowing custom steps to be injected into various points. Anywhere CUSTOM INJECTION is shown below can support any number of additional steps to run. validate semantic version of package lint Helm chart package vulnerability scanning npm Audit (Node.js only) Snyk CUSTOM INJECTION build test image provision dynamic infrastructure for PR deployment and build Azure Service Bus Azure PostgreSQL Azure Function CUSTOM INJECTION run tests lint unit narrow integration local integration (uses dynamic build infrastructure) contract CUSTOM INJECTION publish contract Pact Broker static code analysis SonarCloud build image push to container registry deploy to PR namespace (PR build only) run accessibility tests Pa11y AXE run acceptance tests Selenium Cucumber BrowserStack run security tests OWASP Zap run performance tests JMeter publish Helm chart (main build only) CUSTOM INJECTION create GitHub release (main build only) destroy dedicated infrastructure for build CUSTOM INJECTION Clean up steps On closure or merge of a PR, the clean up pipeline will run. This will destroy any related dynamically created infrastructure or PR deployments.","title":"FFC CI pipeline"},{"location":"z_content-to-be-revised/standards/ci-pipeline/#ffc-ci-pipeline","text":"The FFC CI pipeline is a shared Jenkins library. The purpose of which is to abstract the complexity of building, testing and deploying applications away from consuming microservices. All FFC microservices will use the FFC CI pipeline.","title":"FFC CI pipeline"},{"location":"z_content-to-be-revised/standards/ci-pipeline/#supported-technologies","text":"Node.js .NET Helm charts Azure Functions Docker images npm packages","title":"Supported technologies"},{"location":"z_content-to-be-revised/standards/ci-pipeline/#pipeline-trigger","text":"The pipeline will trigger automatically on any commit to a branch with an open Pull Request (PR) or the main branch. Assuming the repository is configured in line with Source control standards .","title":"Pipeline trigger"},{"location":"z_content-to-be-revised/standards/ci-pipeline/#build-steps","text":"The steps for a PR or main build are more or less the same. The most significant difference is there is no deployment to a dedicated PR Kubernetes namespace with a main build. Instead additional build assets are created. The pipeline supports context specific flexibility by allowing custom steps to be injected into various points. Anywhere CUSTOM INJECTION is shown below can support any number of additional steps to run. validate semantic version of package lint Helm chart package vulnerability scanning npm Audit (Node.js only) Snyk CUSTOM INJECTION build test image provision dynamic infrastructure for PR deployment and build Azure Service Bus Azure PostgreSQL Azure Function CUSTOM INJECTION run tests lint unit narrow integration local integration (uses dynamic build infrastructure) contract CUSTOM INJECTION publish contract Pact Broker static code analysis SonarCloud build image push to container registry deploy to PR namespace (PR build only) run accessibility tests Pa11y AXE run acceptance tests Selenium Cucumber BrowserStack run security tests OWASP Zap run performance tests JMeter publish Helm chart (main build only) CUSTOM INJECTION create GitHub release (main build only) destroy dedicated infrastructure for build CUSTOM INJECTION","title":"Build steps"},{"location":"z_content-to-be-revised/standards/ci-pipeline/#clean-up-steps","text":"On closure or merge of a PR, the clean up pipeline will run. This will destroy any related dynamically created infrastructure or PR deployments.","title":"Clean up steps"},{"location":"z_content-to-be-revised/standards/code-review/","text":"Code review When code reviews are conducted to a high standard, they provide a valuable learning opportunity for the author, reviewer and any observers of the review process. FFC projects implement strict controls such that all changes must be made in feature branches and merged via pull request (PR). In order to merge a PR, it must first be approved by at least one reviewer. When reviewing a pull request on any FFC project, the following guidance should be followed. Tone of code review comments The tone of communications is extremely important in fostering an inclusive, collaborative atmosphere within teams. Remember that your colleagues put a lot of effort into their work and may feel offended by harsh criticism, particularly if you make assumptions or imply a lack of thought. Approach code reviews with kindness and empathy, assuming the best intentions and applauding good solutions as well as making suggestions for improvement. Comments should be used to give praise for good solutions and to point out potential improvements. They should be not be used to criticise your colleagues or make strongly opinionated statements. Always be mindful of your tone, considering how others might perceive your comments, and be explicit about when a comment is non-blocking or unimportant. Be constructive Don't use harsh language or criticise without making constructive suggestions. Do suggest alternative approaches and explain your reasoning. Be specific Don't make vague statements about the changes as a whole. Do point out specific issues and offer specific ideas for how the changes can be improved. Avoid strong opinions Don't make strong, opinionated statements or dictate specific changes. Do ask open-ended questions and keep an open mind about alternative solutions and reasoning that you may not have thought of. Scope of a code review Code reviews should focus on what is being changed and whether the change is appropriate. The scope will be stated in the acceptance criteria of the ticket. Expanding the scope of a pull request at the review stage is not acceptable. It is generally more valuable to swiftly conclude a piece of work that the team has prioritised than to opportunistically seek additional changes, such as refactoring related code. That said, it can be useful to comment on refactoring opportunities without blocking the pull request. It may be that the author has some spare time they can use to make those changes, or can incorporate them with their next piece of work. Examples of things to look for and comment on in a code review: What has changed Are the changes focussed on a specific issue, referenced in the pull request description? If the changes go beyond the intended scope, should they be broken up to make the code review more manageable? If the code review is still a manageable size, consider making a non-blocking comment to remind the author that they could control the scope of future pull requests more carefully. Maintainability Is all new code extensible and easy for other developers to understand? Does it follow common design patterns? Look for unnecessary complexity and remember that this is subjective so take care not to be overly critical or opinionated when commenting on maintainability. Try to make specific suggestions for improvement, rather than simply stating a problem, but use open-ended language to encourage discussion. Duplication Is there duplication within new code or between new and existing code? Could an existing abstraction be reused or should a new abstraction be created? When proposing an abstraction, consider using a non-blocking comment to open a dialogue about what the abstraction should be and whether it needs addressing now or as a separate piece of work. Identifying useful abstractions at the review stage may indicate that there wasn't enough collaboration before coding began. See this as a trigger to review the team's ways of working, rather than blocking a pull request unnecessarily. Reusability Have any new abstractions been introduced? Are they sufficiently reusable? If other parts of the system could be updated to use the new abstractions, consider suggesting this in a non-blocking comment so it can be discussed and either addressed in the same pull request or added to the product backlog as technical debt. Impact on other parts of the system Will the changes have knock-on effects or otherwise necessitate changes to other parts of the system? Unit test coverage Is all new code covered by detailed unit tests? Have any edge cases been missed? Don't rely on metrics such as code coverage. Inspect the code thoroughly and ensure that the tests contain appropriate assertions to confirm that all the intended functionality works as expected. Integration tests Have integration tests been added to cover all changes to functionality? Suggesting improvements When concluding a review, there are three options: Comment Approve the PR Block the PR by requesting changes When to comment You should conclude with a comment when your review asks questions which need answering before you can determine whether the pull request is acceptable. If you haven't proposed a solution to a specific problem in the pull request, it's generally better to leave a neutral review than to block the PR with a request for change. If you have raised concerns or questions through inline comments, it may be helpful to give a concise summary in your concluding comment. When to approve You should approve a pull request when you have confirmed that: it meets the objectives set out in its description it doesn't introduce new defects or code that is hard to maintain all new and modified code has thorough unit test coverage integration tests have been added where appropriate there are no unresolved questions or comments against the pull request Occasionally, it may be prudent to accept a pull request which does not meet all of the above requirements, such as to resolve an urgent issue with the live product. Such cases must always be agreed between the product owner, author(s) and reviewer(s). If comments or questions on a pull request have been addressed elsewhere (e.g. face-to-face or on Slack), ensure that the outcome is recorded in comment replies so that it is visible to anyone looking back at the pull request in future for information. Use the Resolve button to make it clear which of your concerns have been addressed and which still need attention. A pull request should only be approved after all reviewers have explicitly indicated that each of their comments have been resolved. If several reviewers have replied to a comment thread, it may appropriate for any of them to resolve that conversation on behalf of the group. Sometimes, a reviewer may become unavailable after commenting on a pull request. When that happens, a second reviewer may accept the pull request without resolving the first reviewer's comments, as long as the author and second reviewer agree that they believe all legitimate concerns have been addressed. Before accepting the pull request, the second reviewer should reply to each unresolved comment to indicate that they believe it has been addressed. When to request changes You should request changes to a pull request if any of the following are true: it creates a defect in the product it exacerbates an existing defect it doesn't meet the objectives set out in its description it would make the product more difficult to maintain new or modified code lacks thorough unit tests required integration tests have not been included When not to request changes Perfection is the enemy of good A pull request does not need to be perfect to be good enough. Often, there are many solutions to a problem and one which is not the best is still good enough to meet current needs. Perhaps an alternative approach would be more efficient or an abstraction could reduce duplication, but those things may be less important than the next item in the product backlog. Make suggestions for improvement as comments without explicitly approving or rejecting the pull request. This way, your suggestions can open dialogue with the author about how important your suggestions are compared to other work you could each move on to.","title":"Code review"},{"location":"z_content-to-be-revised/standards/code-review/#code-review","text":"When code reviews are conducted to a high standard, they provide a valuable learning opportunity for the author, reviewer and any observers of the review process. FFC projects implement strict controls such that all changes must be made in feature branches and merged via pull request (PR). In order to merge a PR, it must first be approved by at least one reviewer. When reviewing a pull request on any FFC project, the following guidance should be followed.","title":"Code review"},{"location":"z_content-to-be-revised/standards/code-review/#tone-of-code-review-comments","text":"The tone of communications is extremely important in fostering an inclusive, collaborative atmosphere within teams. Remember that your colleagues put a lot of effort into their work and may feel offended by harsh criticism, particularly if you make assumptions or imply a lack of thought. Approach code reviews with kindness and empathy, assuming the best intentions and applauding good solutions as well as making suggestions for improvement. Comments should be used to give praise for good solutions and to point out potential improvements. They should be not be used to criticise your colleagues or make strongly opinionated statements. Always be mindful of your tone, considering how others might perceive your comments, and be explicit about when a comment is non-blocking or unimportant.","title":"Tone of code review comments"},{"location":"z_content-to-be-revised/standards/code-review/#be-constructive","text":"Don't use harsh language or criticise without making constructive suggestions. Do suggest alternative approaches and explain your reasoning.","title":"Be constructive"},{"location":"z_content-to-be-revised/standards/code-review/#be-specific","text":"Don't make vague statements about the changes as a whole. Do point out specific issues and offer specific ideas for how the changes can be improved.","title":"Be specific"},{"location":"z_content-to-be-revised/standards/code-review/#avoid-strong-opinions","text":"Don't make strong, opinionated statements or dictate specific changes. Do ask open-ended questions and keep an open mind about alternative solutions and reasoning that you may not have thought of.","title":"Avoid strong opinions"},{"location":"z_content-to-be-revised/standards/code-review/#scope-of-a-code-review","text":"Code reviews should focus on what is being changed and whether the change is appropriate. The scope will be stated in the acceptance criteria of the ticket. Expanding the scope of a pull request at the review stage is not acceptable. It is generally more valuable to swiftly conclude a piece of work that the team has prioritised than to opportunistically seek additional changes, such as refactoring related code. That said, it can be useful to comment on refactoring opportunities without blocking the pull request. It may be that the author has some spare time they can use to make those changes, or can incorporate them with their next piece of work. Examples of things to look for and comment on in a code review:","title":"Scope of a code review"},{"location":"z_content-to-be-revised/standards/code-review/#what-has-changed","text":"Are the changes focussed on a specific issue, referenced in the pull request description? If the changes go beyond the intended scope, should they be broken up to make the code review more manageable? If the code review is still a manageable size, consider making a non-blocking comment to remind the author that they could control the scope of future pull requests more carefully.","title":"What has changed"},{"location":"z_content-to-be-revised/standards/code-review/#maintainability","text":"Is all new code extensible and easy for other developers to understand? Does it follow common design patterns? Look for unnecessary complexity and remember that this is subjective so take care not to be overly critical or opinionated when commenting on maintainability. Try to make specific suggestions for improvement, rather than simply stating a problem, but use open-ended language to encourage discussion.","title":"Maintainability"},{"location":"z_content-to-be-revised/standards/code-review/#duplication","text":"Is there duplication within new code or between new and existing code? Could an existing abstraction be reused or should a new abstraction be created? When proposing an abstraction, consider using a non-blocking comment to open a dialogue about what the abstraction should be and whether it needs addressing now or as a separate piece of work. Identifying useful abstractions at the review stage may indicate that there wasn't enough collaboration before coding began. See this as a trigger to review the team's ways of working, rather than blocking a pull request unnecessarily.","title":"Duplication"},{"location":"z_content-to-be-revised/standards/code-review/#reusability","text":"Have any new abstractions been introduced? Are they sufficiently reusable? If other parts of the system could be updated to use the new abstractions, consider suggesting this in a non-blocking comment so it can be discussed and either addressed in the same pull request or added to the product backlog as technical debt.","title":"Reusability"},{"location":"z_content-to-be-revised/standards/code-review/#impact-on-other-parts-of-the-system","text":"Will the changes have knock-on effects or otherwise necessitate changes to other parts of the system?","title":"Impact on other parts of the system"},{"location":"z_content-to-be-revised/standards/code-review/#unit-test-coverage","text":"Is all new code covered by detailed unit tests? Have any edge cases been missed? Don't rely on metrics such as code coverage. Inspect the code thoroughly and ensure that the tests contain appropriate assertions to confirm that all the intended functionality works as expected.","title":"Unit test coverage"},{"location":"z_content-to-be-revised/standards/code-review/#integration-tests","text":"Have integration tests been added to cover all changes to functionality?","title":"Integration tests"},{"location":"z_content-to-be-revised/standards/code-review/#suggesting-improvements","text":"When concluding a review, there are three options: Comment Approve the PR Block the PR by requesting changes","title":"Suggesting improvements"},{"location":"z_content-to-be-revised/standards/code-review/#when-to-comment","text":"You should conclude with a comment when your review asks questions which need answering before you can determine whether the pull request is acceptable. If you haven't proposed a solution to a specific problem in the pull request, it's generally better to leave a neutral review than to block the PR with a request for change. If you have raised concerns or questions through inline comments, it may be helpful to give a concise summary in your concluding comment.","title":"When to comment"},{"location":"z_content-to-be-revised/standards/code-review/#when-to-approve","text":"You should approve a pull request when you have confirmed that: it meets the objectives set out in its description it doesn't introduce new defects or code that is hard to maintain all new and modified code has thorough unit test coverage integration tests have been added where appropriate there are no unresolved questions or comments against the pull request Occasionally, it may be prudent to accept a pull request which does not meet all of the above requirements, such as to resolve an urgent issue with the live product. Such cases must always be agreed between the product owner, author(s) and reviewer(s). If comments or questions on a pull request have been addressed elsewhere (e.g. face-to-face or on Slack), ensure that the outcome is recorded in comment replies so that it is visible to anyone looking back at the pull request in future for information. Use the Resolve button to make it clear which of your concerns have been addressed and which still need attention. A pull request should only be approved after all reviewers have explicitly indicated that each of their comments have been resolved. If several reviewers have replied to a comment thread, it may appropriate for any of them to resolve that conversation on behalf of the group. Sometimes, a reviewer may become unavailable after commenting on a pull request. When that happens, a second reviewer may accept the pull request without resolving the first reviewer's comments, as long as the author and second reviewer agree that they believe all legitimate concerns have been addressed. Before accepting the pull request, the second reviewer should reply to each unresolved comment to indicate that they believe it has been addressed.","title":"When to approve"},{"location":"z_content-to-be-revised/standards/code-review/#when-to-request-changes","text":"You should request changes to a pull request if any of the following are true: it creates a defect in the product it exacerbates an existing defect it doesn't meet the objectives set out in its description it would make the product more difficult to maintain new or modified code lacks thorough unit tests required integration tests have not been included","title":"When to request changes"},{"location":"z_content-to-be-revised/standards/code-review/#when-not-to-request-changes","text":"Perfection is the enemy of good A pull request does not need to be perfect to be good enough. Often, there are many solutions to a problem and one which is not the best is still good enough to meet current needs. Perhaps an alternative approach would be more efficient or an abstraction could reduce duplication, but those things may be less important than the next item in the product backlog. Make suggestions for improvement as comments without explicitly approving or rejecting the pull request. This way, your suggestions can open dialogue with the author about how important your suggestions are compared to other work you could each move on to.","title":"When not to request changes"},{"location":"z_content-to-be-revised/standards/configuration-management/","text":"Configuration management Non sensitive application configuration data is persisted in Azure Application Configuration . Sensitive application data is persisted in Azure Key Vault and referenced from Application Configuration. One instance of Azure Key Vault and Azure Application Configuration will exist in every FFC Azure subscription. The configuration keys used in Application Configuration must follow FFC naming convention below to ensure that CI and CD pipelines can retrieve values in a predictable and efficient way. Helm values in deployment During deployment, our CI or CD pipeline will read the values.yaml file in the Helm chart to identify all potential configuration values that may need to be sourced from Application Configuration and then matches each of them by key name. For example, if a helm chart contains the below, then the key to be matched will be container.port . container: port: 3000 If a key exists then the value, 3000 in the example, will be overwritten with the value in Application Configuration. Key naming standards As all FFC teams will be sharing a single instance of Application Configuration in each subscription and that subscription may host multiple environments, we need to ensure our key naming convention is scalable, performant and avoids duplication where possible. There are five accepted formats a key name can follow. The examples all use the container.port key as a demonstration for simplicity. common/key These are values that are consistent, regardless of service or environment. For example common/container.port would mean all services would all get the same value wherever they deploy. The common/ text is necessary due to the limitation the Azure CLI used in pipelines returns keys. Without the prefix there would be no way to only return these common items. environment/key These are values that are environment specific, but not service specific. For example, dev/container.port would mean all services would get the same value when deploying to the dev environment. service/common/key These are values that are service specific, but not environment specific. For example, ffc-demo/common/container.port would mean all deployments belonging to the ffc-demo service would get the same value wherever they deploy. As before the /common/ text is necessary to ensure the Azure CLI returns only relevant values. service/environment/key These are values that are service and environment specific. For example, ffc-demo/dev/container.port would mean all deployments belonging to the ffc-demo service would get the same value when deploying to the dev environment. pr/key These are only used for PR deployments in CI when you need to provide a different value for that PR deployment and still be able to provide individual microservie labels (see below). They are also used by CI to obtain dynamic infrastructure values. For example, pr/container.port would mean all PR deployments would get the same port value. Microservice specific values When you want to provide a value depending on the specific microservice and not just the service you can add a different label to that key referencing the name of the microservice. For example, if you have a key ffc-demo/common/container.port with an unlabelled value of 4000 and a value of 5000 labelled ffc-demo-web , then all ffc-demo services would get a value of 4000 except the ffc-demo-web microservice which would get 5000 . Order values are overridden Within the CI and CD pipelines, the values will be sourced and overwritten in the following order. common/key common/key plus microservice label environment/key environment/key plus microservice label service/common/key service/common/key plus microservice label service/environment/key service/environment/key plus microservice label pr/key (CI PR deployment only) pr/key plus microservice label (CI PR deployment only) Deciding which convention to use To improve performance in CI, keys should follow a convention that gives the narrowest scope possible. For example, if a value is only used for one service or will vary depending on the service then it should be scoped to either service/common or service/environment Only values that are used by everyone should be scoped to common/ or environment/ Exceptions to the rule The are some values that are exceptions to this rule because of dynamic infrastructure provisioning and related test execution in CI. Note: these exceptions only apply to the Sandpit ( SND ) environment. Ingress values FFC Helm charts define their ingress resources like the below: ingress: server: endpoint: class: These values must use the hierarchy environment/key for example dev/ingress.server and use labels to separate each service's value. Database values FFC Helm charts define their database names like the below: postgresService: postgresUser: postgresDb: These values must use the hierarchy environment/key for example dev/postgresService.postgresUser and use labels to separate each service's value. Updating values In Sandpit, values are added direct to Application Configuration via the Azure Portal or Azure CLI. In higher environments values are added by an Azure DevOps pipeline that sources configuration from a git repository . Full details for maintaining application configuration can be found in this wiki .","title":"Configuration management"},{"location":"z_content-to-be-revised/standards/configuration-management/#configuration-management","text":"Non sensitive application configuration data is persisted in Azure Application Configuration . Sensitive application data is persisted in Azure Key Vault and referenced from Application Configuration. One instance of Azure Key Vault and Azure Application Configuration will exist in every FFC Azure subscription. The configuration keys used in Application Configuration must follow FFC naming convention below to ensure that CI and CD pipelines can retrieve values in a predictable and efficient way.","title":"Configuration management"},{"location":"z_content-to-be-revised/standards/configuration-management/#helm-values-in-deployment","text":"During deployment, our CI or CD pipeline will read the values.yaml file in the Helm chart to identify all potential configuration values that may need to be sourced from Application Configuration and then matches each of them by key name. For example, if a helm chart contains the below, then the key to be matched will be container.port . container: port: 3000 If a key exists then the value, 3000 in the example, will be overwritten with the value in Application Configuration.","title":"Helm values in deployment"},{"location":"z_content-to-be-revised/standards/configuration-management/#key-naming-standards","text":"As all FFC teams will be sharing a single instance of Application Configuration in each subscription and that subscription may host multiple environments, we need to ensure our key naming convention is scalable, performant and avoids duplication where possible. There are five accepted formats a key name can follow. The examples all use the container.port key as a demonstration for simplicity.","title":"Key naming standards"},{"location":"z_content-to-be-revised/standards/configuration-management/#commonkey","text":"These are values that are consistent, regardless of service or environment. For example common/container.port would mean all services would all get the same value wherever they deploy. The common/ text is necessary due to the limitation the Azure CLI used in pipelines returns keys. Without the prefix there would be no way to only return these common items.","title":"common/key"},{"location":"z_content-to-be-revised/standards/configuration-management/#environmentkey","text":"These are values that are environment specific, but not service specific. For example, dev/container.port would mean all services would get the same value when deploying to the dev environment.","title":"environment/key"},{"location":"z_content-to-be-revised/standards/configuration-management/#servicecommonkey","text":"These are values that are service specific, but not environment specific. For example, ffc-demo/common/container.port would mean all deployments belonging to the ffc-demo service would get the same value wherever they deploy. As before the /common/ text is necessary to ensure the Azure CLI returns only relevant values.","title":"service/common/key"},{"location":"z_content-to-be-revised/standards/configuration-management/#serviceenvironmentkey","text":"These are values that are service and environment specific. For example, ffc-demo/dev/container.port would mean all deployments belonging to the ffc-demo service would get the same value when deploying to the dev environment.","title":"service/environment/key"},{"location":"z_content-to-be-revised/standards/configuration-management/#prkey","text":"These are only used for PR deployments in CI when you need to provide a different value for that PR deployment and still be able to provide individual microservie labels (see below). They are also used by CI to obtain dynamic infrastructure values. For example, pr/container.port would mean all PR deployments would get the same port value.","title":"pr/key"},{"location":"z_content-to-be-revised/standards/configuration-management/#microservice-specific-values","text":"When you want to provide a value depending on the specific microservice and not just the service you can add a different label to that key referencing the name of the microservice. For example, if you have a key ffc-demo/common/container.port with an unlabelled value of 4000 and a value of 5000 labelled ffc-demo-web , then all ffc-demo services would get a value of 4000 except the ffc-demo-web microservice which would get 5000 .","title":"Microservice specific values"},{"location":"z_content-to-be-revised/standards/configuration-management/#order-values-are-overridden","text":"Within the CI and CD pipelines, the values will be sourced and overwritten in the following order. common/key common/key plus microservice label environment/key environment/key plus microservice label service/common/key service/common/key plus microservice label service/environment/key service/environment/key plus microservice label pr/key (CI PR deployment only) pr/key plus microservice label (CI PR deployment only)","title":"Order values are overridden"},{"location":"z_content-to-be-revised/standards/configuration-management/#deciding-which-convention-to-use","text":"To improve performance in CI, keys should follow a convention that gives the narrowest scope possible. For example, if a value is only used for one service or will vary depending on the service then it should be scoped to either service/common or service/environment Only values that are used by everyone should be scoped to common/ or environment/","title":"Deciding which convention to use"},{"location":"z_content-to-be-revised/standards/configuration-management/#exceptions-to-the-rule","text":"The are some values that are exceptions to this rule because of dynamic infrastructure provisioning and related test execution in CI. Note: these exceptions only apply to the Sandpit ( SND ) environment.","title":"Exceptions to the rule"},{"location":"z_content-to-be-revised/standards/configuration-management/#ingress-values","text":"FFC Helm charts define their ingress resources like the below: ingress: server: endpoint: class: These values must use the hierarchy environment/key for example dev/ingress.server and use labels to separate each service's value.","title":"Ingress values"},{"location":"z_content-to-be-revised/standards/configuration-management/#database-values","text":"FFC Helm charts define their database names like the below: postgresService: postgresUser: postgresDb: These values must use the hierarchy environment/key for example dev/postgresService.postgresUser and use labels to separate each service's value.","title":"Database values"},{"location":"z_content-to-be-revised/standards/configuration-management/#updating-values","text":"In Sandpit, values are added direct to Application Configuration via the Azure Portal or Azure CLI. In higher environments values are added by an Azure DevOps pipeline that sources configuration from a git repository . Full details for maintaining application configuration can be found in this wiki .","title":"Updating values"},{"location":"z_content-to-be-revised/standards/documentation/","text":"Documentation FFC products are delivered following an agile methodology based on evidenced user needs. Documentation is expected to follow this mantra and be iterated with the code as the product evolves. Documentation should be \"just enough\" to cover what needs to be communicated and presented as simply and effectively as possible. Documentation Location Documentation will be stored in the same source code repository as the product it supports. FFC developments will mostly be within an open source microservices ecosystem. Each microservice will have its own repository and its own documentation. Readme A readme file must exist in every repo in markdown format (.md). It must include the following if they apply: - Description of product - Prerequisites - Development tools setup - Test tools setup - How to run in development - How to run tests - How to make changes to the code Additional Detail Depending on the complexity of the product, the following may be included in the readme, or it may be more effective to capture in separate documentation/diagrams referenced by the readme. How product fits into wider architecture Internal product architecture Data structure API end points Monitoring Error handling Audit User access Security Complexity worth documenting Pipelines Microservice Reference Library This is a reference library of all microservices created and maintained by FFC. The purpose of which is so we have an easy access resource to understand what capability we have developed, but also to promote reuse where appropriate. The registry is updated initially by the Solution Architect when designing the service. It is then updated and maintained by the developer as the service iterates through it's lifecycle. The registry is in Confluence Library Content Name of service Owner of service Link to Git repository Link to Jira backlog Summary of service's purpose and capability Architectural diagrams following C4 model, see c4model.com for more information Technology stack Dependencies API specification, can be link to OpenAPI specification file Events, can be link, can be link to AsyncAPI specification file Functions Version history, can be link to GitHub Usage, ie which services use this microservice Useful references Architecture Documentation There may be documentation owned by FFC architecture that developers need to contribute to. For example, a Defra wide API endpoint catalogue or a Cloud Centre of Excellence namespace library. These documents should be completed in line with the standards of the owner. Architectural Decisions Architectural decisions are discussed and agreed at FFC Technical Design Authority (TDA) chaired by the Chief Architect. Agreed outputs are captured as part of the TDA by Architecture. Technical Debt A current view of technical debt is maintained by TDA. Developers are expected to contribute to forming this vision. Code Effective software engineering can negate the need for additional documentation. Code should follow Defra's digital standards and apply appropriate recognised design patterns. We should aim to write readable code that requires minimal comments as per Defra's digital standards. However in some scenarios effective code comments can be used to explain complexity. Well named and considered unit tests can also act as effective documentation. Issues Work items are captured in the form Jira issues. There is no need to duplicate issues in additional documentation. Issues should be referenced in associated Pull Requests. Sensitive Documentation Sensitive documentation should be stored within a private repository within Azure DevOps or Confluence depending on the audience.","title":"Documentation"},{"location":"z_content-to-be-revised/standards/documentation/#documentation","text":"FFC products are delivered following an agile methodology based on evidenced user needs. Documentation is expected to follow this mantra and be iterated with the code as the product evolves. Documentation should be \"just enough\" to cover what needs to be communicated and presented as simply and effectively as possible.","title":"Documentation"},{"location":"z_content-to-be-revised/standards/documentation/#documentation-location","text":"Documentation will be stored in the same source code repository as the product it supports. FFC developments will mostly be within an open source microservices ecosystem. Each microservice will have its own repository and its own documentation.","title":"Documentation Location"},{"location":"z_content-to-be-revised/standards/documentation/#readme","text":"A readme file must exist in every repo in markdown format (.md). It must include the following if they apply: - Description of product - Prerequisites - Development tools setup - Test tools setup - How to run in development - How to run tests - How to make changes to the code","title":"Readme"},{"location":"z_content-to-be-revised/standards/documentation/#additional-detail","text":"Depending on the complexity of the product, the following may be included in the readme, or it may be more effective to capture in separate documentation/diagrams referenced by the readme. How product fits into wider architecture Internal product architecture Data structure API end points Monitoring Error handling Audit User access Security Complexity worth documenting Pipelines","title":"Additional Detail"},{"location":"z_content-to-be-revised/standards/documentation/#microservice-reference-library","text":"This is a reference library of all microservices created and maintained by FFC. The purpose of which is so we have an easy access resource to understand what capability we have developed, but also to promote reuse where appropriate. The registry is updated initially by the Solution Architect when designing the service. It is then updated and maintained by the developer as the service iterates through it's lifecycle. The registry is in Confluence","title":"Microservice Reference Library"},{"location":"z_content-to-be-revised/standards/documentation/#library-content","text":"Name of service Owner of service Link to Git repository Link to Jira backlog Summary of service's purpose and capability Architectural diagrams following C4 model, see c4model.com for more information Technology stack Dependencies API specification, can be link to OpenAPI specification file Events, can be link, can be link to AsyncAPI specification file Functions Version history, can be link to GitHub Usage, ie which services use this microservice Useful references","title":"Library Content"},{"location":"z_content-to-be-revised/standards/documentation/#architecture-documentation","text":"There may be documentation owned by FFC architecture that developers need to contribute to. For example, a Defra wide API endpoint catalogue or a Cloud Centre of Excellence namespace library. These documents should be completed in line with the standards of the owner.","title":"Architecture Documentation"},{"location":"z_content-to-be-revised/standards/documentation/#architectural-decisions","text":"Architectural decisions are discussed and agreed at FFC Technical Design Authority (TDA) chaired by the Chief Architect. Agreed outputs are captured as part of the TDA by Architecture.","title":"Architectural Decisions"},{"location":"z_content-to-be-revised/standards/documentation/#technical-debt","text":"A current view of technical debt is maintained by TDA. Developers are expected to contribute to forming this vision.","title":"Technical Debt"},{"location":"z_content-to-be-revised/standards/documentation/#code","text":"Effective software engineering can negate the need for additional documentation. Code should follow Defra's digital standards and apply appropriate recognised design patterns. We should aim to write readable code that requires minimal comments as per Defra's digital standards. However in some scenarios effective code comments can be used to explain complexity. Well named and considered unit tests can also act as effective documentation.","title":"Code"},{"location":"z_content-to-be-revised/standards/documentation/#issues","text":"Work items are captured in the form Jira issues. There is no need to duplicate issues in additional documentation. Issues should be referenced in associated Pull Requests.","title":"Issues"},{"location":"z_content-to-be-revised/standards/documentation/#sensitive-documentation","text":"Sensitive documentation should be stored within a private repository within Azure DevOps or Confluence depending on the audience.","title":"Sensitive Documentation"},{"location":"z_content-to-be-revised/standards/events/","text":"Events and messages Where possible, service to service communication will be asynchronous using Azure Service Bus. Depending on the pattern being applied the term event or message may be more appropriate. For simplicity, for the remainder of this page, the term event will be used to refer to both these definitions. Use Azure Service Bus Services should use Azure Service Bus as an event broker. This is because the FFC CI pipeline is relatively mature in terms of dynamically provisioning Azure Service Bus infrastructure to support the development cycle and the team have strong experience with it's use. Avoid tight coupling Services should be as loosely coupled as possible. This reduces the dependency a change in one service could have on related services. Therefore, events should be published to Topics as opposed to Queues where possible. Format When sending an event to Azure Service Bus, it will be decorated by default broker properties. For full details on the specification, see the Microsoft documentation Mandatory custom properties All FFC events from any service must also declare the following custom properties when publishing an event. type - the type of event, this should be prefixed with reverse DNS and describe the object and event occuring on that object in the format: <reverse DNS>.ffc.<object>.<event> . For example, uk.gov.ffc.demo.claim.validated source - the service the event originated from, eg ffc-demo-web subject (optional) - the subject which will give consumers context, for example when the body cannot be consistently read or contains blob data These additional properties are based on values included in the CloudEvents specification that are not included in the Azure Service Bus envelope. Refer to the Cloud Events specification for further examples of their usage.","title":"Events and messages"},{"location":"z_content-to-be-revised/standards/events/#events-and-messages","text":"Where possible, service to service communication will be asynchronous using Azure Service Bus. Depending on the pattern being applied the term event or message may be more appropriate. For simplicity, for the remainder of this page, the term event will be used to refer to both these definitions.","title":"Events and messages"},{"location":"z_content-to-be-revised/standards/events/#use-azure-service-bus","text":"Services should use Azure Service Bus as an event broker. This is because the FFC CI pipeline is relatively mature in terms of dynamically provisioning Azure Service Bus infrastructure to support the development cycle and the team have strong experience with it's use.","title":"Use Azure Service Bus"},{"location":"z_content-to-be-revised/standards/events/#avoid-tight-coupling","text":"Services should be as loosely coupled as possible. This reduces the dependency a change in one service could have on related services. Therefore, events should be published to Topics as opposed to Queues where possible.","title":"Avoid tight coupling"},{"location":"z_content-to-be-revised/standards/events/#format","text":"When sending an event to Azure Service Bus, it will be decorated by default broker properties. For full details on the specification, see the Microsoft documentation","title":"Format"},{"location":"z_content-to-be-revised/standards/events/#mandatory-custom-properties","text":"All FFC events from any service must also declare the following custom properties when publishing an event. type - the type of event, this should be prefixed with reverse DNS and describe the object and event occuring on that object in the format: <reverse DNS>.ffc.<object>.<event> . For example, uk.gov.ffc.demo.claim.validated source - the service the event originated from, eg ffc-demo-web subject (optional) - the subject which will give consumers context, for example when the body cannot be consistently read or contains blob data These additional properties are based on values included in the CloudEvents specification that are not included in the Azure Service Bus envelope. Refer to the Cloud Events specification for further examples of their usage.","title":"Mandatory custom properties"},{"location":"z_content-to-be-revised/standards/jenkins/","text":"Jenkins Jenkins is used for CI pipelines across all FFC teams. Where possible, build steps are containerised to reduce plugin and Jenkins configuration dependencies. Shared library All microservices use the FFC CI pipeline for managing builds. This pipeline abstracts complexity such as infrastructure provisioning, building and testing away from consuming microservices, significantly reducing the effort needed to build and deploy services in line with other FFC standards. Jenkinsfiles Jenkinsfiles use the Groovy syntax, adhering to the official Apache Groovy style guide Microservice Jenkinsfiles are source controlled in the same repository as the microservice itself. Secret values and sensitive configuration are not included in Jenkinsfiles, instead use either a parameter or environment variable if needed. Credentials Jenkins credentials should only be used for Jenkins pipeline specific activities. They should not hold credentials relating to microservice configuration or deployment. These credentials should be sourced from Azure Key Vault or for files, a private Azure Repository. Workspace folders All delivery teams will have their own workspace folders to logically separate their pipelines and credentials. The naming convention of these folders will be ffc-<service> . For example, ffc-grants . Each microservice should have it's own Build and Deploy pipeline. Builds Naming convention: ffc-<service>-build Triggered from a GitHub webhook, this pipeline will build and test the microservice. Deployments Naming convention: ffc-<service>-deploy Triggered from a build targetting the main branch, this pipeline will deploy the microservice to the Sandpit environment. Note: the naming convention must be followed to ensure the pipeline is triggered by the build.","title":"Jenkins"},{"location":"z_content-to-be-revised/standards/jenkins/#jenkins","text":"Jenkins is used for CI pipelines across all FFC teams. Where possible, build steps are containerised to reduce plugin and Jenkins configuration dependencies.","title":"Jenkins"},{"location":"z_content-to-be-revised/standards/jenkins/#shared-library","text":"All microservices use the FFC CI pipeline for managing builds. This pipeline abstracts complexity such as infrastructure provisioning, building and testing away from consuming microservices, significantly reducing the effort needed to build and deploy services in line with other FFC standards.","title":"Shared library"},{"location":"z_content-to-be-revised/standards/jenkins/#jenkinsfiles","text":"Jenkinsfiles use the Groovy syntax, adhering to the official Apache Groovy style guide Microservice Jenkinsfiles are source controlled in the same repository as the microservice itself. Secret values and sensitive configuration are not included in Jenkinsfiles, instead use either a parameter or environment variable if needed.","title":"Jenkinsfiles"},{"location":"z_content-to-be-revised/standards/jenkins/#credentials","text":"Jenkins credentials should only be used for Jenkins pipeline specific activities. They should not hold credentials relating to microservice configuration or deployment. These credentials should be sourced from Azure Key Vault or for files, a private Azure Repository.","title":"Credentials"},{"location":"z_content-to-be-revised/standards/jenkins/#workspace-folders","text":"All delivery teams will have their own workspace folders to logically separate their pipelines and credentials. The naming convention of these folders will be ffc-<service> . For example, ffc-grants . Each microservice should have it's own Build and Deploy pipeline.","title":"Workspace folders"},{"location":"z_content-to-be-revised/standards/jenkins/#builds","text":"Naming convention: ffc-<service>-build Triggered from a GitHub webhook, this pipeline will build and test the microservice.","title":"Builds"},{"location":"z_content-to-be-revised/standards/jenkins/#deployments","text":"Naming convention: ffc-<service>-deploy Triggered from a build targetting the main branch, this pipeline will deploy the microservice to the Sandpit environment. Note: the naming convention must be followed to ensure the pipeline is triggered by the build.","title":"Deployments"},{"location":"z_content-to-be-revised/standards/quick-reference/","text":"Quick reference This page represents a summary of the core standards teams should follow when delivering services for FFC. These standards are intended to support a consistent interpretation of the Architecture Vision across the programme allowing as much flexibility as possible. follow the Architecture Vision design containerised microservices around business domains and bounded contexts use asynchronous communication between services via Azure Service Bus orchestrate containers using Kubernetes follow Defra's wider software development standards microservices should be built primarily using Node.js unless there is an evidenced reason not to if a web framework is needed, use Hapi.js if Node.js is not appropriate, then use .NET use PostgreSQL for relational databases avoid storing secrets in the cluster where possible, instead use AAD Pod Identity follow the agreed test structure in each microservice repository use contract testing with Pact to support e2e testing use the FFC shared library for CI use the FFC Helm chart library as a base for Helm charts use the Defra Node.js and .NET docker base images follow feature branch development with PR deployments work in an agile manner keep deployments small and frequent do not store live data in any environment other than production","title":"Quick reference"},{"location":"z_content-to-be-revised/standards/quick-reference/#quick-reference","text":"This page represents a summary of the core standards teams should follow when delivering services for FFC. These standards are intended to support a consistent interpretation of the Architecture Vision across the programme allowing as much flexibility as possible. follow the Architecture Vision design containerised microservices around business domains and bounded contexts use asynchronous communication between services via Azure Service Bus orchestrate containers using Kubernetes follow Defra's wider software development standards microservices should be built primarily using Node.js unless there is an evidenced reason not to if a web framework is needed, use Hapi.js if Node.js is not appropriate, then use .NET use PostgreSQL for relational databases avoid storing secrets in the cluster where possible, instead use AAD Pod Identity follow the agreed test structure in each microservice repository use contract testing with Pact to support e2e testing use the FFC shared library for CI use the FFC Helm chart library as a base for Helm charts use the Defra Node.js and .NET docker base images follow feature branch development with PR deployments work in an agile manner keep deployments small and frequent do not store live data in any environment other than production","title":"Quick reference"},{"location":"z_content-to-be-revised/standards/release-notes/","text":"Release notes We want release notes to be consistent. These standards are for when we manually create a release note, for example within a GitHub repository. For the Description section use the Semantic versioning standards as a guide. Release Note Standards Title semver Version (e.g. 1.0.0 ) Description Major bullet list of major changes Minor bullet list of minor changes Patch bullet list of patch changes Each of the above will be optional and dependant on the changes that have been made. For complex changes include code snippets in the relevant description heading.","title":"Release notes"},{"location":"z_content-to-be-revised/standards/release-notes/#release-notes","text":"We want release notes to be consistent. These standards are for when we manually create a release note, for example within a GitHub repository. For the Description section use the Semantic versioning standards as a guide.","title":"Release notes"},{"location":"z_content-to-be-revised/standards/release-notes/#release-note-standards","text":"","title":"Release Note Standards"},{"location":"z_content-to-be-revised/standards/release-notes/#title","text":"semver Version (e.g. 1.0.0 )","title":"Title"},{"location":"z_content-to-be-revised/standards/release-notes/#description","text":"","title":"Description"},{"location":"z_content-to-be-revised/standards/release-notes/#major","text":"bullet list of major changes","title":"Major"},{"location":"z_content-to-be-revised/standards/release-notes/#minor","text":"bullet list of minor changes","title":"Minor"},{"location":"z_content-to-be-revised/standards/release-notes/#patch","text":"bullet list of patch changes Each of the above will be optional and dependant on the changes that have been made. For complex changes include code snippets in the relevant description heading.","title":"Patch"},{"location":"z_content-to-be-revised/standards/route-to-live/","text":"Route to live Teams across the FFC programme follow the below core workflow. Individual delivery teams are free to add their own processes on top of these core values to best fit their context. Process for change This applies equally to a new feature or bug fix. developer creates a feature branch from main branch developer creates an empty commit with the title of the change and a link to Jira ticket developer opens draft PR for change developer changes semantic version of application developer write code in line with team standards, following TDD where appropriate as developer commits, CI builds and deploys to isolated PR environment CI runs tests to assure code quality when ready, code is reviewed using PR deployment and CI outputs to support if review passes code is merged to main branch CI automatically tears down PR deployment CI packages, versions and prepares for deployment to higher environments pipeline automatically deploys to Sandpit and Development environments sequentially developer approves deployment to Test environment developer raises ticket with CCoE to deploy to PreProduction environment developer prepares run book for PreProduction deployment CCoE approves deployment to PreProduction environment developer raises change request within myIT to deploy to Production environment developer prepares run book for Production deployment CCoE approves deployment to Production environment Environments Shift left Teams should follow a shift left approach to testing where as much testing is performed as early as possible in the development process. The FFC CI pipeline with it's dynamic infrastructure provisioning and test execution is a key enabler of this approach. Test environments are primarily used for end to end, user acceptance and performance testing that are typically not possible earlier. Data Production is the only environment where \"real\" data is permitted. All other environments should use synthetic data.","title":"Route to live"},{"location":"z_content-to-be-revised/standards/route-to-live/#route-to-live","text":"Teams across the FFC programme follow the below core workflow. Individual delivery teams are free to add their own processes on top of these core values to best fit their context.","title":"Route to live"},{"location":"z_content-to-be-revised/standards/route-to-live/#process-for-change","text":"This applies equally to a new feature or bug fix. developer creates a feature branch from main branch developer creates an empty commit with the title of the change and a link to Jira ticket developer opens draft PR for change developer changes semantic version of application developer write code in line with team standards, following TDD where appropriate as developer commits, CI builds and deploys to isolated PR environment CI runs tests to assure code quality when ready, code is reviewed using PR deployment and CI outputs to support if review passes code is merged to main branch CI automatically tears down PR deployment CI packages, versions and prepares for deployment to higher environments pipeline automatically deploys to Sandpit and Development environments sequentially developer approves deployment to Test environment developer raises ticket with CCoE to deploy to PreProduction environment developer prepares run book for PreProduction deployment CCoE approves deployment to PreProduction environment developer raises change request within myIT to deploy to Production environment developer prepares run book for Production deployment CCoE approves deployment to Production environment","title":"Process for change"},{"location":"z_content-to-be-revised/standards/route-to-live/#environments","text":"","title":"Environments"},{"location":"z_content-to-be-revised/standards/route-to-live/#shift-left","text":"Teams should follow a shift left approach to testing where as much testing is performed as early as possible in the development process. The FFC CI pipeline with it's dynamic infrastructure provisioning and test execution is a key enabler of this approach. Test environments are primarily used for end to end, user acceptance and performance testing that are typically not possible earlier.","title":"Shift left"},{"location":"z_content-to-be-revised/standards/route-to-live/#data","text":"Production is the only environment where \"real\" data is permitted. All other environments should use synthetic data.","title":"Data"},{"location":"z_content-to-be-revised/standards/secrets-management/","text":"Secrets Management Client-side git secret detection All developers are required to install detect-secrets when working on open-source FFC git repositories in GitHub. Server-side git secret detection Jenkins will scan for potential secrets in all open-source FFC git repositories in GitHub. Any secrets detected will be reported to the #secret-detection Slack channel in the ffc-notifications workspace.","title":"Secrets Management"},{"location":"z_content-to-be-revised/standards/secrets-management/#secrets-management","text":"","title":"Secrets Management"},{"location":"z_content-to-be-revised/standards/secrets-management/#client-side-git-secret-detection","text":"All developers are required to install detect-secrets when working on open-source FFC git repositories in GitHub.","title":"Client-side git secret detection"},{"location":"z_content-to-be-revised/standards/secrets-management/#server-side-git-secret-detection","text":"Jenkins will scan for potential secrets in all open-source FFC git repositories in GitHub. Any secrets detected will be reported to the #secret-detection Slack channel in the ffc-notifications workspace.","title":"Server-side git secret detection"},{"location":"z_content-to-be-revised/standards/source-code/","text":"Source code The default position for FFC is that code should be open source and hosted within GitHub. If there is a need for a private repository then a private GitHub repository should be used instead. Creating repositories Naming repositories Repositories follow the below naming convention. ffc-<workstream>-<service> For example, ffc-elm-apply . Following this naming convention helps understand ownership of repositories and is also essential for CI/CD pipelines to function correctly. Templates For Node.js repositories, the ffc-template-node template repository should be used as a base. Managing access The FFC GitHub team is added with Write access to all repositories hosted on GitHub. As per Defra policy , contractors should be added as contributors and should not be part of the FFC Team in GitHub. Branch policies main branches are protected by a branch policy where merges requires a Pull Request approved by at least one other person. stale reviews are automatically dismissed. repositories only allow Squash merging. must require signed commit Secret detection All developers are required to install detect-secrets when working on open-source FFC git repositories in github. When setting up a new FFC git repository, it needs to be configured to work with the client-side detect-secrets tool: Add the configuration file called .pre-commit-config.yaml to the repository root directory with the following contents: repos: - repo: https://github.com/Yelp/detect-secrets rev: v0.14.3 hooks: - id: detect-secrets args: ['--baseline', '.secrets.baseline'] Create and add the baseline file. To exclude files from the detect-secrets scan, run with --exclude-files <regex> . Examples: detect-secrets scan > .secrets.baseline detect-secrets scan --exclude-files package-lock.json > .secrets.baseline Licence The Open Government Licence (OGL) Version 3 should be added to the repository. A file version is available in this repository The Open Government Licence (OGL) Version 3 Copyright (c) 2022 Defra This source code is licensed under the Open Government Licence v3.0. To view this licence, visit www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to the Information Policy Team, The National Archives, Kew, Richmond, Surrey, TW9 4DU. Readme A README.md should be added to initialise the repository. As work begins on the repository, the README must adhere to FFC documentation standards . Branch naming convention Branches should be named in the format <ticket id>-<description of change> in lower case. For example, 1234567-add-dockerfile . Pull Requests When following feature branch development, pull requests should be used to collaborate and ensure code quality. Pull requests should be created in line with Defra's standards and reviewed in the spirit of FFC's code review standards . Pull requests should trigger a build and a PR deployment to support the review. Completing Pull Request In order to complete a pull request, all of the following conditions must be met: - at least one approver - PR build must complete successfully - dismiss stale reviews On completion of a pull request a squash merge is performed and the feature/bug branch deleted. The commit message should be amended to give a concise description of the changes, rather than the default list of individual commits.","title":"Source code"},{"location":"z_content-to-be-revised/standards/source-code/#source-code","text":"The default position for FFC is that code should be open source and hosted within GitHub. If there is a need for a private repository then a private GitHub repository should be used instead.","title":"Source code"},{"location":"z_content-to-be-revised/standards/source-code/#creating-repositories","text":"","title":"Creating repositories"},{"location":"z_content-to-be-revised/standards/source-code/#naming-repositories","text":"Repositories follow the below naming convention. ffc-<workstream>-<service> For example, ffc-elm-apply . Following this naming convention helps understand ownership of repositories and is also essential for CI/CD pipelines to function correctly.","title":"Naming repositories"},{"location":"z_content-to-be-revised/standards/source-code/#templates","text":"For Node.js repositories, the ffc-template-node template repository should be used as a base.","title":"Templates"},{"location":"z_content-to-be-revised/standards/source-code/#managing-access","text":"The FFC GitHub team is added with Write access to all repositories hosted on GitHub. As per Defra policy , contractors should be added as contributors and should not be part of the FFC Team in GitHub.","title":"Managing access"},{"location":"z_content-to-be-revised/standards/source-code/#branch-policies","text":"main branches are protected by a branch policy where merges requires a Pull Request approved by at least one other person. stale reviews are automatically dismissed. repositories only allow Squash merging. must require signed commit","title":"Branch policies"},{"location":"z_content-to-be-revised/standards/source-code/#secret-detection","text":"All developers are required to install detect-secrets when working on open-source FFC git repositories in github. When setting up a new FFC git repository, it needs to be configured to work with the client-side detect-secrets tool: Add the configuration file called .pre-commit-config.yaml to the repository root directory with the following contents: repos: - repo: https://github.com/Yelp/detect-secrets rev: v0.14.3 hooks: - id: detect-secrets args: ['--baseline', '.secrets.baseline'] Create and add the baseline file. To exclude files from the detect-secrets scan, run with --exclude-files <regex> . Examples: detect-secrets scan > .secrets.baseline detect-secrets scan --exclude-files package-lock.json > .secrets.baseline","title":"Secret detection"},{"location":"z_content-to-be-revised/standards/source-code/#licence","text":"The Open Government Licence (OGL) Version 3 should be added to the repository. A file version is available in this repository The Open Government Licence (OGL) Version 3 Copyright (c) 2022 Defra This source code is licensed under the Open Government Licence v3.0. To view this licence, visit www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to the Information Policy Team, The National Archives, Kew, Richmond, Surrey, TW9 4DU.","title":"Licence"},{"location":"z_content-to-be-revised/standards/source-code/#readme","text":"A README.md should be added to initialise the repository. As work begins on the repository, the README must adhere to FFC documentation standards .","title":"Readme"},{"location":"z_content-to-be-revised/standards/source-code/#branch-naming-convention","text":"Branches should be named in the format <ticket id>-<description of change> in lower case. For example, 1234567-add-dockerfile .","title":"Branch naming convention"},{"location":"z_content-to-be-revised/standards/source-code/#pull-requests","text":"When following feature branch development, pull requests should be used to collaborate and ensure code quality. Pull requests should be created in line with Defra's standards and reviewed in the spirit of FFC's code review standards . Pull requests should trigger a build and a PR deployment to support the review.","title":"Pull Requests"},{"location":"z_content-to-be-revised/standards/source-code/#completing-pull-request","text":"In order to complete a pull request, all of the following conditions must be met: - at least one approver - PR build must complete successfully - dismiss stale reviews On completion of a pull request a squash merge is performed and the feature/bug branch deleted. The commit message should be amended to give a concise description of the changes, rather than the default list of individual commits.","title":"Completing Pull Request"},{"location":"z_content-to-be-revised/standards/technology-stack/","text":"Technology Stack All technology choices are in line with Defra Digital\u2019s Tools Radar These choice have been used as a baseline to help decide which technologies to build services with. Where there are gaps not covering FFC needs we have aligned ourselves to the FFC architectural vision, Cloud Centre of Excellence recognised patterns and lessons learned from other digital teams. When developing services, teams must use the following technology stack. Where a new need is not covered by the below technology stack, collaboration between FFC architecture and the Tools Authority must take place to agree how to fill the gap. Microservice Development Node.js (with Hapi.js) npm .NET Nuget Docker Docker Compose Helm CI/CD Jenkins (Groovy syntax) ARM templates Azure CLI Azure DevOps Testing Jest xUnit Pact Web Driver IO Cucumber Selenium BrowserStack JMeter Snyk OWASP Zap AXE WAVE Anchore Engine Cloud (private) Azure Kubernetes Service (AKS) Azure Container Registry (ACR) Azure PostgreSQL Azure Service Bus Azure Event Hubs Azure Application Configuration Azure Key Vault Azure Functions Azure Storage AAD Pod Identity Application Insights Azure Repos Cloud (public) GitHub SonarCloud DockerHub BrowserStack npm NuGet Snyk Google Analytics","title":"Technology Stack"},{"location":"z_content-to-be-revised/standards/technology-stack/#technology-stack","text":"All technology choices are in line with Defra Digital\u2019s Tools Radar These choice have been used as a baseline to help decide which technologies to build services with. Where there are gaps not covering FFC needs we have aligned ourselves to the FFC architectural vision, Cloud Centre of Excellence recognised patterns and lessons learned from other digital teams. When developing services, teams must use the following technology stack. Where a new need is not covered by the below technology stack, collaboration between FFC architecture and the Tools Authority must take place to agree how to fill the gap.","title":"Technology Stack"},{"location":"z_content-to-be-revised/standards/technology-stack/#microservice-development","text":"Node.js (with Hapi.js) npm .NET Nuget Docker Docker Compose Helm","title":"Microservice Development"},{"location":"z_content-to-be-revised/standards/technology-stack/#cicd","text":"Jenkins (Groovy syntax) ARM templates Azure CLI Azure DevOps","title":"CI/CD"},{"location":"z_content-to-be-revised/standards/technology-stack/#testing","text":"Jest xUnit Pact Web Driver IO Cucumber Selenium BrowserStack JMeter Snyk OWASP Zap AXE WAVE Anchore Engine","title":"Testing"},{"location":"z_content-to-be-revised/standards/technology-stack/#cloud-private","text":"Azure Kubernetes Service (AKS) Azure Container Registry (ACR) Azure PostgreSQL Azure Service Bus Azure Event Hubs Azure Application Configuration Azure Key Vault Azure Functions Azure Storage AAD Pod Identity Application Insights Azure Repos","title":"Cloud (private)"},{"location":"z_content-to-be-revised/standards/technology-stack/#cloud-public","text":"GitHub SonarCloud DockerHub BrowserStack npm NuGet Snyk Google Analytics","title":"Cloud (public)"},{"location":"z_content-to-be-revised/standards/testing/","text":"Testing To support developer agility and consistent interpretation of the FFC Test Strategy, a common test structure has been agreed for usage across FFC microservices. Static code SonarCloud will provide static code analysis on all repositories regardless of technology choice. In addition to SonarCloud, any JavaScript code is also linted using StandardJs . This linting is performed of a test script in package.json which is also run in CI pipeline. There is no linting used for C# in the programme at present. Unit Unit tests should be used to: target functionality that is not already covered through contract or integration testing add extra assurance or visibility to specific functionality, for example an application that runs a series of calculations to determine a monetary value, may benefit from individual unit tests for each calculation. Code should be written so that where unit tests are required, mocking is not required if possible or is minimal. Unit tests should mock immediate dependencies only. Integration Integration tests verify the interactions of several components, rather than a single unit. They are classified further into narrow and local integration tests. Narrow integration Narrow integration tests should be considered as tests that are bigger than unit tests, but smaller than integration involving other services or modules. For example where a unit test would traditionally mock a web server, a narrow integration test may start the web server albeit without exposing any ports externally. Narrow integration tests should mock immediate dependencies only. Local integration Local integration tests should cover functionality that integrates with dependencies that are local to the service, for example message queues and databases. Rather than mocking the database or queue, local integration tests could use a database or message queue container. This reduces the need for mocks, increasing developer agility, reducing test complexity and a higher level of confidence in tests. Where integration with third party services are needed, not covered by contract tests, then tests should be make use use of Docker containers and stubs to reduce the need for mocking. Contract Contract tests should cover all provider-consumer relationships with other services that are owned by FFC. Where one side of the contract is not owned by FFC then more traditional provider-driven or even broad integration tests with test instances of those services may be preferred. Contract tests should mock the immediate dependencies of the contract similar to unit tests. Service architectures should be written in such a way that reduce the need for these higher risk contracts. For example introducing an FFC managed API gateway between third party services and FFC, would allow all FFC services to contract test against the API gateway abstraction. More complicated end to end testing is then restricted to only the API gateway. Acceptance Acceptance tests should cover key journeys through the application flow via the user interface. These are defined in a Gherkin syntax (Given, When, Then) and use cucumber and webdriver.io to mimic a scenario from the user perspective. They\u2019re executed against the microservices integrated and running as the complete service with the exception of external services (outside of the platform) which are stubbed. Repository structure Node.js app - all application code test unit - unit tests integration - integration tests narrow - narrow integration tests local - local integration tests contract - contract tests acceptance - acceptance tests (for frontend applications) The integration tests are separated in the folder structure to reflect their different dependencies. This enables narrow integration tests that have no external dependencies to be easily run in isolation from the local integration tests. Standards test files should be suffixed with test.js test subfolders should be as flat as possible and not mirror application code structure tests should be named so it is clear which module they relate to. For example a module caclulations/gross.js could have a test file called, calcuation-gross.test.js .NET Core App - application csproj App.Tests - application tests csproj Integration Unit Contract Acceptance (for frontend applications) Standards tests should be named so it is clear which module they relate to. all test types should be in one project Running tests all tests should be executed in CI a docker-compose.test.yaml must exist in the repository to support running tests in CI. for local development, it may be beneficial to create a script/ Docker Compose combination to aid test environment setup and to take advantage of file watching.","title":"Testing"},{"location":"z_content-to-be-revised/standards/testing/#testing","text":"To support developer agility and consistent interpretation of the FFC Test Strategy, a common test structure has been agreed for usage across FFC microservices.","title":"Testing"},{"location":"z_content-to-be-revised/standards/testing/#static-code","text":"SonarCloud will provide static code analysis on all repositories regardless of technology choice. In addition to SonarCloud, any JavaScript code is also linted using StandardJs . This linting is performed of a test script in package.json which is also run in CI pipeline. There is no linting used for C# in the programme at present.","title":"Static code"},{"location":"z_content-to-be-revised/standards/testing/#unit","text":"Unit tests should be used to: target functionality that is not already covered through contract or integration testing add extra assurance or visibility to specific functionality, for example an application that runs a series of calculations to determine a monetary value, may benefit from individual unit tests for each calculation. Code should be written so that where unit tests are required, mocking is not required if possible or is minimal. Unit tests should mock immediate dependencies only.","title":"Unit"},{"location":"z_content-to-be-revised/standards/testing/#integration","text":"Integration tests verify the interactions of several components, rather than a single unit. They are classified further into narrow and local integration tests.","title":"Integration"},{"location":"z_content-to-be-revised/standards/testing/#narrow-integration","text":"Narrow integration tests should be considered as tests that are bigger than unit tests, but smaller than integration involving other services or modules. For example where a unit test would traditionally mock a web server, a narrow integration test may start the web server albeit without exposing any ports externally. Narrow integration tests should mock immediate dependencies only.","title":"Narrow integration"},{"location":"z_content-to-be-revised/standards/testing/#local-integration","text":"Local integration tests should cover functionality that integrates with dependencies that are local to the service, for example message queues and databases. Rather than mocking the database or queue, local integration tests could use a database or message queue container. This reduces the need for mocks, increasing developer agility, reducing test complexity and a higher level of confidence in tests. Where integration with third party services are needed, not covered by contract tests, then tests should be make use use of Docker containers and stubs to reduce the need for mocking.","title":"Local integration"},{"location":"z_content-to-be-revised/standards/testing/#contract","text":"Contract tests should cover all provider-consumer relationships with other services that are owned by FFC. Where one side of the contract is not owned by FFC then more traditional provider-driven or even broad integration tests with test instances of those services may be preferred. Contract tests should mock the immediate dependencies of the contract similar to unit tests. Service architectures should be written in such a way that reduce the need for these higher risk contracts. For example introducing an FFC managed API gateway between third party services and FFC, would allow all FFC services to contract test against the API gateway abstraction. More complicated end to end testing is then restricted to only the API gateway.","title":"Contract"},{"location":"z_content-to-be-revised/standards/testing/#acceptance","text":"Acceptance tests should cover key journeys through the application flow via the user interface. These are defined in a Gherkin syntax (Given, When, Then) and use cucumber and webdriver.io to mimic a scenario from the user perspective. They\u2019re executed against the microservices integrated and running as the complete service with the exception of external services (outside of the platform) which are stubbed.","title":"Acceptance"},{"location":"z_content-to-be-revised/standards/testing/#repository-structure","text":"","title":"Repository structure"},{"location":"z_content-to-be-revised/standards/testing/#nodejs","text":"app - all application code test unit - unit tests integration - integration tests narrow - narrow integration tests local - local integration tests contract - contract tests acceptance - acceptance tests (for frontend applications) The integration tests are separated in the folder structure to reflect their different dependencies. This enables narrow integration tests that have no external dependencies to be easily run in isolation from the local integration tests.","title":"Node.js"},{"location":"z_content-to-be-revised/standards/testing/#standards","text":"test files should be suffixed with test.js test subfolders should be as flat as possible and not mirror application code structure tests should be named so it is clear which module they relate to. For example a module caclulations/gross.js could have a test file called, calcuation-gross.test.js","title":"Standards"},{"location":"z_content-to-be-revised/standards/testing/#net-core","text":"App - application csproj App.Tests - application tests csproj Integration Unit Contract Acceptance (for frontend applications)","title":".NET Core"},{"location":"z_content-to-be-revised/standards/testing/#standards_1","text":"tests should be named so it is clear which module they relate to. all test types should be in one project","title":"Standards"},{"location":"z_content-to-be-revised/standards/testing/#running-tests","text":"all tests should be executed in CI a docker-compose.test.yaml must exist in the repository to support running tests in CI. for local development, it may be beneficial to create a script/ Docker Compose combination to aid test environment setup and to take advantage of file watching.","title":"Running tests"},{"location":"z_content-to-be-revised/standards/version-control/","text":"Version Control These standards are based on Defra Digital's version control standards , but are extended to cover not only the application version, but version control of Docker images and Helm charts. Semantic Versioning We use semantic versioning to manage code, images and charts. Given a version number MAJOR.MINOR.PATCH, increment the: - MAJOR version when you make incompatible API changes, - MINOR version when you add functionality in a backwards compatible manner, and - PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. Full details can be found at semver.org . Application for a Node.js microservice, the version is updated in the package.json file's version property. for a .NET microservice, the version is updated in the .csproj file in a <Version> tag. Image prior to pushing to a container registry, all Docker images must be tagged by CI. The tag must match the application version. if the image is created as part of a pull request workflow, then the PR number is instead used as the image tag. Helm chart Helm chart versions are updated in the Chart.yaml file's version property. as Helm charts are saved in the same repository as the application they manage, the version numbers are updated in sync with the application by CI. Helm charts for PR deployments are not be pushed to a Helm repository. Note when using the FFC Jenkins library , Helm chart versioning is handled automatically, so no action is needed Releases releases are packaged in the source code repository using the version number of the application by CI Databases databases use Liquibase changesets for deployment and rollback. all changesets are versioned independently of the application. there must be an initial 0.0.0 version before any changeset, to enable full rollback of a database","title":"Version Control"},{"location":"z_content-to-be-revised/standards/version-control/#version-control","text":"These standards are based on Defra Digital's version control standards , but are extended to cover not only the application version, but version control of Docker images and Helm charts.","title":"Version Control"},{"location":"z_content-to-be-revised/standards/version-control/#semantic-versioning","text":"We use semantic versioning to manage code, images and charts. Given a version number MAJOR.MINOR.PATCH, increment the: - MAJOR version when you make incompatible API changes, - MINOR version when you add functionality in a backwards compatible manner, and - PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. Full details can be found at semver.org .","title":"Semantic Versioning"},{"location":"z_content-to-be-revised/standards/version-control/#application","text":"for a Node.js microservice, the version is updated in the package.json file's version property. for a .NET microservice, the version is updated in the .csproj file in a <Version> tag.","title":"Application"},{"location":"z_content-to-be-revised/standards/version-control/#image","text":"prior to pushing to a container registry, all Docker images must be tagged by CI. The tag must match the application version. if the image is created as part of a pull request workflow, then the PR number is instead used as the image tag.","title":"Image"},{"location":"z_content-to-be-revised/standards/version-control/#helm-chart","text":"Helm chart versions are updated in the Chart.yaml file's version property. as Helm charts are saved in the same repository as the application they manage, the version numbers are updated in sync with the application by CI. Helm charts for PR deployments are not be pushed to a Helm repository. Note when using the FFC Jenkins library , Helm chart versioning is handled automatically, so no action is needed","title":"Helm chart"},{"location":"z_content-to-be-revised/standards/version-control/#releases","text":"releases are packaged in the source code repository using the version number of the application by CI","title":"Releases"},{"location":"z_content-to-be-revised/standards/version-control/#databases","text":"databases use Liquibase changesets for deployment and rollback. all changesets are versioned independently of the application. there must be an initial 0.0.0 version before any changeset, to enable full rollback of a database","title":"Databases"},{"location":"z_content-to-be-revised/standards/kubernetes/","text":"Kubernetes Standards supporting the usage of Kubernetes Contents Configuration Helm charts Labels Pod priority Probes Role Based Access Control (RBAC) Resource management Secrets","title":"Kubernetes"},{"location":"z_content-to-be-revised/standards/kubernetes/#kubernetes","text":"Standards supporting the usage of Kubernetes","title":"Kubernetes"},{"location":"z_content-to-be-revised/standards/kubernetes/#contents","text":"Configuration Helm charts Labels Pod priority Probes Role Based Access Control (RBAC) Resource management Secrets","title":"Contents"},{"location":"z_content-to-be-revised/standards/kubernetes/configuration/","text":"Configuration Configuration for an application running in a pod should be passed to the pod via a ConfigMap Kubernetes resource. The configuration values should be stored in Azure App Configuration and injected via delivery pipeline as documented in the Configuration Management page. Sensitive values should be passed to the pod via a Secret Kubernetes resource.","title":"Configuration"},{"location":"z_content-to-be-revised/standards/kubernetes/configuration/#configuration","text":"Configuration for an application running in a pod should be passed to the pod via a ConfigMap Kubernetes resource. The configuration values should be stored in Azure App Configuration and injected via delivery pipeline as documented in the Configuration Management page. Sensitive values should be passed to the pod via a Secret Kubernetes resource.","title":"Configuration"},{"location":"z_content-to-be-revised/standards/kubernetes/helm-charts/","text":"Helm chart Helm charts allow multiple Kubernetes resource definitions to be deployed and un-deployed as a single unit. Helm version 3 is used within FFC. Currently FFC is limited to version 3.6 due to breaking changes introduced in version 3.7 . Helm 2 should never be used due to security risk introduced by Tiller in a cluster. Source control Helm charts are source controlled in the same repository as the microservice they relate to Helm charts are saved in a helm directory and named the same as the repository. For example a chart referencing the ELM Payment Service would be stored in the ./helm/elm-payment-service directory Helm chart versions are automatically updated by CI in line with the application version Helm chart library to keep Helm charts DRY, the FFC Helm Chart Library is used as a base for all resource definitions consuming Helm charts only need to define where there is variation from the base Helm chart the library helps teams abstract complexity of Kubernetes resource definitions as well as providing a consistent approach to Helm chart development and container security Helm chart repository Helm charts are published by CI to a Helm chart repository within Azure Container Registry Helm charts are only published on a main branch build Helm charts are automatically versioned by CI pipeline to be consistent with the application version Helm charts for in flight Pull Requests are not published","title":"Helm chart"},{"location":"z_content-to-be-revised/standards/kubernetes/helm-charts/#helm-chart","text":"Helm charts allow multiple Kubernetes resource definitions to be deployed and un-deployed as a single unit. Helm version 3 is used within FFC. Currently FFC is limited to version 3.6 due to breaking changes introduced in version 3.7 . Helm 2 should never be used due to security risk introduced by Tiller in a cluster.","title":"Helm chart"},{"location":"z_content-to-be-revised/standards/kubernetes/helm-charts/#source-control","text":"Helm charts are source controlled in the same repository as the microservice they relate to Helm charts are saved in a helm directory and named the same as the repository. For example a chart referencing the ELM Payment Service would be stored in the ./helm/elm-payment-service directory Helm chart versions are automatically updated by CI in line with the application version","title":"Source control"},{"location":"z_content-to-be-revised/standards/kubernetes/helm-charts/#helm-chart-library","text":"to keep Helm charts DRY, the FFC Helm Chart Library is used as a base for all resource definitions consuming Helm charts only need to define where there is variation from the base Helm chart the library helps teams abstract complexity of Kubernetes resource definitions as well as providing a consistent approach to Helm chart development and container security","title":"Helm chart library"},{"location":"z_content-to-be-revised/standards/kubernetes/helm-charts/#helm-chart-repository","text":"Helm charts are published by CI to a Helm chart repository within Azure Container Registry Helm charts are only published on a main branch build Helm charts are automatically versioned by CI pipeline to be consistent with the application version Helm charts for in flight Pull Requests are not published","title":"Helm chart repository"},{"location":"z_content-to-be-revised/standards/kubernetes/labels/","text":"Labels Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organise and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object. In order to take full advantage of using labels, they should be applied on every resource object within a Helm chart. i.e. all deployments, services, ingresses etc. When using the FFC Helm Chart Library , these labels are automatically applied to all resource objects. Required labels Each Helm chart templated resource should have the below labels. Example placeholders are provided for values. metadata: labels: app: {{ quote .Values.namespace }} app.kubernetes.io/name: {{ quote .Values.name }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/version: {{ quote .Values.labels.version }} app.kubernetes.io/component: {{ quote .Values.labels.component }} app.kubernetes.io/part-of: {{ quote .Values.namespace }} app.kubernetes.io/managed-by: {{ .Release.Service }} helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }} environment: {{ quote .Values.environment }} Note Deployment resource objects should have two sets of labels, one for the actual deployment and another for the pod template the deployment manages. Selectors Services selectors should be matched by app and name. Selectors should be consistent otherwise updates to Helm charts will be rejected. selector: app: {{ quote .Values.name }} app.kubernetes.io/name: {{ quote .Values.name }} Further reading More information is available in Confluence","title":"Labels"},{"location":"z_content-to-be-revised/standards/kubernetes/labels/#labels","text":"Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organise and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object. In order to take full advantage of using labels, they should be applied on every resource object within a Helm chart. i.e. all deployments, services, ingresses etc. When using the FFC Helm Chart Library , these labels are automatically applied to all resource objects.","title":"Labels"},{"location":"z_content-to-be-revised/standards/kubernetes/labels/#required-labels","text":"Each Helm chart templated resource should have the below labels. Example placeholders are provided for values. metadata: labels: app: {{ quote .Values.namespace }} app.kubernetes.io/name: {{ quote .Values.name }} app.kubernetes.io/instance: {{ .Release.Name }} app.kubernetes.io/version: {{ quote .Values.labels.version }} app.kubernetes.io/component: {{ quote .Values.labels.component }} app.kubernetes.io/part-of: {{ quote .Values.namespace }} app.kubernetes.io/managed-by: {{ .Release.Service }} helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }} environment: {{ quote .Values.environment }} Note Deployment resource objects should have two sets of labels, one for the actual deployment and another for the pod template the deployment manages.","title":"Required labels"},{"location":"z_content-to-be-revised/standards/kubernetes/labels/#selectors","text":"Services selectors should be matched by app and name. Selectors should be consistent otherwise updates to Helm charts will be rejected. selector: app: {{ quote .Values.name }} app.kubernetes.io/name: {{ quote .Values.name }}","title":"Selectors"},{"location":"z_content-to-be-revised/standards/kubernetes/labels/#further-reading","text":"More information is available in Confluence","title":"Further reading"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/","text":"Pod priority Kubernetes Pods can have priority levels. Priority indicates the importance of a pod relative to other pods. If a pod cannot be scheduled, the scheduler tries to pre-empt (evict) lower priority pods to make scheduling of the pending pod possible. In the event of over utilisation of a cluster, Kubernetes will start to kill lower priority pods first to maintain stability. Priority levels available to FFC pods There are three defined pod priority levels available within an FFC cluster. A deployment must specify one of these levels. High (1000) Reserved primarily for customer facing or critical workload pods. Default (600) Default option suitable for most pods. Low (200) For pods where downtime is more tolerable. The priorityClass definitions that relate to these levels can be viewed in the ffc-kubernetes-configuration repository. Resource profile impact In the event a cluster has to make a choice between killing one of two services sharing the same priority level, the resource profile configuration will influence which is killed in the order below. 1. Best effort 2. Burstable 3. Guaranteed Further reading More information is available in Confluence","title":"Pod priority"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#pod-priority","text":"Kubernetes Pods can have priority levels. Priority indicates the importance of a pod relative to other pods. If a pod cannot be scheduled, the scheduler tries to pre-empt (evict) lower priority pods to make scheduling of the pending pod possible. In the event of over utilisation of a cluster, Kubernetes will start to kill lower priority pods first to maintain stability.","title":"Pod priority"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#priority-levels-available-to-ffc-pods","text":"There are three defined pod priority levels available within an FFC cluster. A deployment must specify one of these levels.","title":"Priority levels available to FFC pods"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#high-1000","text":"Reserved primarily for customer facing or critical workload pods.","title":"High (1000)"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#default-600","text":"Default option suitable for most pods.","title":"Default (600)"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#low-200","text":"For pods where downtime is more tolerable. The priorityClass definitions that relate to these levels can be viewed in the ffc-kubernetes-configuration repository.","title":"Low (200)"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#resource-profile-impact","text":"In the event a cluster has to make a choice between killing one of two services sharing the same priority level, the resource profile configuration will influence which is killed in the order below. 1. Best effort 2. Burstable 3. Guaranteed","title":"Resource profile impact"},{"location":"z_content-to-be-revised/standards/kubernetes/priority/#further-reading","text":"More information is available in Confluence","title":"Further reading"},{"location":"z_content-to-be-revised/standards/kubernetes/probes/","text":"Probes To increase the stability and predictability of a Kubernetes cluster, services should make use of both readiness and liveness probes to help Kubernetes manage the stability of the service. Probe end points should follow the convention of healthy for readiness probes and healthz for liveness probes. If no probe is defined, Kubernetes will still attempt to monitor the health of a service. However, it will only be able to do so by monitoring the status of the container. This is not a reliable way to determine the health of a service. Readiness probes Readiness probes help Kubernetes manage the availability of a service. They are used to determine if a service is ready to receive traffic. If a service is not ready, Kubernetes will not route traffic to it. Helm using the readiness probe to determine if a service is successfully deployed. If a service does not report ready in five minutes, the deployment will be considered a failure and will be automatically rolled back. Note: the readiness probe only prevents traffic routing through the internal cluster network, it does not for example stop a service from consuming messages via Azure Service Bus. Liveness probes Liveness probes help Kubernetes manage the stability of a service. They are used to determine if a service is still running. If a service is not running, Kubernetes will restart it. Probe configuration Probe configuration is subjective to a service and consideration should be given to the following: the initial delay before probes are started the frequency of the probe how many sequential failures are required before a service is considered unhealthy the timeout the probe should wait for a response what should the probe check for to determine the health of the service","title":"Probes"},{"location":"z_content-to-be-revised/standards/kubernetes/probes/#probes","text":"To increase the stability and predictability of a Kubernetes cluster, services should make use of both readiness and liveness probes to help Kubernetes manage the stability of the service. Probe end points should follow the convention of healthy for readiness probes and healthz for liveness probes. If no probe is defined, Kubernetes will still attempt to monitor the health of a service. However, it will only be able to do so by monitoring the status of the container. This is not a reliable way to determine the health of a service.","title":"Probes"},{"location":"z_content-to-be-revised/standards/kubernetes/probes/#readiness-probes","text":"Readiness probes help Kubernetes manage the availability of a service. They are used to determine if a service is ready to receive traffic. If a service is not ready, Kubernetes will not route traffic to it. Helm using the readiness probe to determine if a service is successfully deployed. If a service does not report ready in five minutes, the deployment will be considered a failure and will be automatically rolled back. Note: the readiness probe only prevents traffic routing through the internal cluster network, it does not for example stop a service from consuming messages via Azure Service Bus.","title":"Readiness probes"},{"location":"z_content-to-be-revised/standards/kubernetes/probes/#liveness-probes","text":"Liveness probes help Kubernetes manage the stability of a service. They are used to determine if a service is still running. If a service is not running, Kubernetes will restart it.","title":"Liveness probes"},{"location":"z_content-to-be-revised/standards/kubernetes/probes/#probe-configuration","text":"Probe configuration is subjective to a service and consideration should be given to the following: the initial delay before probes are started the frequency of the probe how many sequential failures are required before a service is considered unhealthy the timeout the probe should wait for a response what should the probe check for to determine the health of the service","title":"Probe configuration"},{"location":"z_content-to-be-revised/standards/kubernetes/rbac/","text":"Role Based Access Control (RBAC) FFC clusters enable RBAC through RoleBinding Kubernetes resources. Users must be added as Cluster Admins to access Kubernetes through tools such as Lens or kubectl . CCoE can provide this access to users on request.","title":"Role Based Access Control (RBAC)"},{"location":"z_content-to-be-revised/standards/kubernetes/rbac/#role-based-access-control-rbac","text":"FFC clusters enable RBAC through RoleBinding Kubernetes resources. Users must be added as Cluster Admins to access Kubernetes through tools such as Lens or kubectl . CCoE can provide this access to users on request.","title":"Role Based Access Control (RBAC)"},{"location":"z_content-to-be-revised/standards/kubernetes/resource-usage/","text":"Resource usage Predictable demands are important to help Kubernetes find the right place for a pod within a cluster. When it comes to resources such as a CPU and memory, understanding the resource needs of a pod will help Kubernetes allocate the pod to the most appropriate node and generally improve the stability of the cluster. Declaring a profile all pods declare both a request and limit value for CPU and memory production clusters do not contain any best-effort pods pods with consistent usage patterns are run as guaranteed pods (i.e. equal request and limit values ) pods with spiky usage patterns can be run as burstable but effort should be taken to understand why performance is not consistent and whether the service is doing too much Resource quotas FFC clusters will limit available resources within a namespace using a resourceQuota . Teams should frequently review the resource usage of their pods and adjust the resourceQuota accordingly. Resource quotas are deployed to AKS via an ADO pipeline. Resource profiling Performance testing a pod is the only way to understand it's resource utilisation pattern and needs. Performance testing should take place on all pods to accurately understand their usage before they can be deployed to production. Further reading More information is available in Confluence","title":"Resource usage"},{"location":"z_content-to-be-revised/standards/kubernetes/resource-usage/#resource-usage","text":"Predictable demands are important to help Kubernetes find the right place for a pod within a cluster. When it comes to resources such as a CPU and memory, understanding the resource needs of a pod will help Kubernetes allocate the pod to the most appropriate node and generally improve the stability of the cluster.","title":"Resource usage"},{"location":"z_content-to-be-revised/standards/kubernetes/resource-usage/#declaring-a-profile","text":"all pods declare both a request and limit value for CPU and memory production clusters do not contain any best-effort pods pods with consistent usage patterns are run as guaranteed pods (i.e. equal request and limit values ) pods with spiky usage patterns can be run as burstable but effort should be taken to understand why performance is not consistent and whether the service is doing too much","title":"Declaring a profile"},{"location":"z_content-to-be-revised/standards/kubernetes/resource-usage/#resource-quotas","text":"FFC clusters will limit available resources within a namespace using a resourceQuota . Teams should frequently review the resource usage of their pods and adjust the resourceQuota accordingly. Resource quotas are deployed to AKS via an ADO pipeline.","title":"Resource quotas"},{"location":"z_content-to-be-revised/standards/kubernetes/resource-usage/#resource-profiling","text":"Performance testing a pod is the only way to understand it's resource utilisation pattern and needs. Performance testing should take place on all pods to accurately understand their usage before they can be deployed to production.","title":"Resource profiling"},{"location":"z_content-to-be-revised/standards/kubernetes/resource-usage/#further-reading","text":"More information is available in Confluence","title":"Further reading"},{"location":"z_content-to-be-revised/standards/kubernetes/secrets/","text":"Secrets Where possible, secrets should not be stored within an application or Kubernetes pod. Instead, clusters should use AAD Pod Identity as per Microsoft recommendation. When secrets in a pod are unavoidable, for example when a third party API key is needed, they should be injected into the pod via a Secret Kubernetes resource by the CI pipeline. The secret itself should be stored in Azure Key Vault with a reference in Azure App Configuration .","title":"Secrets"},{"location":"z_content-to-be-revised/standards/kubernetes/secrets/#secrets","text":"Where possible, secrets should not be stored within an application or Kubernetes pod. Instead, clusters should use AAD Pod Identity as per Microsoft recommendation. When secrets in a pod are unavoidable, for example when a third party API key is needed, they should be injected into the pod via a Secret Kubernetes resource by the CI pipeline. The secret itself should be stored in Azure Key Vault with a reference in Azure App Configuration .","title":"Secrets"}]}