{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Farming and Countryside Programme (FCP) Development Guide","text":"<p>This is a repository of standards and supporting guidance for all developers working within the Farming and Countryside Programme (FCP).</p> <p>The purpose of the standards are to ensure that delivery supports the Architecture Vision set out by the programme and better enable developer agility and mobility through consistent patterns and practices.</p> <p>The guide also provides help and support for common setup and problem solving.</p>"},{"location":"#architecture-vision","title":"Architecture Vision","text":"<p>The FCP Architecture vision is to deliver a modern, cloud native, event driven microservice ecosystem.</p> <p>Containerised Node.js with JavaScript microservices are the primary delivery mechanism for the programme.  These are deployed to the Azure Kubernetes service (AKS) and are expected to be highly available, scalable and resilient.</p> <p>For scenarios where Node.js is not appropriate, Defra's secondary framework and language is .NET and C#.</p>"},{"location":"#defra-standards","title":"Defra standards","text":"<p>The standards and guidance contained here is intended to be an FCP context specific layer over Defra's wider development standards.</p> <p>This guide assumes that Developers already understand and follow these core standards.</p>"},{"location":"#gds-standards","title":"GDS standards","text":"<p>The Government Digital Service (GDS) also have a set of standards that all government services must adhere to.  These are also assumed to be understood and followed by developers working within FCP.</p> <p>Read the full GDS Service Manual for more information.</p>"},{"location":"archive/guides/","title":"Guides","text":"<p>These guides help provide additional context to support our standards.</p>"},{"location":"archive/guides/#contents","title":"Contents","text":"<ul> <li>Accounts</li> <li>Application Insights Availability Alerts</li> <li>Backlog management</li> <li>Collaboration</li> <li>Debug .NET Core in a Linux container</li> <li>Demo service</li> <li>Developing and debugging in a container</li> <li>Google Tag Manager</li> <li>Jenkins</li> <li>Joiners, movers and leavers</li> <li>Kubernetes</li> <li>AAD Pod Identity</li> <li>Configure NGINX Ingress Controller</li> <li>Creating a workstream namespace</li> <li>Install Kubernetes Dashboard</li> <li>Interact with cluster</li> <li>Probes</li> <li>Managing vulnerabilities</li> <li>Microservice resource provisioning</li> <li>CI pipeline</li> <li>Managed Identity</li> <li>Postgres database</li> <li>Release pipeline</li> <li>Service Bus queues, topics and subscriptions</li> <li>Pact Broker</li> <li>Redis caching in Hapi</li> <li>Secrets management</li> <li>Shared assets</li> <li>VS Code and WSL1</li> <li>WAF waivers</li> <li>Troubleshooting guide</li> </ul>"},{"location":"archive/guides/kubernetes/","title":"Kubernetes","text":"<p>Guides supporting the usage of Kubernetes</p>"},{"location":"archive/guides/kubernetes/#contents","title":"Contents","text":"<ul> <li>AAD Pod Identity</li> <li>Configure NGINX Ingress Controller</li> <li>Creating a workstream namespace</li> <li>Install Kubernetes Dashboard</li> <li>Interact with cluster</li> <li>Probes</li> </ul>"},{"location":"archive/guides/kubernetes/configure-nginx-ingress-controller/","title":"Configure NGINX Ingress Controller","text":"<p>An Ingress controller is an application that runs in a Kubernetes cluster and configures an HTTP load balancer according to Ingress resources.</p> <p>In the case of NGINX, the Ingress controller is deployed in a pod along with the load balancer.</p>"},{"location":"archive/guides/kubernetes/configure-nginx-ingress-controller/#installation","title":"Installation","text":"<p>The documentation for NGINX's chart includes details on how to install it.</p> <p>TL;DR:</p> <p><code>helm install stable/nginx-ingress --name nginx-ingress</code></p>"},{"location":"archive/guides/kubernetes/create-namespace/","title":"Creating a workstream namespace","text":"<p>Each workstream delivery team in FFC will have their own dedicated namespace in each cluster.</p> <p>This allows logical separation between services as well as the enabling simpler implementation of monitoring, RBAC and stability mechanisms.</p>"},{"location":"archive/guides/kubernetes/create-namespace/#requirements","title":"Requirements","text":"<p>The following are the required outcomes of a workstream namespace.# - namespace follows the naming convention <code>ffc-WORKSTREAM</code> eg. <code>ffc-elm</code> - <code>ResourceQuota</code> resource to limit resource usage - <code>RoleBinding</code> resource to restrict interaction to delivery team</p>"},{"location":"archive/guides/kubernetes/create-namespace/#process","title":"Process","text":"<p>Full instructions are included in the FFC Kubernetes Configuration repository.</p>"},{"location":"archive/guides/kubernetes/install-kubernetes-dashboard/","title":"Install Kubernetes dashboard","text":"<p>The Kubernetes dashboard is a web-based Kubernetes user interface.</p>"},{"location":"archive/guides/kubernetes/install-kubernetes-dashboard/#installation","title":"Installation","text":"<p>Install the dashboard to a Kubernetes cluster using the <code>kubectl apply</code> command specified in the Deploying the dashboard UI section in the below link.</p> <p>https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</p> <p>Example: <code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml</code></p>"},{"location":"archive/guides/kubernetes/install-kubernetes-dashboard/#create-default-user-and-access-token","title":"Create default user and access token","text":"<p>Follow the guide in the below link. https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md</p>"},{"location":"archive/guides/kubernetes/install-kubernetes-dashboard/#run-dashboard","title":"Run dashboard","text":"<ol> <li>Run terminal command <code>kubectl proxy</code></li> <li>Access dashboard at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</li> </ol>"},{"location":"archive/guides/kubernetes/interaction/","title":"Interact with cluster","text":"<p><code>kubectl</code> is used to interact with the cluster.  In order to use <code>kubectl</code> with an FFC cluster, a <code>kubeconfig</code> file for the cluster is required.</p> <p>A cluster can only be accessed when connected to VPN.</p>"},{"location":"archive/guides/kubernetes/interaction/#acquiring-a-kubeconfig-file-for-a-cluster","title":"Acquiring a Kubeconfig file for a cluster","text":"<p>To acquire a Kubeconfig, a subscription Id is needed along with the name of the cluster and the resource group in which it resides.  This information can be acquired via the Azure Portal or from CCoE.</p> <p><code>az account set --subscription SUBSCRIPTION_ID</code></p> <p><code>az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER --file WHERE_TO_SAVE_KUBECONFIG</code></p> <p>Note if the file parameter is not passed, the Kubeconfig will be merged with the users default configuration file stored at <code>~/.kube/config</code>.</p>"},{"location":"archive/guides/kubernetes/interaction/#productivity","title":"Productivity","text":"<p>Developers may find it more productive to use tools such as Lens k9s or kubectl-aliases to avoid needing to regularly type long terminal commands to interact with the cluster.</p>"},{"location":"archive/guides/kubernetes/pod-identity/","title":"AAD Pod Identity","text":"<p>AAD Pod Identity enables Kubernetes applications to access cloud resources securely with Azure Active Directory (AAD).</p> <p>Using Kubernetes primitives, administrators configure identities and bindings to match pods. Then your containerized applications can leverage any resource in the cloud that depends on AAD as an identity provider.</p>"},{"location":"archive/guides/kubernetes/pod-identity/#further-reading","title":"Further reading","text":"<p>More information is available in Confluence </p>"},{"location":"archive/guides/kubernetes/probes/","title":"Probes","text":"<p>Kubernetes has two types of probes, readiness and liveness.</p> <p>Kubernetes uses readiness probes to know when a container is ready to start accepting traffic.</p> <p>Kubernetes uses liveness probes to know when to restart a container.</p> <p>The FFC Helm chart library includes templates for both readiness and liveness probes.</p>"},{"location":"archive/guides/kubernetes/probes/#configuring-probes","title":"Configuring probes","text":"<p>Probes can be configured in the Helm chart on a <code>Deployment</code> resource, under the container node.</p> <p>The above is a simple example of an Http readiness and liveness probes.</p> <pre><code>readinessProbe:\n  path: /healthy\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n\nlivenessProbe:\n  path: /healthz\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n</code></pre> <p>In this example, the cluster will wait for 10 seconds after the pod is deployed.  It will then poll both the liveness and readiness endpoints on port 3000 every 10 seconds.  </p> <p>If it receives three successive status codes other than 200 for the readiness probe it will stop routing traffic to that pod.</p> <p>If it receives three successive status codes other than 200 for the liveness probe it will assume the pod is unresponsive and kill it.</p> <p>Note that a liveness probe works in conjunction with the restartPolicy value. In order to restart the restartPolicy must be set to Always or OnFailure.</p>"},{"location":"archive/guides/kubernetes/probes/#values","title":"Values","text":"<p><code>path</code>: the URL route the liveness probe should sent a response to.</p> <p><code>port</code>: the port on which the service is exposing</p> <p><code>initialDelaySeconds</code>: how long before the first probe should be sent. This should be safely longer than it takes the pod to start up, otherwise the pod could be stuck in a reboot loop</p> <p><code>periodSeconds</code>: how often the liveness probe should check the pod is responsive. Recommendation is between 10 and 20 seconds</p> <p><code>failureThreshold</code>: how many probe failures before the pod is automatically restarted</p> <p><code>timeoutSeconds</code>: how long to wait for a response before considering the probe failed</p> <p>As well as Http probes, there are also command and TCP based probes, full details can be found in the documentation https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</p>"},{"location":"archive/guides/resource-provisioning/","title":"Resource provisionsing","text":"<p>Guides supporting the addition of Azure resources to microservices running on the FFC Platform.</p>"},{"location":"archive/guides/resource-provisioning/#contents","title":"Contents","text":"<ul> <li>CI pipeline</li> <li>Managed Identity</li> <li>Postgres database</li> <li>Release pipeline</li> <li>Service Bus queues, topics and subscriptions</li> </ul>"},{"location":"archive/guides/resource-provisioning/postgres-database/","title":"Postgres Database","text":"<p>This guide describes how to configure access to an Azure PostgreSQL database from microservices running within Azure Kubernetes Service.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#create-managed-identity-for-your-microservice","title":"Create Managed Identity for your microservice","text":"<p>If not already configured add Managed Identity to your microservice</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#request-creation-of-microservice-database","title":"Request creation of microservice database","text":"<p>Request Cloud Services through your usual support channel to create a database within your Azure Database for PostgreSQL.  The name of the database should match the microservice repository name.  Microservices should not share a database.</p> <p>For example, a microservice named <code>ffc-demo-claim-service</code> would have a database named <code>ffc_demo_claim_service</code></p> <p>Note here the use of underscores instead of the normal hyphen convention. Postgres hyphens require escaping with double quote marks so underscores are preferred.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#request-creation-of-microservice-database-role","title":"Request creation of microservice database role","text":"<p>Request Cloud Services to create a database role that is bound to the Managed Identity created for the microservice (Azure guidence), for example <code>ffc-snd-demo-claim-role</code>.</p> <p>This identity must also be assigned to the Jenkins VM to ensure that Liquibase migrations can run.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#create-a-liquibase-changelog","title":"Create a Liquibase changelog","text":"<p>The FFC Platfrom CI and deployment pipelines support database migrations using Liquibase.</p> <p>Create a Liquibase changelog defining the structure of your database available from the root of your microservice repoository in <code>changelog/db.changelog.xml</code>.</p> <p>Guidence on creating a Liquibase changelog is outside of the scope of this guide, so please check current best practice with the FFC Platform Team.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#update-docker-compose-files-to-use-postgres-service-and-environment-variables","title":"Update Docker Compose files to use Postgres service and environment variables","text":"<p>Update <code>docker-compose.yaml</code>, <code>docker-compose.override.yaml</code>, and <code>docker-compose.test.yaml</code> to include a Postgres service and add Postgres environment variables to the microservice.</p> <p>An example Postgres service:</p> <pre><code>services:\n  # Microservice definition here\n  ffc-&lt;workstream&gt;-&lt;service&gt;-postgres:\n    image: postgres:11.4-alpine\n    environment:\n      POSTGRES_DB: ffc_&lt;workstream&gt;_&lt;service&gt;\n      POSTGRES_PASSWORD: ppp\n      POSTGRES_USER: postgres\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\nvolumes:\n  postgres_data: {}\n</code></pre> <p>Add dependency on the Postgres service and environment variables the microservice <code>services</code> definition:</p> <pre><code>services:\n  # Microservice definition here\n    depends_on:\n      - ffc-&lt;workstream&gt;-&lt;service&gt;-postgres\n    environment:\n      POSTGRES_DB: ffc_&lt;workstream&gt;_&lt;service&gt;\n      POSTGRES_PASSWORD: ppp\n      POSTGRES_USER: postgres\n      POSTGRES_HOST: ffc-&lt;workstream&gt;-&lt;service&gt;-postgres\n      POSTGRES_PORT: 5432\n      POSTGRES_SCHEMA_NAME: public\n</code></pre> <p>Replace <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#add-docker-compose-files-to-run-liquibase-migrations","title":"Add Docker Compose files to run Liquibase migrations","text":"<p>Add a <code>docker-compose.migrate.yaml</code> to the root of the microservice based on the template provided in resources.</p> <p>Replace <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#update-microservice-helm-chart","title":"Update microservice Helm chart","text":""},{"location":"archive/guides/resource-provisioning/postgres-database/#create-a-postgres-service","title":"Create a Postgres Service","text":"<p>Create a Kubernetes template for a Postgres Service in <code>helm/&lt;REPO_NAME&gt;/templates/postgres-service.yaml</code>:</p> <pre><code>{{- if .Values.postgresService.postgresExternalName }}\n{{- include \"ffc-helm-library.postgres-service\" (list . \"&lt;REPO_NAME&gt;.postgres-service\") -}}\n{{- end }}\n{{- define \"&lt;REPO_NAME&gt;.postgres-service\" -}}\n{{- end -}}\n</code></pre> <p>replacing <code>&lt;REPO_NAME&gt;</code> with the git repository name.</p> <p>Update the Helm chart values file (<code>helm/&lt;REPO_NAME&gt;/values.yaml</code>) with default values for the Postgres service:</p> <pre><code>postgresService:\n  postgresDb: ffc_&lt;workstream&gt;_&lt;service&gt;\n  postgresExternalName:\n  postgresHost: ffc-&lt;workstream&gt;-&lt;service&gt;-postgres\n  postgresPort: 5432\n  postgresSchema: public\n  postgresUser: postgres\n</code></pre> <p>replacing <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#update-configmap","title":"Update ConfigMap","text":"<p>Update the <code>ConfigMap</code> template of the Helm Chart (<code>helm/&lt;REPO_NAME&gt;/templates/config-map.yaml</code>) to include the environment variables for the Postgres database:</p> <pre><code>POSTGRES_DB: {{ quote .Values.postgresService.postgresDb }}\nPOSTGRES_HOST: {{ quote .Values.postgresService.postgresHost }}\nPOSTGRES_PORT: {{ quote .Values.postgresService.postgresPort }}\nPOSTGRES_SCHEMA_NAME: {{ quote .Values.postgresService.postgresSchema }}\n</code></pre>"},{"location":"archive/guides/resource-provisioning/postgres-database/#createupdate-the-container-secret","title":"Create/Update the container Secret","text":"<p>Create (or update) the Secret template in <code>helm/&lt;REPO_NAME&gt;/templates/container-secret.yaml</code>:</p> <pre><code>{{- include \"ffc-helm-library.container-secret\" (list . \"&lt;REPO_NAME&gt;.container-secret\") -}}\n{{- define \"&lt;REPO_NAME&gt;.container-secret\" -}}\nstringData:\n  POSTGRES_USER: {{ .Values.postgresService.postgresUser | quote }}\n{{- end -}}\n</code></pre> <p>replacing <code>&lt;REPO_NAME&gt;</code> with the git repository name.</p> <p>Update the Helm chart values file (<code>helm/&lt;REPO_NAME&gt;/values.yaml</code>) with a name for the Secret:</p> <pre><code>containerSecret:\n  name: ffc-&lt;workstream&gt;-&lt;service&gt;-container-secret\n  type: Opaque\n</code></pre> <p>replacing <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> as per naming convention described above.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#add-liquibase-migration-scripts","title":"Add Liquibase migration scripts","text":"<p>Copy the scripts from resources to create the following scripts at the root of your microservice: * <code>scripts/migration/database-down</code> * <code>scripts/migration/database-up</code> * <code>scripts/postgres-wait</code></p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#add-values-to-azure-key-vault-and-app-configuration","title":"Add values to Azure Key Vault and App Configuration","text":"<p>Azure Key Vault is used to store the Postgres username and Azure App Configuration is used to stores values required by the Jenkins CI pipelines.</p> <p>Create the following secret in Azure Key Vault via the Azure Portal:</p> <ul> <li>Name: <code>snd-postgres&lt;workstream&gt;&lt;service&gt;User</code>; Value: <code>&lt;managed-identity&gt;@&lt;azure-postgres-instance&gt;</code> (e.g. <code>ffc-snd-demo-web-role@mypostgresserver</code>)</li> </ul> <p>Create the following entries in Azure App Configuraiton via the Azure Portal:</p> <ol> <li> <p>A key-value entry where Key: <code>&lt;environment&gt;/postgresService.postgresDb</code> (e.g. <code>dev/postgresService.postgresDb</code>); Value: <code>ffc_&lt;workstream&gt;_&lt;service&gt;</code> (e.g. <code>ffc_demo_claim_service</code>); Label: <code>&lt;REPO_NAME&gt;</code> (e.g. <code>ffc-demo-claim-service</code>)</p> </li> <li> <p>A Key Vault reference entry where Key: <code>dev/postgresService.postgresUser</code>; Key Vault Secret Key: <code>dev-postgres&lt;workstream&gt;&lt;service&gt;User</code>; Label: <code>&lt;REPO_NAME&gt;</code> (e.g. <code>ffc-demo-claim-service</code>)</p> </li> </ol> <p>where <code>&lt;workstream&gt;</code> and <code>&lt;service&gt;</code> refer to those parts of the queue name described above.</p> <p>Note in environments beyond Sandpit, Azure DevOps will provision databases suffixed with the target environment.  The values should be ammended accordingly. e.g. <code>ffc_demo_claim_service_dev</code></p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#add-database-code-to-the-microservice","title":"Add database code to the microservice","text":"<p>Update your microservice code using the relevant Azure authentication SDKs for your language.</p> <p>Patterns for using a Postgres database in microservice code are outside of the scope of this guide. An example is shown below for a Node.js microservice, but please check current best practice with the FFC Platform Team.</p>"},{"location":"archive/guides/resource-provisioning/postgres-database/#nodejs-example","title":"Node.js example","text":"<p>Install the Azure Authentication SDK NPM package: <code>npm install @azure/ms-rest-nodeauth</code>.</p> <p>With the Managed Identity bound to your microservice in the Kubernetes cluster (following the guidence above), you can then access the database using the username <code>&lt;managed-identity&gt;@&lt;azure-postgres-instance&gt;</code> (e.g. <code>ffc-snd-demo-web-role@mypostgresserver</code>) and an access token as the password:</p> <pre><code>async function example() {\n  const auth = require('@azure/ms-rest-nodeauth')\n  const credentials = await auth.loginWithVmMSI({ resource: 'https://ossrdbms-aad.database.windows.net' })\n  const databasePassword = await credentials.getToken()\n\n  // Use databasePassword along with Postgres role bound to Managed Identity to authenticate to your database\n}\n</code></pre>"},{"location":"archive/guides/resource-provisioning/release-pipeline/","title":"Release pipelines","text":""},{"location":"archive/guides/resource-provisioning/release-pipeline/#sandpit","title":"Sandpit","text":"<p>Sandpit release pipelines are run from Jenkins and a deployment to the sandpit environment is automatically triggered by the CI pipeline if building from a main branch.</p>"},{"location":"archive/guides/resource-provisioning/release-pipeline/#configure-sandpit-release-pipeline","title":"Configure sandpit release pipeline","text":"<ul> <li>navigate to your workstream folder in Jenkins</li> <li>creating a new credential is optional and if not done, the value of <code>default-deploy-token</code> should be used</li> <li>select <code>Credentials -&gt; (global) -&gt; Add Credentials</code></li> <li>select <code>Secret text</code> type and set both <code>Description</code> and <code>Id</code> to <code>&lt;respository name&gt;-deploy-token</code>, for example <code>ffc-demo-web-deploy-token</code></li> <li>set the secret value to be a unique string, this value will be used to authenticate the CI pipeline when triggering a release</li> <li>navigate to your workspace folder and select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>&lt;repository name&gt;-deploy</code>, for example <code>ffc-demo-web-deploy</code></li> <li>select <code>Pipeline</code></li> <li>select <code>Ok</code></li> <li>set <code>This project is parameterised</code> to <code>true</code> and add the following <code>string</code> parameters</li> </ul> Name Default Value Description <code>chartVersion</code> <code>1.0.0</code> <code>Version of service to deploy</code> <code>chartName</code> <code>&lt;repository name&gt;</code> for example, <code>ffc-demo-web</code> <code>Service to deploy</code> <code>namespace</code> <code>ffc-&lt;service name&gt;</code> for example, <code>ffc-grants</code> <code>Cluster namespace to deploy to</code> <code>environment</code> <code>dev</code> <code>Cluster environment to deploy to</code> <code>helmChartRepoType</code> <code>acr</code> <code>Location of Helm charts</code> <ul> <li>set <code>Trigger builds remotely</code> to <code>true</code></li> <li>enter unique deploy token created above</li> <li>add inline pipeline script with the following content</li> </ul> <p>```   @Library('defra-library@v-9') _</p> <p>deployToCluster environment: params.environment,                 namespace: params.namespace,                 chartName: params.chartName,                 chartVersion: params.chartVersion,                 helmChartRepoType: params.helmChartRepoType   <code>`` - select</code>Save`</p>"},{"location":"archive/guides/resource-provisioning/release-pipeline/#development-test-preproduction-and-production-release-pipelines","title":"Development, Test, PreProduction and Production release pipelines","text":"<p>Release pipelines to higher environments are run from Azure DevOps using a common microservice pipeline.</p> <p>A deployment to the development and test environments environment are automatically triggered following a build from a main branch.  These pipelines can also be run manually by delivery teams.</p> <p>Deployments to PreProd require a ticket raised with CCoE.</p> <p>Deployments to Production require an RFC raised in myIT to authorise CCoE to deploy.</p>"},{"location":"archive/guides/resource-provisioning/release-pipeline/#preparations-for-higher-environments","title":"Preparations for higher environments","text":"<p>Teams should ensure they have completed the following before deploying to higher environments.</p> <ul> <li>Update Azure Application Configuration is updated with all required Helm chart configurations</li> <li>Share any new databases created with the Platform team.  The release pipelines are currently dependent on knowing which services have databases in advance.  This is expected to change in the near future.</li> <li>Ensure all managed identities are created</li> <li>Ensure all Service Bus queues are created</li> </ul>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/","title":"Azure Service Bus Queues, Topics and Subscriptions","text":"<p>This guide describes how to configure access to Azure Service Bus from microservices running within Azure Kubernetes Service (AKS).</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#create-a-microservice-managed-identity","title":"Create a microservice Managed Identity","text":"<p>If not already configured, add an Azure Managed Identity to your microservice.</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#request-creation-of-service-bus-queues-topics-and-subscriptions","title":"Request creation of Service Bus queues, topics and subscriptions","text":""},{"location":"archive/guides/resource-provisioning/servicebus-queues/#for-point-to-point-queues","title":"For point-to-point queues","text":"<p>For each queue required by your microservices, you will need: * A Service Bus queue for each environment * A Service Bus queue for each of the developers on the workstream in the Sandpit environment to use for local development</p> <p>Queues should follow the following naming convention:</p> <pre><code>ffc-&lt;workstream&gt;-&lt;identifier&gt;-&lt;purpose&gt;\n</code></pre> <p>where <code>&lt;purpose&gt;</code> either denotes the environment e.g. <code>ffc-demo-payment-snd</code>, or the initials of a developer for the local development queues e.g. <code>ffc-demo-payment-jw</code>.</p> <p>Request the creation of the required queues through your usual Cloud Services support channel.</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#for-topics-and-subscriptions","title":"For topics and subscriptions","text":"<p>For each topic, you will need: * A Service Bus topic for each environment * A Service Bus topic for each of the developers on the workstream in the Sandpit environment to use for local development</p> <p>Subscriptions are created on a topic. For every subscription, you will also need: * A Service Bus subscription to the topic for each environment * A Service Bus subscription to each developer topic in the Sandpit environment to use for local development</p> <p>Topics and subscriptions should follow the following naming convention:</p> <pre><code>ffc-&lt;workstream&gt;-&lt;identifier&gt;-&lt;purpose&gt;\n</code></pre> <p>where <code>&lt;purpose&gt;</code> either denotes the environment e.g. <code>ffc-demo-reporting-snd</code>, or the initials of a developer for the local development queues e.g. <code>ffc-demo-reporting-jw</code>.</p> <p>Request the creation of the required topics and subscriptions through your usual Cloud Services support channel.</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#update-managed-identity-permissions","title":"Update Managed Identity permissions","text":"<p>For each environment queue, topic and subscription created, read and/or write permissions should be added to the Managed Identites of the microservices that will be using it.</p> <p>Request Cloud Services to add the relevant permissions to the Managed Identities.</p> <p>Permissions do not need to be added to Managed Identities for the local development queues.</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#pr-queue-provisioning","title":"PR queue provisioning","text":"<p>Queues, topics and subscriptions can be automatically provisioned for each microservice PR by the Jenkins CI pipleine to ensure encapsulation of infrastructure.</p> <p>Create a <code>provision.azure.yaml</code> file in the root directory of your microservice, and populate with the Service Bus infrastructure that need provisioning:</p> <pre><code>resources:\n  queues:\n    - name: &lt;queue_identifier&gt;\n  topics:\n    - name: &lt;topic_identifier&gt;\n</code></pre> <p>where the <code>&lt;queue_identifier&gt;</code> and/or <code>&lt;topic_identifier&gt;</code> relates to the part of the queue or topic name described above. For example for the queue <code>ffc-demo-payment-dev</code>, <code>&lt;queue_identifier&gt;</code> would be replaced with <code>payment</code>.</p> <p>For every topic specified in <code>provision.azure.yaml</code>, the CI pipeline will also create a subscription, so subscriptions do not need to be explictly spcecified.</p> <p>Add a <code>name</code> entry in the <code>provision.azure.yaml</code> for each required queue and topic/subscription.</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#update-local-development-environment","title":"Update local development environment","text":"<p>Configure your development environment so that the following environment variables are available for local development:</p> <pre><code>MESSAGE_QUEUE_HOST=&lt;INSERT_VALUE_FROM_AZURE_PORTAL&gt;\nMESSAGE_QUEUE_PASSWORD=&lt;INSERT_VALUE_FROM_AZURE_PORTAL&gt;\nMESSAGE_QUEUE_USER=RootManageSharedAccessKey\n</code></pre> <p>Values for <code>MESSAGE_QUEUE_HOST</code> and <code>MESSAGE_QUEUE_PASSWORD</code> will be found in the Azure Portal.</p> <p>Also create a variable for each of your developer queues, topics and subscriptions:</p> <pre><code>&lt;IDENTIFIER&gt;_&lt;QUEUE|TOPIC|SUBSCRIPTION&gt;_ADDRESS=ffc-&lt;workstream&gt;-&lt;identifier&gt;-&lt;purpose&gt;\n</code></pre> <p>where <code>&lt;IDENTIFIER&gt;</code> is the same as the <code>&lt;identifier&gt;</code> part of the queue/topic/subscription name.</p> <p>Options for storing these environment variables include: * A <code>.env</code> file in the root of the microservice that uses Service bus, making sure you Do not commit the <code>.env</code> file to the git repository (add it to the <code>.gitignore</code>) * Your shell <code>rc</code> file e.g. <code>.bashrc</code> or <code>.zshrc</code></p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#update-docker-compose-to-use-service-bus-environment-variables","title":"Update Docker Compose to use Service Bus environment variables","text":"<p>Add the following environment variables to the microserive <code>environment</code> section of the <code>docker-compose.yaml</code>:</p> <pre><code>MESSAGE_QUEUE_HOST: ${MESSAGE_QUEUE_HOST:-notset}\nMESSAGE_QUEUE_PASSWORD: ${MESSAGE_QUEUE_PASSWORD:-notset}\nMESSAGE_QUEUE_USER: ${MESSAGE_QUEUE_USER:-notset}\n</code></pre> <p>And for every queue, topic and subscription you have created also add:</p> <pre><code>&lt;IDENTIFIER&gt;_&lt;QUEUE|TOPIC|SUBSCRIPTION&gt;_ADDRESS: ${&lt;IDENTIFIER&gt;_&lt;QUEUE|TOPIC|SUBSCRIPTION&gt;_ADDRESS:-notset}\n</code></pre>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#update-microservice-helm-chart","title":"Update microservice Helm chart","text":"<p>Update the <code>ConfigMap</code> template of the Helm Chart (<code>helm/&lt;REPO_NAME&gt;/templates/config-map.yaml</code>) to include the environment variables for the message queue host and every queue, topic and subscription you have created, for example:</p> <pre><code>MESSAGE_QUEUE_HOST: {{ quote .Values.container.messageQueueHost }}\n&lt;IDENTIFIER&gt;_&lt;QUEUE|TOPIC|SUBSCRIPTION&gt;_ADDRESS: {{ quote .Values.container.&lt;identifier&gt;&lt;Queue|Topic|Subscription&gt;Address }}\n</code></pre> <p>Then create default values for these in the <code>container</code> section of the Helm Chart values file (<code>helm/&lt;REPO_NAME&gt;/values.yaml</code>):</p> <pre><code>container:\n  messageQueueHost: dummy\n  &lt;identifier&gt;&lt;Queue|Topic|Subscription&gt;Address: &lt;identifier&gt;\n</code></pre>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#add-queue-topic-and-subscription-names-to-azure-app-configuration","title":"Add queue, topic and subscription names to Azure App Configuration","text":"<p>Azure App Configuration stores values required by the Jenkins CI pipelines. For each queue, topic and subscription create a key-value entry for each environment via the Azure Portal:</p> <ul> <li>Key: <code>dev/container.&lt;identifier&gt;&lt;Queue|Topic|Subscription&gt;Address</code>; Value: <code>ffc-&lt;workstream&gt;-&lt;identifier&gt;-&lt;environment&gt;</code></li> </ul> <p>where <code>&lt;workstream&gt;</code> and <code>&lt;identifier&gt;</code> refer to those parts of the queue name described above, and <code>&lt;environment&gt;</code> denotes the environment e.g. <code>ffc-demo-payment-dev</code></p> <p>Do not add a label to these App Configuration entries.</p>"},{"location":"archive/guides/resource-provisioning/servicebus-queues/#add-messaging-code-to-the-microservice","title":"Add messaging code to the microservice","text":"<p>Update your microservice code using the relevant Azure authentication and Service Bus SDKs for your language.</p> <p>The easiest way to do this is by using the <code>ffc-messaging</code> module.</p>"},{"location":"archive/standards/","title":"Standards","text":"<p>These standards must be adhered to by all FCP developers.</p>"},{"location":"archive/standards/#contents","title":"Contents","text":"<ul> <li>APIs</li> <li>Backlog management</li> <li>Code reviews</li> <li>Configuration management</li> <li>Documentation</li> <li>Events and messages</li> <li>FFC CI pipeline</li> <li>Jenkins</li> <li>Kubernetes</li> <li>Configuration</li> <li>Helm charts</li> <li>Labels</li> <li>Pod priority</li> <li>Probes</li> <li>Role Based Access Control (RBAC)</li> <li>Resource management</li> <li>Secrets</li> <li>Quick reference</li> <li>Release notes</li> <li>Route to live</li> <li>Secrets management</li> <li>Source code</li> <li>Technology stack</li> <li>Testing</li> <li>Version control</li> </ul>"},{"location":"archive/standards/api/","title":"API Standards","text":"<p>Teams must document the API of all microservices they develop. Both REST APIs such as HTTP and Asynchronous APIs such as messages and events. Links to the documents mentioned below should also be included in the Microservice Reference Library</p>"},{"location":"archive/standards/api/#rest","title":"REST","text":"<p>Teams need to document their API using OpenAPI 3.0 specification which can be found here.</p> <p>This approach is supported by GDS technical standards which states \"The open standards board recommends that government organisations use OpenAPI version 3 to describe RESTful APIs\" for more information you can read</p> <p>The OpenAPI file should be kept in a <code>docs</code> folder in the top level of the folder and named <code>openapi.yaml</code> unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named <code>openapi-&lt;version&gt;.yaml</code>. This should be documented in the <code>API</code> section of the <code>Microservice Reference Library</code>.</p> <p>An example of how to do this can be found in ffc-demo-payment-service.</p>"},{"location":"archive/standards/api/#asynchronous","title":"Asynchronous","text":"<p>Teams need to document their API using AsyncAPI specification which can be found here</p> <p>The AsyncAPI file should be kept in a <code>docs</code> folder in the top level of the folder and named <code>asyncapi.yaml</code> unless there is a scenario which requires multiple versions being used at the same time in which case the file should be named <code>asyncapi-&lt;version&gt;.yaml</code>. This should be documented in the <code>Events</code> section of the <code>Microservice Reference Library</code>.</p> <p>An example of how to do this can be found in ffc-demo-payment-service.</p>"},{"location":"archive/standards/code-review/","title":"Code review","text":"<p>When code reviews are conducted to a high standard, they provide a valuable learning opportunity for the author, reviewer and any observers of the review process. FFC projects implement strict controls such that all changes must be made in feature branches and merged via pull request (PR). In order to merge a PR, it must first be approved by at least one reviewer.</p> <p>When reviewing a pull request on any FFC project, the following guidance should be followed.</p>"},{"location":"archive/standards/code-review/#tone-of-code-review-comments","title":"Tone of code review comments","text":"<p>The tone of communications is extremely important in fostering an inclusive, collaborative atmosphere within teams.</p> <p>Remember that your colleagues put a lot of effort into their work and may feel offended by harsh criticism, particularly if you make assumptions or imply a lack of thought. Approach code reviews with kindness and empathy, assuming the best intentions and applauding good solutions as well as making suggestions for improvement.</p> <p>Comments should be used to give praise for good solutions and to point out potential improvements. They should be not be used to criticise your colleagues or make strongly opinionated statements. Always be mindful of your tone, considering how others might perceive your comments, and be explicit about when a comment is non-blocking or unimportant.</p>"},{"location":"archive/standards/code-review/#be-constructive","title":"Be constructive","text":"<p>Don't use harsh language or criticise without making constructive suggestions. Do suggest alternative approaches and explain your reasoning.</p>"},{"location":"archive/standards/code-review/#be-specific","title":"Be specific","text":"<p>Don't make vague statements about the changes as a whole. Do point out specific issues and offer specific ideas for how the changes can be improved.</p>"},{"location":"archive/standards/code-review/#avoid-strong-opinions","title":"Avoid strong opinions","text":"<p>Don't make strong, opinionated statements or dictate specific changes. Do ask open-ended questions and keep an open mind about alternative solutions and reasoning that you may not have thought of.</p>"},{"location":"archive/standards/code-review/#scope-of-a-code-review","title":"Scope of a code review","text":"<p>Code reviews should focus on what is being changed and whether the change is appropriate.  The scope will be stated in the acceptance criteria of the ticket.</p> <p>Expanding the scope of a pull request at the review stage is not acceptable. It is generally more valuable to swiftly conclude a piece of work that the team has prioritised than to opportunistically seek additional changes, such as refactoring related code. That said, it can be useful to comment on refactoring opportunities without blocking the pull request. It may be that the author has some spare time they can use to make those changes, or can incorporate them with their next piece of work.</p> <p>Examples of things to look for and comment on in a code review:</p>"},{"location":"archive/standards/code-review/#what-has-changed","title":"What has changed","text":"<p>Are the changes focussed on a specific issue, referenced in the pull request description? If the changes go beyond the intended scope, should they be broken up to make the code review more manageable? If the code review is still a manageable size, consider making a non-blocking comment to remind the author that they could control the scope of future pull requests more carefully.</p>"},{"location":"archive/standards/code-review/#maintainability","title":"Maintainability","text":"<p>Is all new code extensible and easy for other developers to understand? Does it follow common design patterns? Look for unnecessary complexity and remember that this is subjective so take care not to be overly critical or opinionated when commenting on maintainability. Try to make specific suggestions for improvement, rather than simply stating a problem, but use open-ended language to encourage discussion.</p>"},{"location":"archive/standards/code-review/#duplication","title":"Duplication","text":"<p>Is there duplication within new code or between new and existing code? Could an existing abstraction be reused or should a new abstraction be created? When proposing an abstraction, consider using a non-blocking comment to open a dialogue about what the abstraction should be and whether it needs addressing now or as a separate piece of work.</p> <p>Identifying useful abstractions at the review stage may indicate that there wasn't enough collaboration before coding began. See this as a trigger to review the team's ways of working, rather than blocking a pull request unnecessarily.</p>"},{"location":"archive/standards/code-review/#reusability","title":"Reusability","text":"<p>Have any new abstractions been introduced? Are they sufficiently reusable? If other parts of the system could be updated to use the new abstractions, consider suggesting this in a non-blocking comment so it can be discussed and either addressed in the same pull request or added to the product backlog as technical debt.</p>"},{"location":"archive/standards/code-review/#impact-on-other-parts-of-the-system","title":"Impact on other parts of the system","text":"<p>Will the changes have knock-on effects or otherwise necessitate changes to other parts of the system?</p>"},{"location":"archive/standards/code-review/#unit-test-coverage","title":"Unit test coverage","text":"<p>Is all new code covered by detailed unit tests? Have any edge cases been missed? Don't rely on metrics such as code coverage. Inspect the code thoroughly and ensure that the tests contain appropriate assertions to confirm that all the intended functionality works as expected.</p>"},{"location":"archive/standards/code-review/#integration-tests","title":"Integration tests","text":"<p>Have integration tests been added to cover all changes to functionality?</p>"},{"location":"archive/standards/code-review/#suggesting-improvements","title":"Suggesting improvements","text":"<p>When concluding a review, there are three options:</p> <ol> <li>Comment</li> <li>Approve the PR</li> <li>Block the PR by requesting changes</li> </ol>"},{"location":"archive/standards/code-review/#when-to-comment","title":"When to comment","text":"<p>You should conclude with a comment when your review asks questions which need answering before you can determine whether the pull request is acceptable. If you haven't proposed a solution to a specific problem in the pull request, it's generally better to leave a neutral review than to block the PR with a request for change.</p> <p>If you have raised concerns or questions through inline comments, it may be helpful to give a concise summary in your concluding comment.</p>"},{"location":"archive/standards/code-review/#when-to-approve","title":"When to approve","text":"<p>You should approve a pull request when you have confirmed that:</p> <ol> <li>it meets the objectives set out in its description</li> <li>it doesn't introduce new defects or code that is hard to maintain</li> <li>all new and modified code has thorough unit test coverage</li> <li>integration tests have been added where appropriate</li> <li>there are no unresolved questions or comments against the pull request</li> </ol> <p>Occasionally, it may be prudent to accept a pull request which does not meet all of the above requirements, such as to resolve an urgent issue with the live product. Such cases must always be agreed between the product owner, author(s) and reviewer(s).</p> <p>If comments or questions on a pull request have been addressed elsewhere (e.g. face-to-face or on Slack), ensure that the outcome is recorded in comment replies so that it is visible to anyone looking back at the pull request in future for information.</p> <p>Use the Resolve button to make it clear which of your concerns have been addressed and which still need attention. A pull request should only be approved after all reviewers have explicitly indicated that each of their comments have been resolved. If several reviewers have replied to a comment thread, it may appropriate for any of them to resolve that conversation on behalf of the group.</p> <p>Sometimes, a reviewer may become unavailable after commenting on a pull request. When that happens, a second reviewer may accept the pull request without resolving the first reviewer's comments, as long as the author and second reviewer agree that they believe all legitimate concerns have been addressed. Before accepting the pull request, the second reviewer should reply to each unresolved comment to indicate that they believe it has been addressed.</p>"},{"location":"archive/standards/code-review/#when-to-request-changes","title":"When to request changes","text":"<p>You should request changes to a pull request if any of the following are true:</p> <ol> <li>it creates a defect in the product</li> <li>it exacerbates an existing defect</li> <li>it doesn't meet the objectives set out in its description</li> <li>it would make the product more difficult to maintain</li> <li>new or modified code lacks thorough unit tests</li> <li>required integration tests have not been included</li> </ol>"},{"location":"archive/standards/code-review/#when-not-to-request-changes","title":"When not to request changes","text":"<p>Perfection is the enemy of good</p> <p>A pull request does not need to be perfect to be good enough. Often, there are many solutions to a problem and one which is not the best is still good enough to meet current needs. Perhaps an alternative approach would be more efficient or an abstraction could reduce duplication, but those things may be less important than the next item in the product backlog.</p> <p>Make suggestions for improvement as comments without explicitly approving or rejecting the pull request. This way, your suggestions can open dialogue with the author about how important your suggestions are compared to other work you could each move on to.</p>"},{"location":"archive/standards/documentation/","title":"Documentation","text":"<p>FFC products are delivered following an agile methodology based on evidenced user needs. Documentation is expected to follow this mantra and be iterated with the code as the product evolves.</p> <p>Documentation should be \"just enough\" to cover what needs to be communicated and presented as simply and effectively as possible.</p>"},{"location":"archive/standards/documentation/#documentation-location","title":"Documentation Location","text":"<p>Documentation will be stored in the same source code repository as the product it supports.</p> <p>FFC developments will mostly be within an open source microservices ecosystem. Each microservice will have its own repository and its own documentation.</p>"},{"location":"archive/standards/documentation/#readme","title":"Readme","text":"<p>A readme file must exist in every repo in markdown format (.md). It must include the following if they apply: - Description of product - Prerequisites - Development tools setup - Test tools setup - How to run in development - How to run tests - How to make changes to the code</p>"},{"location":"archive/standards/documentation/#additional-detail","title":"Additional Detail","text":"<p>Depending on the complexity of the product, the following may be included in the readme, or it may be more effective to capture in separate documentation/diagrams referenced by the readme.</p> <ul> <li>How product fits into wider architecture</li> <li>Internal product architecture</li> <li>Data structure</li> <li>API end points</li> <li>Monitoring</li> <li>Error handling</li> <li>Audit</li> <li>User access</li> <li>Security</li> <li>Complexity worth documenting</li> <li>Pipelines</li> </ul>"},{"location":"archive/standards/documentation/#microservice-reference-library","title":"Microservice Reference Library","text":"<p>This is a reference library of all microservices created and maintained by FFC.  The purpose of which is so we have an easy access resource to understand what capability we have developed, but also to promote reuse where appropriate.</p> <p>The registry is updated initially by the Solution Architect when designing the service.  It is then updated and maintained by the developer as the service iterates through it's lifecycle.</p> <p>The registry is in Confluence</p>"},{"location":"archive/standards/documentation/#library-content","title":"Library Content","text":"<ul> <li>Name of service</li> <li>Owner of service</li> <li>Link to Git repository</li> <li>Link to Jira backlog</li> <li>Summary of service's purpose and capability</li> <li>Architectural diagrams following C4 model, see c4model.com for more information</li> <li>Technology stack</li> <li>Dependencies</li> <li>API specification, can be link to OpenAPI specification file</li> <li>Events, can be link, can be link to AsyncAPI specification file</li> <li>Functions</li> <li>Version history, can be link to GitHub</li> <li>Usage, ie which services use this microservice</li> <li>Useful references</li> </ul>"},{"location":"archive/standards/documentation/#architecture-documentation","title":"Architecture Documentation","text":"<p>There may be documentation owned by FFC architecture that developers need to contribute to.</p> <p>For example, a Defra wide API endpoint catalogue or a Cloud Centre of Excellence namespace library. These documents should be completed in line with the standards of the owner.</p>"},{"location":"archive/standards/documentation/#architectural-decisions","title":"Architectural Decisions","text":"<p>Architectural decisions are discussed and agreed at FFC Technical Design Authority (TDA) chaired by the Chief Architect. Agreed outputs are captured as part of the TDA by Architecture.</p>"},{"location":"archive/standards/documentation/#technical-debt","title":"Technical Debt","text":"<p>A current view of technical debt is maintained by TDA. Developers are expected to contribute to forming this vision.</p>"},{"location":"archive/standards/documentation/#code","title":"Code","text":"<p>Effective software engineering can negate the need for additional documentation. Code should follow Defra's digital standards and apply appropriate recognised design patterns.</p> <p>We should aim to write readable code that requires minimal comments as per Defra's digital standards. However in some scenarios effective code comments can be used to explain complexity.</p> <p>Well named and considered unit tests can also act as effective documentation.</p>"},{"location":"archive/standards/documentation/#issues","title":"Issues","text":"<p>Work items are captured in the form Jira issues.</p> <p>There is no need to duplicate issues in additional documentation.</p> <p>Issues should be referenced in associated Pull Requests.</p>"},{"location":"archive/standards/documentation/#sensitive-documentation","title":"Sensitive Documentation","text":"<p>Sensitive documentation should be stored within a private repository within Azure DevOps or Confluence depending on the audience.</p>"},{"location":"archive/standards/events/","title":"Events and messages","text":"<p>Where possible, service to service communication will be asynchronous using Azure Service Bus.  </p> <p>Depending on the pattern being applied the term <code>event</code> or <code>message</code> may be more appropriate.  For simplicity, for the remainder of this page, the term <code>event</code> will be used to refer to both these definitions.</p>"},{"location":"archive/standards/events/#use-azure-service-bus","title":"Use Azure Service Bus","text":"<p>Services should use Azure Service Bus as an event broker.  This is because the FFC CI pipeline is relatively mature in terms of dynamically provisioning Azure Service Bus infrastructure to support the development cycle and the team have strong experience with it's use.</p>"},{"location":"archive/standards/events/#avoid-tight-coupling","title":"Avoid tight coupling","text":"<p>Services should be as loosely coupled as possible.  This reduces the dependency a change in one service could have on related services.</p> <p>Therefore, events should be published to <code>Topics</code> as opposed to <code>Queues</code> where possible.</p>"},{"location":"archive/standards/events/#format","title":"Format","text":"<p>When sending an event to Azure Service Bus, it will be decorated by default broker properties.  For full details on the specification, see the Microsoft documentation</p>"},{"location":"archive/standards/events/#mandatory-custom-properties","title":"Mandatory custom properties","text":"<p>All FFC events from any service must also declare the following custom properties when publishing an event.</p> <ul> <li><code>type</code> - the type of event, this should be prefixed with reverse DNS and describe the object and event occuring on that object in the format: <code>&lt;reverse DNS&gt;.ffc.&lt;object&gt;.&lt;event&gt;</code>.    For example, <code>uk.gov.ffc.demo.claim.validated</code></li> <li><code>source</code> - the service the event originated from, eg <code>ffc-demo-web</code></li> <li><code>subject (optional)</code> - the subject which will give consumers context, for example when the body cannot be consistently read or contains blob data</li> </ul> <p>These additional properties are based on values included in the CloudEvents specification that are not included in the Azure Service Bus envelope.  Refer to the Cloud Events specification for further examples of their usage.</p>"},{"location":"archive/standards/jenkins/","title":"Jenkins","text":"<p>Jenkins is used for CI pipelines across all FFC teams.</p> <p>Where possible, build steps are containerised to reduce plugin and Jenkins configuration dependencies.</p>"},{"location":"archive/standards/jenkins/#shared-library","title":"Shared library","text":"<p>All microservices use the FFC CI pipeline for managing builds.  This pipeline abstracts complexity such as infrastructure provisioning, building and testing away from consuming microservices, significantly reducing the effort needed to build and deploy services in line with other FFC standards.</p>"},{"location":"archive/standards/jenkins/#jenkinsfiles","title":"Jenkinsfiles","text":"<p>Jenkinsfiles use the Groovy syntax, adhering to the official Apache Groovy style guide</p> <p>Microservice Jenkinsfiles are source controlled in the same repository as the microservice itself.</p> <p>Secret values and sensitive configuration are not included in Jenkinsfiles, instead use either a parameter or environment variable if needed.</p>"},{"location":"archive/standards/jenkins/#credentials","title":"Credentials","text":"<p>Jenkins credentials should only be used for Jenkins pipeline specific activities.  They should not hold credentials relating to microservice configuration or deployment.  These credentials should be sourced from Azure Key Vault or for files, a private Azure Repository.</p>"},{"location":"archive/standards/jenkins/#workspace-folders","title":"Workspace folders","text":"<p>All delivery teams will have their own workspace folders to logically separate their pipelines and credentials.  The naming convention of these folders will be <code>ffc-&lt;service&gt;</code>.  For example, <code>ffc-grants</code>.</p> <p>Each microservice should have it's own <code>Build</code> and <code>Deploy</code> pipeline.</p>"},{"location":"archive/standards/jenkins/#builds","title":"Builds","text":"<p>Naming convention: <code>ffc-&lt;service&gt;-build</code></p> <p>Triggered from a GitHub webhook, this pipeline will build and test the microservice.</p>"},{"location":"archive/standards/jenkins/#deployments","title":"Deployments","text":"<p>Naming convention: <code>ffc-&lt;service&gt;-deploy</code></p> <p>Triggered from a build targetting the <code>main</code> branch, this pipeline will deploy the microservice to the Sandpit environment.</p> <p>Note: the naming convention must be followed to ensure the pipeline is triggered by the build.</p>"},{"location":"archive/standards/quick-reference/","title":"Quick reference","text":"<p>This page represents a summary of the core standards teams should follow when delivering services for FFC.</p> <p>These standards are intended to support a consistent interpretation of the Architecture Vision across the programme allowing as much flexibility as possible.</p> <ul> <li>follow the Architecture Vision</li> <li>design containerised microservices around business domains and bounded contexts</li> <li>use asynchronous communication between services via Azure Service Bus</li> <li>orchestrate containers using Kubernetes</li> <li>follow Defra's wider software development standards</li> <li>microservices should be built primarily using Node.js unless there is an evidenced reason not to</li> <li>if a web framework is needed, use Hapi.js</li> <li>if Node.js is not appropriate, then use .NET</li> <li>use PostgreSQL for relational databases</li> <li>avoid storing secrets in the cluster where possible, instead use AAD Pod Identity</li> <li>follow the agreed test structure in each microservice repository</li> <li>use contract testing with Pact to support e2e testing</li> <li>use the FFC shared library for CI</li> <li>use the FFC Helm chart library as a base for Helm charts</li> <li>use the Defra Node.js and .NET docker base images</li> <li>follow feature branch development with PR deployments</li> <li>work in an agile manner</li> <li>keep deployments small and frequent</li> <li>do not store live data in any environment other than production</li> </ul>"},{"location":"archive/standards/release-notes/","title":"Release notes","text":"<p>We want release notes to be consistent. These standards are for when we manually create a release note, for example within a GitHub repository.</p> <p>For the Description section use the Semantic versioning standards as a guide.</p>"},{"location":"archive/standards/release-notes/#release-note-standards","title":"Release Note Standards","text":""},{"location":"archive/standards/release-notes/#title","title":"Title","text":"<p>semver Version (e.g. <code>1.0.0</code>)</p>"},{"location":"archive/standards/release-notes/#description","title":"Description","text":""},{"location":"archive/standards/release-notes/#major","title":"Major","text":"<ul> <li>bullet list of major changes</li> </ul>"},{"location":"archive/standards/release-notes/#minor","title":"Minor","text":"<ul> <li>bullet list of minor changes</li> </ul>"},{"location":"archive/standards/release-notes/#patch","title":"Patch","text":"<ul> <li>bullet list of patch changes</li> </ul> <p>Each of the above will be optional and dependant on the changes that have been made.</p> <p>For complex changes include code snippets in the relevant description heading.</p>"},{"location":"archive/standards/route-to-live/","title":"Route to live","text":"<p>Teams across the FFC programme follow the below core workflow.</p> <p>Individual delivery teams are free to add their own processes on top of these core values to best fit their context.</p>"},{"location":"archive/standards/route-to-live/#process-for-change","title":"Process for change","text":"<p>This applies equally to a new feature or bug fix.</p> <ol> <li>developer creates a feature branch from main branch</li> <li>developer creates an empty commit with the title of the change and a link to Jira ticket</li> <li>developer opens draft PR for change</li> <li>developer changes semantic version of application</li> <li>developer write code in line with team standards, following TDD where appropriate</li> <li>as developer commits, CI builds and deploys to isolated PR environment</li> <li>CI runs tests to assure code quality</li> <li>when ready, code is reviewed using PR deployment and CI outputs to support</li> <li>if review passes code is merged to main branch</li> <li>CI automatically tears down PR deployment</li> <li>CI packages, versions and prepares for deployment to higher environments</li> <li>pipeline automatically deploys to Sandpit and Development environments sequentially</li> <li>developer approves deployment to Test environment</li> <li>developer raises ticket with CCoE to deploy to PreProduction environment</li> <li>developer prepares run book for PreProduction deployment</li> <li>CCoE approves deployment to PreProduction environment</li> <li>developer raises change request within myIT to deploy to Production environment</li> <li>developer prepares run book for Production deployment</li> <li>CCoE approves deployment to Production environment</li> </ol> <p></p>"},{"location":"archive/standards/route-to-live/#environments","title":"Environments","text":""},{"location":"archive/standards/route-to-live/#shift-left","title":"Shift left","text":"<p>Teams should follow a shift left approach to testing where as much testing is performed as early as possible in the development process.  The FFC CI pipeline with it's dynamic infrastructure provisioning and test execution is a key enabler of this approach.</p> <p>Test environments are primarily used for end to end, user acceptance and performance testing that are typically not possible earlier.</p>"},{"location":"archive/standards/route-to-live/#data","title":"Data","text":"<p>Production is the only environment where \"real\" data is permitted.  All other environments should use synthetic data.</p>"},{"location":"archive/standards/secrets-management/","title":"Secrets Management","text":""},{"location":"archive/standards/secrets-management/#client-side-git-secret-detection","title":"Client-side git secret detection","text":"<p>All developers are required to install detect-secrets when working on open-source FFC git repositories in GitHub.</p>"},{"location":"archive/standards/secrets-management/#server-side-git-secret-detection","title":"Server-side git secret detection","text":"<p>Jenkins will scan for potential secrets in all open-source FFC git repositories in GitHub.</p> <p>Any secrets detected will be reported to the <code>#secret-detection</code> Slack channel in the <code>ffc-notifications</code> workspace.</p>"},{"location":"archive/standards/source-code/","title":"Source code","text":""},{"location":"archive/standards/source-code/#branch-naming-convention","title":"Branch naming convention","text":"<p>Branches should be named in the format <code>&lt;ticket id&gt;-&lt;description of change&gt;</code> in lower case.</p> <p>For example, <code>1234567-add-dockerfile</code>.</p>"},{"location":"archive/standards/source-code/#pull-requests","title":"Pull Requests","text":"<p>When following feature branch development, pull requests should be used to collaborate and ensure code quality.</p> <p>Pull requests should be created in line with Defra's standards and reviewed in the spirit of FFC's code review standards.</p> <p>Pull requests should trigger a build and a PR deployment to support the review.</p>"},{"location":"archive/standards/source-code/#completing-pull-request","title":"Completing Pull Request","text":"<p>In order to complete a pull request, all of the following conditions must be met: - at least one approver - PR build must complete successfully - dismiss stale reviews</p> <p>On completion of a pull request a squash merge is performed and the feature/bug branch deleted. The commit message should be amended to give a concise description of the changes, rather than the default list of individual commits.</p>"},{"location":"archive/standards/testing/","title":"Testing","text":"<p>To support developer agility and consistent interpretation of the FFC Test Strategy, a common test structure has been agreed for usage across FFC microservices.</p>"},{"location":"archive/standards/testing/#static-code","title":"Static code","text":"<p>SonarCloud will provide static code analysis on all repositories regardless of technology choice.  </p> <p>In addition to SonarCloud, any JavaScript code is also linted using StandardJs.  This linting is performed of a test script in package.json which is also run in CI pipeline.</p> <p>There is no linting used for C# in the programme at present.</p>"},{"location":"archive/standards/testing/#unit","title":"Unit","text":"<p>Unit tests should be used to: </p> <p>target functionality that is not already covered through contract or integration testing</p> <p>add extra assurance or visibility to specific functionality, for example an application that runs a series of calculations to determine a monetary value, may benefit from individual unit tests for each calculation.</p> <p>Code should be written so that where unit tests are required, mocking is not required if possible or is minimal.</p> <p>Unit tests should mock immediate dependencies only.</p>"},{"location":"archive/standards/testing/#integration","title":"Integration","text":"<p>Integration tests verify the interactions of several components, rather than a single unit. They are classified further into narrow and local integration tests. </p>"},{"location":"archive/standards/testing/#narrow-integration","title":"Narrow integration","text":"<p>Narrow integration tests should be considered as tests that are bigger than unit tests, but smaller than integration involving other services or modules.</p> <p>For example where a unit test would traditionally mock a web server, a narrow integration test may start the web server albeit without exposing any ports externally.</p> <p>Narrow integration tests should mock immediate dependencies only.</p>"},{"location":"archive/standards/testing/#local-integration","title":"Local integration","text":"<p>Local integration tests should cover functionality that integrates with dependencies that are local to the service, for example message queues and databases.</p> <p>Rather than mocking the database or queue, local integration tests could use a database or message queue container.  This reduces the need for mocks, increasing developer agility, reducing test complexity and a higher level of confidence in tests.</p> <p>Where integration with third party services are needed, not covered by contract tests, then tests should be make use use of Docker containers and stubs to reduce the need for mocking.</p>"},{"location":"archive/standards/testing/#contract","title":"Contract","text":"<p>Contract tests should cover all provider-consumer relationships with other services that are owned by FFC.  Where one side of the contract is not owned by FFC then more traditional provider-driven or even broad integration tests with test instances of those services may be preferred.  </p> <p>Contract tests should mock the immediate dependencies of the contract similar to unit tests. </p> <p>Service architectures should be written in such a way that reduce the need for these higher risk contracts.  For example introducing an FFC managed API gateway between third party services and FFC, would allow all FFC services to contract test against the API gateway abstraction.  More complicated end to end testing is then restricted to only the API gateway.</p>"},{"location":"archive/standards/testing/#acceptance","title":"Acceptance","text":"<p>Acceptance tests should cover key journeys through the application flow via the user interface. These are  defined in a Gherkin syntax (Given, When, Then) and use cucumber and webdriver.io to mimic a scenario from the user perspective. They\u2019re executed against the microservices integrated and running as the complete service with the exception of external services (outside of the platform) which are stubbed.</p>"},{"location":"archive/standards/testing/#repository-structure","title":"Repository structure","text":""},{"location":"archive/standards/testing/#nodejs","title":"Node.js","text":"<ul> <li>app - all application code</li> <li>test</li> <li>unit - unit tests</li> <li>integration - integration tests<ul> <li>narrow - narrow integration tests</li> <li>local - local integration tests</li> </ul> </li> <li>contract - contract tests</li> <li>acceptance - acceptance tests (for frontend applications)</li> </ul> <p>The integration tests are separated in the folder structure to reflect their different dependencies. </p> <p>This enables narrow integration tests that have no external dependencies to be easily run in isolation from the local integration tests.</p>"},{"location":"archive/standards/testing/#standards","title":"Standards","text":"<ul> <li>test files should be suffixed with <code>test.js</code> </li> <li>test subfolders should be as flat as possible and not mirror application code structure</li> <li>tests should be named so it is clear which module they relate to.  For example a module caclulations/gross.js could have a test file called, calcuation-gross.test.js</li> </ul>"},{"location":"archive/standards/testing/#net-core","title":".NET Core","text":"<ul> <li>App - application csproj</li> <li>App.Tests - application tests csproj</li> <li>Integration</li> <li>Unit</li> <li>Contract</li> <li>Acceptance (for frontend applications)</li> </ul>"},{"location":"archive/standards/testing/#standards_1","title":"Standards","text":"<ul> <li>tests should be named so it is clear which module they relate to.</li> <li>all test types should be in one project</li> </ul>"},{"location":"archive/standards/testing/#running-tests","title":"Running tests","text":"<ul> <li>all tests should be executed in CI</li> <li>a <code>docker-compose.test.yaml</code> must exist in the repository to support running tests in CI.</li> <li>for local development, it may be beneficial to create a script/ Docker Compose combination to aid test environment setup and to take advantage of file watching.</li> </ul>"},{"location":"archive/standards/version-control/","title":"Version Control","text":"<p>These standards are based on Defra Digital's version control standards, but are extended to cover not only the application version, but version control of Docker images and Helm charts.</p>"},{"location":"archive/standards/version-control/#semantic-versioning","title":"Semantic Versioning","text":"<p>We use semantic versioning to manage code, images and charts.</p> <p>Given a version number MAJOR.MINOR.PATCH, increment the: - <code>MAJOR</code> version when you make incompatible API changes, - <code>MINOR</code> version when you add functionality in a backwards compatible manner, and - <code>PATCH</code> version when you make backwards compatible bug fixes.</p> <p>Additional labels for pre-release and build metadata are available as extensions to the <code>MAJOR.MINOR.PATCH</code> format.</p> <p>Full details can be found at semver.org.</p>"},{"location":"archive/standards/version-control/#application","title":"Application","text":"<ul> <li> <p>for a Node.js microservice, the version is updated in the <code>package.json</code> file's <code>version</code> property.  </p> </li> <li> <p>for a .NET microservice, the version is updated in the <code>.csproj</code> file in a <code>&lt;Version&gt;</code> tag.</p> </li> </ul>"},{"location":"archive/standards/version-control/#image","title":"Image","text":"<ul> <li> <p>prior to pushing to a container registry, all Docker images must be tagged by CI. The tag must match the application version.</p> </li> <li> <p>if the image is created as part of a pull request workflow, then the PR number is instead used as the image tag.</p> </li> </ul>"},{"location":"archive/standards/version-control/#helm-chart","title":"Helm chart","text":"<ul> <li> <p>Helm chart versions are updated in the <code>Chart.yaml</code> file's <code>version</code> property.</p> </li> <li> <p>as Helm charts are saved in the same repository as the application they manage, the version numbers are updated in sync with the application by CI.</p> </li> <li> <p>Helm charts for PR deployments are not be pushed to a Helm repository.</p> </li> </ul> <p>Note when using the FFC Jenkins library, Helm chart versioning is handled automatically, so no action is needed</p>"},{"location":"archive/standards/version-control/#releases","title":"Releases","text":"<ul> <li>releases are packaged in the source code repository using the version number of the application by CI</li> </ul>"},{"location":"archive/standards/version-control/#databases","title":"Databases","text":"<ul> <li> <p>databases use Liquibase changesets for deployment and rollback.  </p> </li> <li> <p>all changesets are versioned independently of the application.</p> </li> <li> <p>there must be an initial <code>0.0.0</code> version before any changeset, to enable full rollback of a database</p> </li> </ul>"},{"location":"archive/standards/kubernetes/","title":"Kubernetes","text":"<p>Standards supporting the usage of Kubernetes</p>"},{"location":"archive/standards/kubernetes/#contents","title":"Contents","text":"<ul> <li>Configuration</li> <li>Helm charts</li> <li>Labels</li> <li>Pod priority</li> <li>Probes</li> <li>Role Based Access Control (RBAC)</li> <li>Resource management</li> <li>Secrets</li> </ul>"},{"location":"archive/standards/kubernetes/configuration/","title":"Configuration","text":"<p>Configuration for an application running in a pod should be passed to the pod via a <code>ConfigMap</code> Kubernetes resource.</p> <p>The configuration values should be stored in Azure App Configuration and injected via delivery pipeline as documented in the Configuration Management page.</p> <p>Sensitive values should be passed to the pod via a <code>Secret</code> Kubernetes resource.</p>"},{"location":"archive/standards/kubernetes/helm-charts/","title":"Helm chart","text":"<p>Helm charts allow multiple Kubernetes resource definitions to be deployed and un-deployed as a single unit.</p> <p>Helm version 3 is used within FFC.  Currently FFC is limited to version <code>3.6</code> due to breaking changes introduced in version <code>3.7</code>.</p> <p>Helm 2 should never be used due to security risk introduced by Tiller in a cluster.</p>"},{"location":"archive/standards/kubernetes/helm-charts/#source-control","title":"Source control","text":"<ul> <li> <p>Helm charts are source controlled in the same repository as the microservice they relate to</p> </li> <li> <p>Helm charts are saved in a <code>helm</code> directory and named the same as the repository. For example a chart referencing the ELM Payment Service would be stored in the <code>./helm/elm-payment-service</code> directory</p> </li> <li> <p>Helm chart versions are automatically updated by CI in line with the application version</p> </li> </ul>"},{"location":"archive/standards/kubernetes/helm-charts/#helm-chart-library","title":"Helm chart library","text":"<ul> <li> <p>to keep Helm charts DRY, the FFC Helm Chart Library is used as a base for all resource definitions</p> </li> <li> <p>consuming Helm charts only need to define where there is variation from the base Helm chart</p> </li> <li> <p>the library helps teams abstract complexity of Kubernetes resource definitions as well as providing a consistent approach to Helm chart development and container security</p> </li> </ul>"},{"location":"archive/standards/kubernetes/helm-charts/#helm-chart-repository","title":"Helm chart repository","text":"<ul> <li> <p>Helm charts are published by CI to a Helm chart repository within Azure Container Registry</p> </li> <li> <p>Helm charts are only published on a main branch build</p> </li> <li> <p>Helm charts are automatically versioned by CI pipeline to be consistent with the application version</p> </li> <li> <p>Helm charts for in flight Pull Requests are not published</p> </li> </ul>"},{"location":"archive/standards/kubernetes/labels/","title":"Labels","text":"<p>Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. </p> <p>Labels can be used to organise and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p> <p>In order to take full advantage of using labels, they should be applied on every resource object within a Helm chart. i.e. all deployments, services, ingresses etc.</p> <p>When using the FFC Helm Chart Library, these labels are automatically applied to all resource objects.</p>"},{"location":"archive/standards/kubernetes/labels/#required-labels","title":"Required labels","text":"<p>Each Helm chart templated resource should have the below labels. Example placeholders are provided for values.</p> <pre><code>metadata:\n  labels:\n    app: {{ quote .Values.namespace }}\n    app.kubernetes.io/name: {{ quote .Values.name }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: {{ quote .Values.labels.version }}\n    app.kubernetes.io/component: {{ quote .Values.labels.component }}\n    app.kubernetes.io/part-of: {{ quote .Values.namespace }}\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\n    environment: {{ quote .Values.environment }}\n</code></pre> <p>Note <code>Deployment</code> resource objects should have two sets of labels, one for the actual deployment and another for the pod template the deployment manages.</p>"},{"location":"archive/standards/kubernetes/labels/#selectors","title":"Selectors","text":"<p>Services selectors should be matched by app and name. Selectors should be consistent otherwise updates to Helm charts will be rejected.</p> <pre><code>selector:\n  app: {{ quote .Values.name }}\n  app.kubernetes.io/name: {{ quote .Values.name }}\n</code></pre>"},{"location":"archive/standards/kubernetes/labels/#further-reading","title":"Further reading","text":"<p>More information is available in Confluence</p>"},{"location":"archive/standards/kubernetes/priority/","title":"Pod priority","text":"<p>Kubernetes Pods can have priority levels. Priority indicates the importance of a pod relative to other pods. If a pod cannot be scheduled, the scheduler tries to pre-empt (evict) lower priority pods to make scheduling of the pending pod possible.</p> <p>In the event of over utilisation of a cluster, Kubernetes will start to kill lower priority pods first to maintain stability.</p>"},{"location":"archive/standards/kubernetes/priority/#priority-levels-available-to-ffc-pods","title":"Priority levels available to FFC pods","text":"<p>There are three defined pod priority levels available within an FFC cluster.  A deployment must specify one of these levels.</p>"},{"location":"archive/standards/kubernetes/priority/#high-1000","title":"High (1000)","text":"<p>Reserved primarily for customer facing or critical workload pods.</p>"},{"location":"archive/standards/kubernetes/priority/#default-600","title":"Default (600)","text":"<p>Default option suitable for most pods.</p>"},{"location":"archive/standards/kubernetes/priority/#low-200","title":"Low (200)","text":"<p>For pods where downtime is more tolerable.</p> <p>The <code>priorityClass</code> definitions that relate to these levels can be viewed in the ffc-kubernetes-configuration repository.</p>"},{"location":"archive/standards/kubernetes/priority/#resource-profile-impact","title":"Resource profile impact","text":"<p>In the event a cluster has to make a choice between killing one of two services sharing the same priority level, the resource profile configuration will influence which is killed in the order below.   1. Best effort   2. Burstable   3. Guaranteed</p>"},{"location":"archive/standards/kubernetes/priority/#further-reading","title":"Further reading","text":"<p>More information is available in Confluence</p>"},{"location":"archive/standards/kubernetes/probes/","title":"Probes","text":"<p>To increase the stability and predictability of a Kubernetes cluster, services should make use of both readiness and liveness probes to help Kubernetes manage the stability of the service.</p> <p>Probe end points should follow the convention of <code>healthy</code> for readiness probes and <code>healthz</code> for liveness probes.</p> <p>If no probe is defined, Kubernetes will still attempt to monitor the health of a service.  However, it will only be able to do so by monitoring the status of the container.  This is not a reliable way to determine the health of a service.</p>"},{"location":"archive/standards/kubernetes/probes/#readiness-probes","title":"Readiness probes","text":"<p>Readiness probes help Kubernetes manage the availability of a service.  They are used to determine if a service is ready to receive traffic.  If a service is not ready, Kubernetes will not route traffic to it.</p> <p>Helm using the readiness probe to determine if a service is successfully deployed.  If a service does not report ready in five minutes, the deployment will be considered a failure and will be automatically rolled back.</p> <p>Note: the readiness probe only prevents traffic routing through the internal cluster network, it does not for example stop a service from consuming messages via Azure Service Bus.</p>"},{"location":"archive/standards/kubernetes/probes/#liveness-probes","title":"Liveness probes","text":"<p>Liveness probes help Kubernetes manage the stability of a service.  They are used to determine if a service is still running.  If a service is not running, Kubernetes will restart it.</p>"},{"location":"archive/standards/kubernetes/probes/#probe-configuration","title":"Probe configuration","text":"<p>Probe configuration is subjective to a service and consideration should be given to the following:</p> <ul> <li>the initial delay before probes are started</li> <li>the frequency of the probe</li> <li>how many sequential failures are required before a service is considered unhealthy</li> <li>the timeout the probe should wait for a response</li> <li>what should the probe check for to determine the health of the service</li> </ul>"},{"location":"archive/standards/kubernetes/rbac/","title":"Role Based Access Control (RBAC)","text":"<p>FFC clusters enable RBAC through <code>RoleBinding</code> Kubernetes resources.  Users must be added as Cluster Admins to access Kubernetes through tools such as Lens or kubectl.</p> <p>CCoE can provide this access to users on request.</p>"},{"location":"archive/standards/kubernetes/resource-usage/","title":"Resource usage","text":"<p>Predictable demands are important to help Kubernetes find the right place for a pod within a cluster. When it comes to resources such as a CPU and memory, understanding the resource needs of a pod will help Kubernetes allocate the pod to the most appropriate node and generally improve the stability of the cluster.</p>"},{"location":"archive/standards/kubernetes/resource-usage/#declaring-a-profile","title":"Declaring a profile","text":"<ul> <li>all pods declare both a <code>request</code> and <code>limit</code> value for CPU and memory</li> <li>production clusters do not contain any <code>best-effort</code> pods</li> <li>pods with consistent usage patterns are run as<code>guaranteed</code> pods (i.e. equal <code>request</code> and <code>limit values</code>)</li> <li>pods with spiky usage patterns can be run as <code>burstable</code> but effort should be taken to understand why performance is not consistent and whether the service is doing too much</li> </ul>"},{"location":"archive/standards/kubernetes/resource-usage/#resource-quotas","title":"Resource quotas","text":"<p>FFC clusters will limit available resources within a namespace using a <code>resourceQuota</code>. </p> <p>Teams should frequently review the resource usage of their pods and adjust the <code>resourceQuota</code> accordingly.</p> <p>Resource quotas are deployed to AKS via an ADO pipeline.</p>"},{"location":"archive/standards/kubernetes/resource-usage/#resource-profiling","title":"Resource profiling","text":"<p>Performance testing a pod is the only way to understand it's resource utilisation pattern and needs. Performance testing should take place on all pods to accurately understand their usage before they can be deployed to production.</p>"},{"location":"archive/standards/kubernetes/resource-usage/#further-reading","title":"Further reading","text":"<p>More information is available in Confluence</p>"},{"location":"archive/standards/kubernetes/secrets/","title":"Secrets","text":"<p>Where possible, secrets should not be stored within an application or Kubernetes pod.  Instead, clusters should use AAD Pod Identity as per Microsoft recommendation.</p> <p>When secrets in a pod are unavoidable, for example when a third party API key is needed, they should be injected into the pod via a <code>Secret</code> Kubernetes resource by the CI pipeline.  The secret itself should be stored in Azure Key Vault with a reference in Azure App Configuration.</p>"},{"location":"create-a-new-service/ado/","title":"Setup an Azure DevOps CD pipeline","text":"<p>Common Azure DevOps (ADO) pipeline definitions have been created and can be reused to deploy services to the various environments.</p> <p>These pipelines are responsible for not only deploying applications, but for provisioning and configuring the Azure environments the service run in.</p>"},{"location":"create-a-new-service/ado/#pipeline-categories","title":"Pipeline categories","text":"<p>Pipelines are split into different capabilities which must be run sequentially into an environment when changes are made.</p>"},{"location":"create-a-new-service/ado/#platform-core","title":"<code>platform-core</code>","text":"<p>Deploy the core platform services such as PaaS services and networking.  This pipeline rarely needs to be run and should only be done via the Platform team.</p>"},{"location":"create-a-new-service/ado/#platform-aks-configuration","title":"<code>platform-aks-configuration</code>","text":"<p>Deploy configuration for the AKS.  This pipeline should be run when a new namespace is added to support a new bounded context.</p>"},{"location":"create-a-new-service/ado/#platform-services","title":"<code>platform-services</code>","text":"<p>Deploy updates to the environment for a specific resource types including:</p> <ul> <li>Managed Identities</li> <li>Azure Service Bus</li> <li>Azure Storage</li> </ul> <p>To select the service definition, when running the pipeline select the <code>Service</code> and <code>Resource Type</code>.</p> <p>The pipeline is also capable deploying all databases and helm charts for a bounded context, but should not be used for that purpose.  Instead the Helm and database pipelines should be used as they allow deploying individual services by semantic version.</p>"},{"location":"create-a-new-service/ado/#platform-services-database","title":"<code>platform-services-database</code>","text":"<p>Deploy a database migration for a specific application version.  This pipeline should be run when a new database migration is required selecting <code>service</code>, <code>databaseRepo</code> and <code>version</code>.</p>"},{"location":"create-a-new-service/ado/#platform-services-helm","title":"<code>platform-services-helm</code>","text":"<p>Deploy a helm chart for a specific application version.  This pipeline should be run whenever an update to a service running in AKS is required selecting <code>service</code>, <code>helmChart</code> and <code>chartVersion</code>.</p>"},{"location":"create-a-new-service/ado/#creating-a-new-service-definition","title":"Creating a new service definition","text":"<p>The above pipelines are dependent on teams maintaining an ADO Git repository with their service definitions and configuration.</p> <p>This repository will need to be cloned locally and updated through a pull request.</p>"},{"location":"create-a-new-service/ado/#create-bounded-context-folder","title":"Create bounded context folder","text":"<p>Similar to Jenkins, each service should group it's service definitions in a folder matching the name of the bounded context under the <code>services</code> folder. </p> <p>For example, <code>ffc-demo</code>.</p>"},{"location":"create-a-new-service/ado/#create-definitions","title":"Create definitions","text":""},{"location":"create-a-new-service/ado/#kubernetes","title":"Kubernetes","text":""},{"location":"create-a-new-service/ado/#managed-identity","title":"Managed Identity","text":""},{"location":"create-a-new-service/ado/#helm","title":"Helm","text":""},{"location":"create-a-new-service/ado/#database","title":"Database","text":""},{"location":"create-a-new-service/ado/#service-bus","title":"Service Bus","text":""},{"location":"create-a-new-service/ado/#storage","title":"Storage","text":""},{"location":"create-a-new-service/ado/#azure-app-gateway","title":"Azure App Gateway","text":""},{"location":"create-a-new-service/choose-a-technology/","title":"Choose a technology","text":""},{"location":"create-a-new-service/choose-a-technology/#development-language-and-framework","title":"Development language and Framework","text":"<p>The primary technology choice for new FCP services is Node.js with JavaScript deployed to Azure Kubernetes Service (AKS).</p> <p>Delivery capability in the programme has been optimised for this technology stack.</p> <p>For scenarios where Node.js is not appropriate, Defra's secondary framework and language is .NET and C#.</p> <p>To deviate from Node.js and JavaScript, you must have a strong case and have received approval from a Principal Developer.</p> <p>For full details of Defra's development framework standards, see the Defra Software Development Standards.</p>"},{"location":"create-a-new-service/choose-a-technology/#nodejs","title":"Node.js","text":"<p>For web based services, Hapi.js as the web framework.  Other web frameworks such as Express are not permitted.</p> <p>Code must be written in JavaScript linted with StandardJs.  The use of TypeScript is not permitted.</p>"},{"location":"create-a-new-service/choose-a-technology/#net","title":".NET","text":"<p>For scenarios where Node.js is not appropriate, .NET can be used with approval of a Principal Developer.</p> <p>The FCP Platform has been developed to support .NET, however there are not as many supporting tools and libraries given the low adoption of the technology within the programme.</p>"},{"location":"create-a-new-service/choose-a-technology/#compute","title":"Compute","text":"<p>Azure Kubernetes Service (AKS) is the preferred compute technology.</p> <p>Kubernetes deployments must be managed using Helm.</p>"},{"location":"create-a-new-service/choose-a-technology/#databases","title":"Databases","text":"<p>PostgreSQL is the approved database technology.  It is the only database that is fully supported by the FCP Platform.</p> <p>Azure Database for PostgreSQL has been deployed to Azure.</p>"},{"location":"create-a-new-service/choose-a-technology/#code-first-migrations","title":"Code first migrations","text":"<p>Liquibase is the approved tool for managing database schema changes.  Both CI and CD pipelines have been setup to support Liquibase.</p> <p>Whilst technologies such as Sequelize and Entity Framework can be used, they should not be responsible for managing database schema changes.</p>"},{"location":"create-a-new-service/choose-a-technology/#messaging","title":"Messaging","text":"<p>Azure Service Bus is the preferred messaging technology.</p>"},{"location":"create-a-new-service/choose-a-technology/#caching","title":"Caching","text":"<p>Redis is the approved caching technology.</p> <p>Azure Cache for Redis has been deployed to Azure.</p>"},{"location":"create-a-new-service/choose-a-technology/#storage","title":"Storage","text":"<p>Azure Blob Storage, Azure File Storage and Azure Table Storage are the preferred storage technologies.</p> <p>Blob Storage should be favoured over File Storage where possible due to it's superior local development experience.</p>"},{"location":"create-a-new-service/choose-a-technology/#containers","title":"Containers","text":"<p>Docker is the approved container technology.</p>"},{"location":"create-a-new-service/conventions/","title":"Repository conventions","text":"<p>Repositories should be setup with the following conventions to ensure consistency and to ensure the CI pipeline functions correctly.</p>"},{"location":"create-a-new-service/conventions/#structure","title":"Structure","text":"<p>The following structure should be adhered to for all repositories.  Note that not all contents will be applicable depending on the nature of the service.</p> <ul> <li><code>&lt;root&gt;</code></li> <li><code>changelog</code><ul> <li>Liquibase changesets</li> </ul> </li> <li><code>docs</code><ul> <li>Documentation such as AsyncAPI and OpenAPI specifications</li> </ul> </li> <li><code>helm/&lt;repository name&gt;</code><ul> <li>Helm chart for the service</li> </ul> </li> <li><code>scripts</code><ul> <li>Scripts to support local development and testing</li> </ul> </li> <li><code>.dockerignore</code></li> <li><code>.gitignore</code></li> <li><code>.pre-commit-config.yaml</code></li> <li><code>.snyk</code></li> <li><code>secrets.baseline</code></li> <li><code>Dockerfile</code></li> <li><code>Jenkinsfile</code></li> <li><code>LICENCE</code></li> <li><code>README</code></li> <li><code>docker-compose.debug.yaml</code></li> <li><code>docker-compose.link.yaml</code></li> <li><code>docker-compose.migrate.yaml</code></li> <li><code>docker-compose.override.yaml</code></li> <li><code>docker-compose.test.debug.yaml</code></li> <li><code>docker-compose.test.watch.yaml</code></li> <li><code>docker-compose.test.yaml</code></li> <li><code>docker-compose.yaml</code></li> <li><code>provision.azure.yaml</code></li> <li><code>sonar-project.properties</code></li> </ul>"},{"location":"create-a-new-service/conventions/#nodejs","title":"Node.js","text":"<p>In addition to the above, Node.js services should include the following content.</p> <ul> <li><code>&lt;root&gt;</code></li> <li><code>app</code><ul> <li>All application code</li> </ul> </li> <li><code>test</code><ul> <li><code>unit</code></li> <li>Unit tests</li> <li><code>integration</code></li> <li><code>narrow</code><ul> <li>Narrow integration tests</li> </ul> </li> <li><code>local</code><ul> <li>Local integration tests</li> </ul> </li> <li><code>contract</code></li> <li>Contract tests</li> <li><code>acceptance</code></li> <li>UI and compatibility tests (for frontend applications)</li> </ul> </li> </ul>"},{"location":"create-a-new-service/conventions/#net","title":".NET","text":"<p>In addition to the above, .NET services should include the following content.</p> <ul> <li><code>&lt;root&gt;</code></li> <li><code>&lt;Project Name&gt;</code><ul> <li>All application code</li> </ul> </li> <li><code>&lt;Project Name&gt;.Tests</code><ul> <li>Application tests</li> </ul> </li> </ul>"},{"location":"create-a-new-service/github/","title":"Create a source code repository","text":"<p>As per Defra and GDS standards, source code should be open source and hosted on GitHub.  Other tools such as Azure DevOps, Bitbucket and GitLab are not permitted for application development.</p> <p>Scenarios where open source is not appropriate are rare and must be approved by a Principal Developer.</p> <p>All source code is hosted in the DEFRA GitHub Organisation</p> <p>Repositories should contain a single microservice/application.  Monorepos are not supported by CI pipeline.</p>"},{"location":"create-a-new-service/github/#naming-conventions","title":"Naming conventions","text":"<p>Repositories should be named in a consistent manner to aid discoverability and to ensure that they are easily identifiable as part of FCP.</p> <p>Below is the agreed naming convention for repositories.</p> <p><code>ffc-&lt;bounded context&gt;-&lt;service&gt;</code></p> <p>Where bounded context is an identifier to a set of related microservices and services is an identifier for the specific service.</p> <p>For example, <code>ffc-pay-enrichment</code> would belong to FCP, be part of the Payment ecosystem and perform an enrichment function.</p> <p>Following this naming convention helps understand ownership of repositories, avoid collisions and is essential for some CI/CD pipelines to function correctly.</p>"},{"location":"create-a-new-service/github/#creating-a-new-repository","title":"Creating a new repository","text":"<p>When creating a new repository intended for a Node.js application, the repository can be based on the ffc-template-node template repository.  </p> <p>The template includes everything needed to get a minimal Node.js application up and running with CI/CD pipelines and follows all FCP structure and naming conventions.  Once created follow the instructions in the <code>README.md</code> file to rename the assets contained within.</p> <p>For .NET there is no template repository, however, this demo repository can be used for guidance.</p> <p>Due to the GitHub permission model, only certain users can create new repositories.  If you are unable to create a new repository, please contact a Principal Developer.</p>"},{"location":"create-a-new-service/github/#repository-setup","title":"Repository setup","text":""},{"location":"create-a-new-service/github/#update-access","title":"Update access","text":"<p>All repositories should be setup to allow access to the FCP team.  This is done by adding the <code>ffc</code> team with <code>Write</code> access to the repository.</p> <p>For scenarios where it may not be appropriate to add the FCP team, note that the <code>ffcplatform</code> user account should be added with <code>Write</code> access to allow the CI pipeline to function correctly.</p>"},{"location":"create-a-new-service/github/#branch-policies","title":"Branch policies","text":"<p>Teams can extend branch policies to fit their needs, however, the following are the minimum requirements for the <code>main</code> branch on all repositories.</p> <ul> <li>protected by a branch policy</li> <li>requires at least one pull request review before merging</li> <li>stale reviews are automatically dismissed</li> <li>repositories only allow Squash merging</li> <li>must require signed commits</li> </ul> <p>For prototype and spike repositories, the branch policy can be relaxed to allow direct commits to the <code>main</code> branch.</p>"},{"location":"create-a-new-service/github/#secret-scanning","title":"Secret scanning","text":"<p>All developers are required to install Detect Secrets.</p> <p>When setting up a new repository, it needs to be configured to work with the client-side <code>detect-secrets</code> tool:</p> <p>Note that if the repository was created from the ffc-template-node template repository, this configuration will already be in place.</p> <ol> <li>Add the configuration file called <code>.pre-commit-config.yaml</code> to the repository root directory with the following contents:</li> </ol> <pre><code>repos:\n- repo: https://github.com/Yelp/detect-secrets\n  rev: v0.14.3\n  hooks:\n  - id: detect-secrets\n    args: ['--baseline', '.secrets.baseline']\n</code></pre> <ol> <li>Create and add the baseline file. To exclude files from the <code>detect-secrets</code> scan, run with <code>--exclude-files &lt;regex&gt;</code>. Examples:</li> </ol> <pre><code>detect-secrets scan &gt; .secrets.baseline\n</code></pre> <pre><code>detect-secrets scan --exclude-files package-lock.json &gt; .secrets.baseline\n</code></pre>"},{"location":"create-a-new-service/github/#licence","title":"Licence","text":"<p>The Open Government Licence (OGL) Version 3 should be added to the repository.  Example content is below.</p> <pre><code>The Open Government Licence (OGL) Version 3\n\nCopyright (c) 2024 Defra\n\nThis source code is licensed under the Open Government Licence v3.0. To view this\nlicence, visit www.nationalarchives.gov.uk/doc/open-government-licence/version/3\nor write to the Information Policy Team, The National Archives, Kew, Richmond,\nSurrey, TW9 4DU.\n</code></pre>"},{"location":"create-a-new-service/github/#jenkinsfile","title":"<code>Jenkinsfile</code>","text":"<p>In order to use the FFC CI pipeline, a <code>Jenkinsfile</code> should be added to the repository.  This file references the version of the shared pipeline to use and sets the parameters for the build.</p> <p>Details on the content of the <code>Jenkinsfile</code> can be found in the FCP CI pipeline documentation</p>"},{"location":"create-a-new-service/github/#webhooks","title":"Webhooks","text":"<p>In order to support the CI/CD pipeline, the following webhooks should be added to the repository.</p> <ol> <li>navigate to <code>Settings -&gt; Webhooks -&gt; Add webhook</code></li> <li>set <code>Payload URL</code> to be <code>https://jenkins-ffc-api.azure.defra.cloud/github-webhook/</code></li> <li>set <code>Content type</code> to be <code>application/json</code></li> <li>set <code>Secret</code> to be the webhook secret value.  This can be retrieved from Azure Key Vault <code>github-webhook-secret</code> value in the FCP Shared Services Azure subscription.</li> <li>set the <code>Pull requests</code> and <code>Pushes</code> events to trigger the webhook</li> <li>select <code>Add webhook</code></li> </ol>"},{"location":"create-a-new-service/github/#sonarcloud","title":"SonarCloud","text":""},{"location":"create-a-new-service/github/#configure-sonarcloud","title":"Configure SonarCloud","text":"<p>This step should be performed before running a build or you may end up with duplicate projects.</p> <ol> <li>navigate to the Defra organisation within SonarCloud</li> <li>select <code>Analyze new project</code></li> <li>select repository to analyse</li> <li>select <code>Set Up</code></li> <li>select <code>Administration -&gt; Analysis Method</code> and disable <code>SonarCloud Automatic Analysis</code></li> <li>select <code>Administration -&gt; Update key</code> and ensure key matches the name of the repository, removing the <code>DEFRA_</code> prefix, for example, <code>ffc-pay-enrichment</code></li> </ol>"},{"location":"create-a-new-service/github/#synk","title":"Synk","text":"<p>A Snyk scan is run as part of the CI pipeline. By default, a project will be set to private. In order to access unlimited free tests the project needs to be marked as open source. To do so, follow the instructions.</p>"},{"location":"create-a-new-service/jenkins/","title":"Setup a Jenkins CI pipeline","text":"<p>Assuming the repository has been configured correctly, including setup of SonarCloud and Snyk, the following steps should be followed to setup a Jenkins CI pipeline.</p>"},{"location":"create-a-new-service/jenkins/#create-bounded-context-folder","title":"Create bounded context folder","text":"<p>Each service should group it's CI pipelines in a Jenkins folder matching the name of the bounded context.  This is to ensure that the Jenkins UI remains manageable as the number of services grows.</p> <p>For example, a repository named <code>ffc-pay-enrichment</code> would be placed in a folder named <code>ffc-pay</code>.</p> <p>If a bounded context folder does not exist, create one by:</p> <ol> <li>navigating to the Jenkins home page</li> <li>select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>ffc-&lt;bounded context&gt;</code>, for example <code>ffc-pay</code></li> <li>select <code>Folder</code></li> <li>select <code>Ok</code></li> </ol>"},{"location":"create-a-new-service/jenkins/#create-a-build-pipeline","title":"Create a build pipeline","text":"<ol> <li>navigate to your bounded context folder in Jenkins</li> <li>select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>&lt;repository name&gt;-build</code>, for example <code>ffc-demo-web-build</code></li> <li>select <code>Multibranch Pipeline</code></li> <li>select <code>Ok</code></li> <li>enter <code>GitHub</code> as a Branch Source</li> <li>for credentials select the <code>github-token</code> with the <code>ffcplatform</code> user</li> <li>enter your GitHub URL in the <code>HTTPS URL</code> field, for example <code>https://github.com/DEFRA/ffc-demo-web.git</code></li> <li>set <code>Discover branches</code> to <code>All branches</code></li> <li>delete <code>Discover pull requests from origin</code> and <code>Discover pull requests from forks</code></li> <li>set <code>Scan Multibranch Pipeline Triggers -&gt; Periodically if not otherwise run</code> to <code>true</code> with an interval of <code>1 hour</code></li> <li>set <code>Pipeline Action Triggers -&gt; Pipeline Delete Event</code> set <code>ffc-housekeeping/cleanup-on-branch-delete</code></li> <li>set <code>Pipeline Action Triggers -&gt; Include Filter</code> to be <code>*</code></li> <li>set <code>Pipeline Action Triggers -&gt; Additional Parameter -&gt; Parameter Name</code> to be <code>repoName</code> and <code>Parameter Value</code> to be the name of the repository, for example, <code>ffc-demo-web</code></li> </ol> <p>Definitions can be copied from existing pipelines.  To save time, when creating a new pipeline select <code>Copy from</code> and enter the name of an existing pipeline.  Remember to update the <code>repoName</code> and <code>GitHub URL</code> fields.</p>"},{"location":"create-a-new-service/jenkins/#create-a-deployment-pipeline","title":"Create a deployment pipeline","text":"<p>Although Azure DevOps is used for deployments, Jenkins is used to trigger the deployment pipeline following the successful run of a main branch build.  </p> <p>The deployment to the Sandpit environment is also orchestrated by Jenkins.</p> <ol> <li>navigate to your bounded context folder in Jenkins</li> <li>select <code>New Item</code></li> <li>enter the <code>item name</code> in the format <code>&lt;repository name&gt;-deploy</code>, for example <code>ffc-demo-web-deploy</code></li> <li>select <code>Pipeline</code></li> <li>select <code>Ok</code></li> <li>select <code>This project is parameterized</code></li> <li>add the following parameters:</li> <li><code>environment</code> with a default value of <code>snd</code></li> <li><code>namespace</code> with a default value of <code>&lt;bounded context&gt;</code></li> <li><code>chartName</code> with a default value of <code>&lt;repository name&gt;</code></li> <li><code>chartVersion</code> with a default value of <code>1.0.0</code></li> <li><code>helmChartRepoType</code> with a default value of <code>acr</code></li> <li>select <code>Trigger builds remotely (e.g., from scripts)</code></li> <li>enter <code>Authentication token</code> which can be found in any existing deployment pipeline</li> <li>enter the below script in the <code>Pipeline</code> section with a <code>Definition</code> of <code>Pipeline script</code></li> </ol> <pre><code>@Library('defra-library@v-9') _\n\ndeployToCluster environment: params.environment, \n                namespace: params.namespace, \n                chartName: params.chartName, \n                chartVersion: params.chartVersion,\n                helmChartRepoType: params.helmChartRepoType\n</code></pre>"},{"location":"create-a-new-service/jenkins/#test-the-pipeline","title":"Test the pipeline","text":"<p>Commits to both feature branches and main branches should trigger the build pipeline.  Successful builds of the main branch should trigger the deployment pipeline.</p> <p>Following deployment the new service should be available in the Sandpit AKS cluster in a namespace matching the bounded context.  For example, <code>ffc-demo-web</code> would be deployed to the <code>ffc-demo</code> namespace.</p> <p>The specific steps that run during the pipeline are based on the content of the repository assuming it follows the standard structure.</p> <p>Builds can also be triggered manually by selecting <code>Build Now</code> from the pipeline page.</p>"},{"location":"development-patterns/analytics/","title":"Analytics","text":"<p>Google Tag Manager (GTM) is a tag management system that allows you to quickly and easily update tags and code snippets on your website or mobile app, such as those intended for traffic analysis and marketing optimisation. Once the GTM code is added to your site, you can configure tags via the GTM web interface without having to alter your website code.</p>"},{"location":"development-patterns/analytics/#setup-google-tag-manager","title":"Setup Google Tag Manager","text":"<p>A GTM container needed to be setup for a new service for both Non-Production and Production. </p> <p>Note that at time of writing ownership of GTM within Defra is still to be determined. Please contact a Principal Developer for support.</p> <p>Those requiring access will need create a google account using their Defra email address. Accounts can be created without using Gmail here.</p> <p>Once a container has been created, a GTM codes will be provided.  They may give the full JavaScript code snippets similar to the following:</p> <pre><code>&lt;!-- Google Tag Manager --&gt;\n&lt;script&gt;(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\nnew Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\nj=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=\n'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n  })(window,document,'script','dataLayer','GTM-M5YK7JL');&lt;/script&gt;\n&lt;!-- End Google Tag Manager --&gt;\n\n&lt;!-- Google Tag Manager (noscript) --&gt;\n&lt;noscript&gt;&lt;iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-M5YK7JL\" \nheight=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"&gt;&lt;/iframe&gt;&lt;/noscript&gt;\n&lt;!-- End Google Tag Manager (noscript) --&gt;\n</code></pre> <p>Once the GTM has been provided, the GTM guidance explains how to add the GTM code to your site.</p>"},{"location":"development-patterns/cache/","title":"Caching","text":""},{"location":"development-patterns/cache/#redis","title":"Redis","text":"<p>When an external cache is needed, Azure Cache for Redis is the recommended choice.  For local development, a Docker image can be used.</p>"},{"location":"development-patterns/cache/#hapijs","title":"Hapi.js","text":"<p>Typically a cache is needed to work with a web server such as Hapi.js.  Hapi caching is described in their official documentation. The following guide specifically describes how server-side Redis caching can be added to a Hapi based microservice.</p>"},{"location":"development-patterns/cache/#server-side-caching-in-hapi","title":"Server-side caching in Hapi","text":"<p>Hapi server-side caching uses the catbox interface to abstract away the underlying caching technology being used (e.g. memory, Redis, Memcached).</p> <p></p> <p>There are three main concepts to Hapi server-side caching: * The cache strategy (or provider): is the underlying caching technology being employed. Here catbox-redis, the Redis adapter for catbox, is the strategy. * The cache client: is the low-level cache abstraction, and is initialised using a cache stragegy (e.g. memory or Redis). Hapi initialises an in-memory cache client by default, and you can create additional cache clients using the same or different strategies (e.g. you can have one in-memory cache, and one Redis cache). * The cache policy: is a higher-level cache abstraction that sets a policy on the storage within the cache (e.g. expiry times). The cache policy also provides additional segmentation within the cache client. Typically the cache policy is how you would interact with cache values via the <code>get</code> and <code>set</code> menthods.</p>"},{"location":"development-patterns/cache/#configuring-the-default-cache-client","title":"Configuring the default cache client","text":"<p>As mentioned above, Hapi initialises an in-memory cache client by default. If you wish you can make the default cache client use a different strategy. For example, the following would make the default cache strategy use Redis or memory depending on the value of <code>config.useRedis</code>:</p> <pre><code>const catbox = config.useRedis ? require('@hapi/catbox-redis') : require('@hapi/catbox-memory')\nconst catboxOptions = config.useRedis\n  ? {\n      host: process.env.REDIS_HOSTNAME,\n      port: process.env.REDIS_PORT,\n      password: process.env.REDIS_PASSWORD,\n      partition: process.env.REDIS_PARTITION,\n      tls: process.env.NODE_ENV === 'production' ? {} : undefined\n    }\n  : {}\n\nconst server = hapi.server({\n  port: config.port,\n  cache: [{\n    provider: {\n      constructor: catbox,\n      options: catboxOptions\n    }\n  }]\n}\n</code></pre>"},{"location":"development-patterns/cache/#configuring-new-cache-clients","title":"Configuring new cache clients","text":"<p>Additional cache clients can be created when initialising the Hapi server by adding new definitions to the <code>cache</code> array. Additional caches are required to be given a <code>name</code>. For example, the following will create a new Redis cache client called session.</p> <pre><code>const catbox = require('@hapi/catbox-redis')\nconst catboxOptions = {\n  host: process.env.REDIS_HOSTNAME,\n  port: process.env.REDIS_PORT,\n  password: process.env.REDIS_PASSWORD,\n  partition: process.env.REDIS_PARTITION,\n  tls: process.env.NODE_ENV === 'production' ? {} : undefined\n}\n\nconst server = hapi.server({\n  port: config.port,\n  cache: [{\n    name: 'session',\n    provider: {\n      constructor: catbox,\n      options: catboxOptions\n    }\n  }]\n}\n</code></pre> <p>NOTE 1: This example will create two cache clients, the default in-memory cache client and a new cache client called <code>session</code> that uses Redis</p> <p>NOTE 2: Hapi will always use the default in-memory cache client unless you specify the <code>name</code> when using it (either directly or via the cache policy, see below)</p>"},{"location":"development-patterns/cache/#creating-and-using-a-cache-policy","title":"Creating and using a cache policy","text":"<p>Lastly we create the cache policy, which is typically how we interact with the cache (see the catbox policy documentation for more details and how set and get data in the cache).</p> <p>When creating a cache policy, if you don't explicitly provide the name of a cache client (via the <code>cache</code> property), it will use the default cache client.</p> <p>To create a cache policy using a segment within the default cache client:</p> <pre><code>myCache = server.cache({\n  expiresIn: 36000,\n  segment: 'mySegment'\n  // ... any other configuration\n})\n</code></pre> <p>To create a cache policy using a segment within a named cache client (in this case <code>session</code>):</p> <pre><code>myCache = server.cache({\n  cache: 'session'\n  expiresIn: 36000,\n  segment: 'mySegment'\n  // ... any other configuration\n})\n</code></pre>"},{"location":"development-patterns/cache/#integration-with-yar-session-cookies","title":"Integration with yar session cookies","text":"<p>Hapi yar is a plugin that adds unauthenticated session support (state across multiple browser request) to Hapi. By default it tries to fit session data into a session cookie, but will use server-side storage via the Hapi cache interface if the session data is greater than the max size specified when registering the plugin.</p> <p>Combining Hapi yar with Redis caching is one way to allow multiple replicates of a web server microservice to share server-side user session data.</p> <p>Example configuration using the default cache client:</p> <pre><code>server.register({\n  plugin: require('@hapi/yar'),\n  options: {\n    cache: {\n      expiresIn: 36000\n    },\n    maxCookieSize: 0 // this will force server-side caching\n    // ... other config here\n  }\n})\n</code></pre> <p>Example configuration using a named cache client:</p> <pre><code>server.register({\n  plugin: require('@hapi/yar'),\n  options: {\n    cache: {\n      cache: 'session'\n      expiresIn: 36000\n    },\n    maxCookieSize: 0 // this will force server-side caching\n    // ... other config here\n  }\n})\n</code></pre>"},{"location":"development-patterns/configuration/","title":"Configuration and secrets","text":"<p>Non sensitive application configuration data is persisted in the Platform repository for all environments other than Sandpit.</p> <p>For Sandpit, Azure Application Configuration is used.</p> <p>Sensitive application data is persisted in Azure Key Vault and referenced from the main configuration store.</p> <p>Each environment has it's own instance of the config stores and Key Vault.</p> <p>The configuration keys used must follow the naming convention below to ensure that CI and CD pipelines can retrieve values in a predictable and efficient way.</p>"},{"location":"development-patterns/configuration/#helm-values-in-deployment","title":"Helm values in deployment","text":"<p>During deployment, our CI or CD pipeline will read the <code>values.yaml</code> file in the Helm chart to identify all potential configuration values that may need to be sourced from Application Configuration and then matches each of them by key name.</p> <p>For example, if a helm chart contains the below, then the key to be matched will be <code>container.port</code>.</p> <pre><code>container:\n  port: 3000\n</code></pre> <p>If a key exists then the value, <code>3000</code> in the example, will be overwritten with the value in Application Configuration.</p>"},{"location":"development-patterns/configuration/#key-naming-standards","title":"Key naming standards","text":"<p>As all FFC teams will be sharing a single instance of Application Configuration in each subscription and that subscription may host multiple environments, we need to ensure our key naming convention is scalable, performant and avoids duplication where possible.</p> <p>There are five accepted formats a key name can follow.  The examples all use the <code>container.port</code> key as a demonstration for simplicity.</p>"},{"location":"development-patterns/configuration/#commonkey","title":"<code>common/key</code>","text":"<p>These are values that are consistent, regardless of service or environment. </p> <p>For example <code>common/container.port</code> would mean all services would all get the same value wherever they deploy.</p> <p>The <code>common/</code> text is necessary due to the limitation the Azure CLI used in pipelines returns keys.  Without the prefix there would be no way to only return these common items.</p>"},{"location":"development-patterns/configuration/#environmentkey","title":"<code>environment/key</code>","text":"<p>These are values that are environment specific, but not service specific.  </p> <p>For example, <code>dev/container.port</code> would mean all services would get the same value when deploying to the <code>dev</code> environment.</p>"},{"location":"development-patterns/configuration/#servicecommonkey","title":"<code>service/common/key</code>","text":"<p>These are values that are service specific, but not environment specific.</p> <p>For example, <code>ffc-demo/common/container.port</code> would mean all deployments belonging to the <code>ffc-demo</code> service would get the same value wherever they deploy.</p> <p>As before the <code>/common/</code> text is necessary to ensure the Azure CLI returns only relevant values.</p>"},{"location":"development-patterns/configuration/#serviceenvironmentkey","title":"<code>service/environment/key</code>","text":"<p>These are values that are service and environment specific.</p> <p>For example, <code>ffc-demo/dev/container.port</code> would mean all deployments belonging to the <code>ffc-demo</code> service would get the same value when deploying to the <code>dev</code> environment.</p>"},{"location":"development-patterns/configuration/#prkey","title":"<code>pr/key</code>","text":"<p>These are only used for PR deployments in CI when you need to provide a different value for that PR deployment and still be able to provide individual microservie labels (see below).  They are also used by CI to obtain dynamic infrastructure values.</p> <p>For example, <code>pr/container.port</code> would mean all PR deployments would get the same port value.</p>"},{"location":"development-patterns/configuration/#microservice-specific-values","title":"Microservice specific values","text":"<p>When you want to provide a value depending on the specific microservice and not just the service you can add a different label to that key referencing the name of the microservice.</p> <p>For example, if you have a key <code>ffc-demo/common/container.port</code> with an unlabelled value of <code>4000</code> and a value of <code>5000</code> labelled <code>ffc-demo-web</code>, then all <code>ffc-demo</code> services would get a value of <code>4000</code> except the <code>ffc-demo-web</code> microservice which would get <code>5000</code>.</p>"},{"location":"development-patterns/configuration/#order-values-are-overridden","title":"Order values are overridden","text":"<p>Within the CI and CD pipelines, the values will be sourced and overwritten in the following order.</p> <ol> <li><code>common/key</code></li> <li><code>common/key</code> plus microservice label</li> <li><code>environment/key</code></li> <li><code>environment/key</code> plus microservice label</li> <li><code>service/common/key</code></li> <li><code>service/common/key</code> plus microservice label</li> <li><code>service/environment/key</code></li> <li><code>service/environment/key</code> plus microservice label</li> <li><code>pr/key</code> (CI PR deployment only)</li> <li><code>pr/key</code> plus microservice label (CI PR deployment only)</li> </ol>"},{"location":"development-patterns/configuration/#deciding-which-convention-to-use","title":"Deciding which convention to use","text":"<p>To improve performance in CI, keys should follow a convention that gives the narrowest scope possible.  </p> <p>For example, if a value is only used for one service or will vary depending on the service then it should be scoped to either <code>service/common</code> or <code>service/environment</code></p> <p>Only values that are used by everyone should be scoped to <code>common/</code> or <code>environment/</code></p>"},{"location":"development-patterns/configuration/#exceptions-to-the-rule","title":"Exceptions to the rule","text":"<p>The are some values that are exceptions to this rule because of dynamic infrastructure provisioning and related test execution in CI.</p> <p>Note: these exceptions only apply to the Sandpit (<code>SND</code>) environment.</p>"},{"location":"development-patterns/configuration/#ingress-values","title":"Ingress values","text":"<p>Helm charts define their ingress resources like the below:</p> <pre><code>ingress:\n  server:\n  endpoint:\n  class:\n</code></pre> <p>These values must use the hierarchy <code>environment/key</code> for example <code>dev/ingress.server</code> and use labels to separate each service's value.</p>"},{"location":"development-patterns/configuration/#database-values","title":"Database values","text":"<p>Helm charts define their database names like the below:</p> <pre><code>postgresService:\n  postgresUser:\n  postgresDb:\n</code></pre> <p>These values must use the hierarchy <code>environment/key</code> for example <code>dev/postgresService.postgresUser</code> and use labels to separate each service's value. </p>"},{"location":"development-patterns/configuration/#updating-values","title":"Updating values","text":"<p>In order for an application to pick up new configuration changes, the application must be redeployed by re-running the release pipeline.</p> <p>In Sandpit, values are added direct to Application Configuration via the Azure Portal or Azure CLI.  For other environments, the Platform repository is updated via a pull request, prior to re-running the pipeline.</p> <p>Full details for maintaining configuration in the Platform repository can be found in this wiki.</p> <p>Note: Key vault values MUST be added to Key Vault before merging any changes to the pipeline repository.  Otherwise all builds will fail for that service.  Only CCoE can update PreProduction and Production Key Vault values.</p>"},{"location":"development-patterns/containers/","title":"Developing in a container","text":"<p>The Architecture Vision prescribes a containerised microservice ecosystem.  Docker is the containerisation technology used within Defra.</p> <p>Containers are lightweight and fast. One of their main benefits for developers is that it is simple to replicate an application\u2019s environment and dependencies locally consistently. </p> <p>Crucially, they enable a workflow for your code that allows you to develop and test locally, push to upstream, and be confident that what you have built locally will work in CI and any environment.</p>"},{"location":"development-patterns/containers/#docker","title":"Docker","text":"<p>All FFC microservices are built from supported Defra parent images for Node.js and .NET.  A <code>Dockerfile</code> will be included in each microservice repository containing a multi-stage build definition referencing these images.</p> <p>All microservice repositories created from the FFC Node template will include this <code>Dockerfile</code> already configured.</p>"},{"location":"development-patterns/containers/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications using yaml configuration files.</p> <p>All microservice repositories created from the FCP Node template include a pre-configured set of Docker Compose yaml files to support local development and testing as well as some being a prerequisite for CI capability.</p> <p>An example Node.js <code>Dockerfile</code> showing a multi-stage build for both development and production.  Note that the <code>development</code> image runs the application in <code>watch</code> mode to support local development and testing, whilst <code>production</code> simply runs the application.</p> <p><code>development</code> is dependent on the local <code>package.json</code> including a watch script.  More on this below.</p> <pre><code>ARG PARENT_VERSION=1.2.9-node14.17.6\nARG PORT=3000\nARG PORT_DEBUG=9229\n\n# Development\nFROM defradigital/node-development:${PARENT_VERSION} AS development\nARG PARENT_VERSION\nLABEL uk.gov.defra.ffc.parent-image=defradigital/node-development:${PARENT_VERSION}\n\nARG PORT\nARG PORT_DEBUG\nENV PORT ${PORT}\nEXPOSE ${PORT} ${PORT_DEBUG}\n\nCOPY --chown=node:node package*.json ./\nRUN npm install\nCOPY --chown=node:node . .\nCMD [ \"npm\", \"run\", \"start:watch\" ]\n\n# Production\nFROM defradigital/node:${PARENT_VERSION} AS production\nARG PARENT_VERSION\nLABEL uk.gov.defra.ffc.parent-image=defradigital/node:${PARENT_VERSION}\n\nARG PORT\nENV PORT ${PORT}\nEXPOSE ${PORT}\n\nCOPY --from=development /home/node/app/ ./app/\nCOPY --from=development /home/node/package*.json ./\nRUN npm ci\nCMD [ \"node\", \"app\" ]\n</code></pre>"},{"location":"development-patterns/containers/#docker-composeyaml","title":"<code>docker-compose.yaml</code>","text":"<p>Used to define creation of the production image locally and in CI.  This file should include all configuration needed to create a clean production image.  Port and local volume mapping should be avoided in this file.</p> <p>The template repository will set a <code>container_name</code> property in this file so that containers created have shorter and more predictable names to support local development.  However, if local scaling of container instances is required, then this property should be removed as container names will need to be dynamic in that scenario.</p> <p>To avoid duplication, other dependent container images can be defined in this file such as PostgreSQL or Redis, but no volume or port bindings for those dependencies should be included.</p>"},{"location":"development-patterns/containers/#docker-composeoverrideyaml","title":"<code>docker-compose.override.yaml</code>","text":"<p>Used to apply overrides to <code>docker-compose.yaml</code> to support local development.  This is where port and volume mappings should be declared.</p> <p>If dependencies such as PostgreSQL or Redis are used, this is the file where volume and port bindings should be declared for those dependencies.</p> <p>This image will build the <code>development</code> image which typically is the same as production but will run the code in <code>watch</code> mode so changes made to the code locally are automatically picked up in the container and restart the application.</p>"},{"location":"development-patterns/containers/#avoiding-port-conflicts","title":"Avoiding port conflicts","text":"<p>When binding container ports to localhost, it is important to consider any conflicts that may occur with other services developers may wish to run locally.</p> <p>For example, if a service is made up of two microservices, both running on port <code>3000</code>.  Then both cannot be mapped to <code>localhost:3000</code> without a conflict.</p> <p>In this scenario, to successfully run both services on the same device with port binding, one of the services should bind the container's port <code>3000</code> to a different localhost port.</p> <p>The same consideration should be given to the debug port exposed to avoid a conflict on port <code>9229</code>, the default Node debug port.</p> <pre><code># service 1 docker-compose.override.yaml\nports:\n  - \"3000:3000\"\n  - \"9229:9229\"\n\n# service 2 docker-compose.override.yaml\nports:\n  - \"3001:3000\"\n  - \"9230:9229\"\n</code></pre> <p>This equally applies when binding dependency images such as PostgreSQL and Redis.</p>"},{"location":"development-patterns/containers/#docker-composedebugyaml","title":"<code>docker-compose.debug.yaml</code>","text":"<p>Used to start application in debug mode.  This is only required if you wish the application to wait for the debugger before starting the application.  If you just wish to attach a debugger to an already running instance, the override file is sufficient.</p>"},{"location":"development-patterns/containers/#docker-composetestyaml","title":"<code>docker-compose.test.yaml</code>","text":"<p>Used to run all tests in the repository.  This is a dependency of the FFC CI pipeline.  Port bindings should be avoided in this file to avoid conflicts between running builds.</p>"},{"location":"development-patterns/containers/#docker-composetestwatchyaml","title":"<code>docker-compose.test.watch.yaml</code>","text":"<p>Used as an override to <code>docker-compose.test.yaml</code> to run tests in <code>watch</code> mode to support Test Driven Development (TDD).  Changes to either application or test code will automatically trigger re-running of affected tests.</p> <p>All Node.js FFC microservices use Jest.  Jest has powerful capability to support multiple watch scenarios such as running individual tests, only tests affected by changes, only failed tests, filtering by regular expression as well as running the full suite.</p> <p>In order to understand which code has changed, Jest uses the local <code>.git</code> directory.  This means when running tests in a container, the local <code>.git</code> folder must be mounted to the container in <code>docker-compose.watch.yaml</code>.</p> <pre><code>volumes:\n  - ./.git:/home/node/.git\n</code></pre>"},{"location":"development-patterns/containers/#docker-composetestdebugyaml","title":"<code>docker-compose.test.debug.yaml</code>","text":"<p>Used to run Jest in watch mode but support debugging of tests.  This allows developers to have all the capability of <code>watch</code> but with the added bonus of being able to attach a debugger.</p> <p>Details of debugging tests are below.</p>"},{"location":"development-patterns/containers/#packagejson-scripts","title":"package.json scripts","text":"<p>To enable the capability provided by the above Docker Compose files, <code>package.json</code> needs to be configured to support the scripts referenced in the <code>command</code>.</p> <p>Below is an extract of the default <code>package.json</code> file provided by FFC Node template.</p> <pre><code>\"scripts\": {\n    \"pretest\": \"npm run test:lint\",\n    \"test\": \"jest --runInBand --forceExit\",\n    \"test:watch\": \"jest --coverage=false --onlyChanged --watch --runInBand\",\n    \"test:debug\": \"node --inspect-brk=0.0.0.0 ./node_modules/jest/bin/jest.js --coverage=false --onlyChanged --watch --runInBand --no-cache\",\n    \"test:lint\": \"standard\",\n    \"start:watch\": \"nodemon --inspect=0.0.0.0 --ext js --legacy-watch app/index.js\",\n    \"start:debug\": \"nodemon --inspect-brk=0.0.0.0 --ext js --legacy-watch app/index.js\"\n</code></pre>"},{"location":"development-patterns/containers/#pretest","title":"<code>pretest</code>","text":"<p>This will automatically run before the <code>test</code> script and will lint all JavaScript files in according with StandardJs standards.</p>"},{"location":"development-patterns/containers/#test","title":"<code>test</code>","text":"<p>This will run all Jest tests within the repository with no watch mode enabled and will output code coverage results on test completion.  This is primary used for CI, but can be run locally as a quick check of test status.</p> <p><code>--runInBand</code> will ensure that tests run sequentially rather than parallel.  Although this will result in slower running overall, it means that integration tests spanning containers have connections that are cleanly and predictably open and closed to avoid test disruption.</p> <p><code>--forceExit</code> will force Jest close a test with open connections 1 second after completion of the test.  Ideally this would not be needed, however in some scenarios Jest is unable to determine whether a Hapi server is still running even if it is cleanly shut down in the test.</p>"},{"location":"development-patterns/containers/#testwatch","title":"<code>test:watch</code>","text":"<p>This will run tests in <code>watch</code> mode and is the most commonly used by developers to support TDD.</p> <p><code>--coverage=false</code> - as typically only running a subset of tests, there is little value displaying a test coverage summary.  Disabling it also reduces lines written to the console to support developer focus.</p> <p><code>--onlyChanged</code> - start by only running tests that are affected by code changes.  Accuracy of this is dependent on the <code>.git</code> folder being mounted to the volume as described above as well as the folders containing test and application code.</p> <p><code>--watch</code> - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run.  Developers have the option to change the behaviour of watch mode.</p>"},{"location":"development-patterns/containers/#testwatch_1","title":"<code>test:watch</code>","text":"<p>This will run tests in <code>watch</code> mode and is the most commonly used by developers to support TDD.</p> <p><code>--coverage=false</code> - as typically only running a subset of tests, there is little value displaying a test coverage summary.  Disabling it also reduces lines written to the console to support developer focus.</p> <p><code>--onlyChanged</code> - start by only running tests that are affected by code changes.  Accuracy of this is dependent on the <code>.git</code> folder being mounted to the volume as described above.</p> <p><code>--watch</code> - will run tests in watch mode, so as code, either in application or test, is changed the tests are automatically re-run.  Developers have the option to change the behaviour of watch mode.</p>"},{"location":"development-patterns/containers/#testdebug","title":"<code>test:debug</code>","text":"<p>This runs tests in <code>watch</code> mode and has all the same behaviour and running options as <code>test:watch</code>.  However, the key difference it this will wait for a debugger to be attached before starting test execution.  This enables developers to apply breakpoints in the test and application code to debug troublesome tests.</p> <p>An example Visual Studio Code debugging profile to use this script is provided below.</p> <p><code>--no-cache</code> - Jest caches files between test runs which can result in some breakpoints not being hit.  This disables that behaviour.</p>"},{"location":"development-patterns/containers/#testlint","title":"<code>test:lint</code>","text":"<p>Run linting with StandardJs only.</p>"},{"location":"development-patterns/containers/#startwatch","title":"<code>start:watch</code>","text":"<p>Starts the application in watch mode.  This is typically how developers will run all applications locally.  As code is changed, the running application in the container automatically identifies the changes and restarts the running application.</p> <p>Nodemon is used to orchestrate restarting of the application.</p> <p>This is dependent on the application code folder having a volume binding to the container.</p>"},{"location":"development-patterns/containers/#startdebug","title":"<code>start:debug</code>","text":"<p>This has the same behaviour as <code>start:watch</code> but like <code>test:debug</code> will wait for a debugger to be attached before executing any code.</p>"},{"location":"development-patterns/containers/#convenience-scripts","title":"Convenience scripts","text":"<p>Repositories created from FFC Node template will include two convenience scripts to support developers easily running the application utilising the above setup.</p>"},{"location":"development-patterns/containers/#scriptsstart","title":"<code>./scripts/start</code>","text":"<p>Run the application using Docker Compose.  Typically this is just a simple abstraction over <code>docker-compose up</code> however, is can be extended to ensure container runs in a specific container network or run database migrations prior to starting the application for example.</p>"},{"location":"development-patterns/containers/#scriptstest","title":"<code>./scripts/test</code>","text":"<p>Run tests.  Without any arguments provided will run the <code>test</code> script in <code>package.json</code>.</p> <p>To ensure clean running, all test containers are recreated.  Note that this will not affect data persisted in development databases for example if the above setup is followed.</p>"},{"location":"development-patterns/containers/#optional-arguments","title":"Optional arguments","text":"<p><code>-h</code> - shows all available arguments.</p> <p><code>-w</code> - runs tests in watch mode using <code>test:watch</code> script</p> <p><code>-d</code> - runs tests in debug mode using <code>test:debug</code> script</p>"},{"location":"development-patterns/containers/#debugging-code-running-in-a-container","title":"Debugging code running in a container","text":"<p>If the above setup is followed, then everything is in place to support debugging of applications and tests in containers.</p> <p>Developers are free to use their own choice of IDE, however, all example debug configurations within this guide will assume Visual Studio Code is used.</p> <p>These debug configurations should all be included in a <code>launch.json</code> file in the <code>.vscode</code> folder at the root of the repository.  This folder should be excluded from source control.</p>"},{"location":"development-patterns/containers/#application-debugging-profiles","title":"Application debugging profiles","text":""},{"location":"development-patterns/containers/#attach-to-an-already-running-container","title":"Attach to an already running container","text":"<pre><code>{\n  \"name\": \"Docker: Attach\",\n  \"type\": \"node\",\n  \"request\": \"attach\",\n  \"restart\": true,\n  \"port\": 9229,\n  \"remoteRoot\": \"/home/node\",\n  \"skipFiles\": [\n    \"&lt;node_internals&gt;/**\",\n    \"**/node_modules/**\"\n  ]\n}\n</code></pre> <p>This will attach to the node process exposed by the debug port.  Note that this uses the <code>localhost</code> port not the container port.  So if port <code>9229</code> is bound to a different port locally, then this value should be changed to match here.</p> <p><code>restart</code> - will ensure that as code is changed and the application restarted, the debugger is automatically reattached.</p> <p><code>remoteRoot</code> - must match the location of the code that matches the local workspace structure.  When using the Node.js Defra Docker base images the location will always be <code>home/node</code>.</p> <p><code>skipFiles</code> - an array of locations where debugging such skip.  Typically this would be internal Node.js code as well as those from third party npm modules.</p>"},{"location":"development-patterns/containers/#start-an-application-in-debug-mode","title":"Start an application in debug mode","text":"<pre><code>{\n      \"name\": \"Docker: Attach Launch\",\n      \"type\": \"node\",\n      \"request\": \"attach\",\n      \"remoteRoot\": \"/home/node\",\n      \"restart\": true,\n      \"port\": 9229,\n      \"skipFiles\": [\n        \"&lt;node_internals&gt;/**\",\n        \"**/node_modules/**\"\n      ],\n      \"preLaunchTask\": \"compose-debug-up\",\n      \"postDebugTask\": \"compose-debug-down\"\n    },\n</code></pre> <p>This will start a new container in debug mode using the <code>start:debug</code> <code>package.json</code> script.  The application will wait for a debugger before running any code.</p> <p>This is dependent on <code>preLaunchTask</code> and <code>postDebugTask</code> being defined in a <code>.vscode/tasks.json</code> file.</p> <p>An example of this is below.</p> <pre><code>{\n  \"version\": \"2.0.0\",\n  \"tasks\": [\n    {\n      \"label\": \"compose-debug-up\",\n      \"type\": \"shell\",\n      \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml up -d\"\n    },\n    {\n      \"label\": \"compose-debug-down\",\n      \"type\": \"shell\",\n      \"command\": \"docker-compose -f docker-compose.yaml -f docker-compose.override.yaml -f docker-compose.debug.yaml down\"\n    }\n  ]\n}\n</code></pre>"},{"location":"development-patterns/containers/#test-debugging","title":"Test debugging","text":"<pre><code>{\n  \"name\": \"Docker: Jest Attach\",\n  \"type\": \"node\",\n  \"request\": \"attach\",\n  \"port\": 9229,\n  \"restart\": true,\n  \"timeout\": 10000,\n  \"remoteRoot\": \"/home/node\",\n  \"disableOptimisticBPs\": true,\n  \"continueOnAttach\": true,\n  \"skipFiles\": [\n    \"&lt;node_internals&gt;/**\",\n    \"**/node_modules/**\"\n  ]\n}\n</code></pre> <p>This assumes that the <code>./script/test -d</code> command referenced above has already been run and the test suit is waiting for the debugger to attach.  This profile will attach that debugger.</p> <p><code>disableOptimisticBPs</code> - this needs to be set as <code>true</code> as Jest takes a copy of all test files and uses these copies for execution.  If this is not disabled then this process can result in breakpoints not being mapped to their correct location.</p> <p><code>continueOnAttach</code> - instruct the pending test execution to continue once the debugger is attached.</p>"},{"location":"development-patterns/containers/#other-debugging-profiles","title":"Other debugging profiles","text":"<p>A range of different debugging profiles can be found in this repository as well as a test application setup to the above standards to test them.</p>"},{"location":"development-patterns/containers/#debugging-net-in-a-linux-container","title":"Debugging .NET in a Linux container","text":"<p>.NET services can be developed using VS Code or Visual Studio.</p> <p>As all FCP services are designed to be developed and run in Linux containers, debugging them requires the attachment of a debugger from the running container.  </p> <p>In the case of .NET, this is dependent on a remote debugger being present in the container image.</p> <p>All FCP services based on the Defra .NET development image include the <code>vsdbg</code> remote debugger.</p>"},{"location":"development-patterns/containers/#attaching-to-the-remote-debugger","title":"Attaching to the remote debugger","text":""},{"location":"development-patterns/containers/#vs-code","title":"VS Code","text":"<ul> <li>add your breakpoint</li> <li>start the container with <code>docker-compose up --build</code> from the repository root directory</li> <li>in VS Code create a <code>launch.json</code> configuration similar to the below substituting the name of the container, <code>ffc-demo-payment-service-core</code></li> </ul> <p><code>json   {     \"version\": \"0.2.0\",     \"configurations\": [       {         \"name\": \".NET Core Docker Attach\",         \"type\": \"coreclr\",         \"request\": \"attach\",         \"processId\": \"${command:pickRemoteProcess}\",         \"pipeTransport\": {           \"pipeProgram\": \"docker\",           \"pipeArgs\": [ \"exec\", \"-i\", \"ffc-demo-payment-service-core\" ],           \"debuggerPath\": \"/vsdbg/vsdbg\",           \"pipeCwd\": \"${workspaceRoot}\",           \"quoteArgs\": false         },         \"sourceFileMap\": {           \"/home/dotnet\": \"${workspaceFolder}\"         }       }     ]   }</code></p> <ul> <li>start the VS Code debugger using this launch configuration</li> <li>in the context menu, select the process matching the running application, eg. <code>FFCDemoPaymentService</code></li> <li>the breakpoint can now be hit within VS Code</li> </ul>"},{"location":"development-patterns/containers/#visual-studio","title":"Visual Studio","text":"<p>Visual Studio does not integrate with the WSL filesystem, so WSL users must clone the repository in Windows to debug using Visual Studio. </p> <p>It is important that the following git configuration setting is present to ensure that cloning in Windows does not alter existing line endings</p> <pre><code>git config --global core.autocrlf input\n</code></pre> <p>For services which require environment variables to be read from the host, it is recommended to store these in a <code>.env</code> file in the repository as Docker Compose will automatically read this file when running the container.  This file must be excluded from source control.</p> <p>This process has a prerequisite of the user having Docker Desktop installed which includes Docker Compose by default.</p> <ol> <li>add your break point</li> <li>using Powershell, start the container with <code>docker-compose up --build</code> from the repository root directory</li> <li>in Visual Studio, select <code>Debug -&gt; Attach to process</code></li> <li>select <code>Docker (Linux Container)</code> for <code>Connection type</code></li> <li>type the name of the container in <code>Connection target</code>, eg. <code>ffc-demo-payment-service</code></li> <li>click <code>Refresh</code></li> <li>select process matching running application, eg <code>FFCDemoPaymentService</code></li> <li>click <code>Attach</code></li> <li>select <code>Managed (.NET Core for Unix)</code> code type</li> <li>click <code>Ok</code></li> <li>the breakpoint can now be hit within Visual Studio</li> </ol> <p>Note volume mounts do not appear to work with this approach, so for changes to be picked up, the container will need to be recreated.</p>"},{"location":"development-patterns/containers/#other-useful-docker-development-guides","title":"Other useful Docker development guides","text":"<p>Defra has well documented standards and guidance on developing with containers, that provides further examples of good local development practice.</p>"},{"location":"development-patterns/git-secrets/","title":"Git secrets","text":"<p>When working in the open it is important to ensure that no secrets are committed to a public repository. </p> <p>All developers setup <code>detect-secrets</code> to scan their code for secrets before committing.</p>"},{"location":"development-patterns/git-secrets/#dealing-with-false-positives","title":"Dealing with false positives","text":"<p><code>detect-secrets</code> often identifies false positives (something it thinks is a secret, but is not), which will stop the developer from committing their changes. We have two strategies for dealing with false positives.</p> <ol> <li> <p>For one-time false positives, they can be overridden by committing with the <code>--no-verify</code> flag. This will commit the change, but any future commits with changes to the file containing the false positive will result in it being detected again.</p> </li> <li> <p>False positives can be permanently ignored by adding them to the secrets baseline. Run the following command and commit the updated <code>.secrets.baseline</code> file:</p> </li> </ol> <pre><code>detect-secrets scan --update .secrets.baseline\n</code></pre>"},{"location":"development-patterns/identity/","title":"Identity","text":"<p>Managed Identities should be used to authenticate applications with PaaS resources in Azure.</p> <p>Credentials such as connection strings and passwords should be avoided where possible to avoid the need for secret management and rotation.</p> <p>Managed identities can be created by updating the Platform repository and running the appropriate pipeline.</p> <p>A Managed Identity should correspond to an application and an application should only have one identity.</p>"},{"location":"development-patterns/identity/#create-a-managed-identity-sandpit-only","title":"Create a Managed Identity (Sandpit only)","text":"<p>In Sandpit, Managed Identities must be created separately.</p> <p>The Managed Identity should adhere to the following naming convention:</p> <p><code>ffc-&lt;cloud-environment&gt;-&lt;workstream&gt;-&lt;service&gt;-role</code></p> <p>for example <code>ffc-snd-demo-web-role</code>.</p> <p>Once created, Azure Role Assignments can be added for the Managed Identity to access other Azure resources.</p>"},{"location":"development-patterns/identity/#update-helm-chart","title":"Update Helm chart","text":"<p>The FCP Platform uses Azure Active Directory Pod Identity to bind the Managed Identity to the Pod within AKS.</p> <p>The enable this, add and configure the <code>AzureIdentity</code> and <code>AzureIdentityBinding</code> Kubernetes templates from the ffc-helm-library to your Helm Chart (<code>helm/&lt;REPO_NAME&gt;/templates/</code>) following the Helm Library guidance.</p> <p>Note the <code>usePodIdentity</code> value in the <code>values.yaml</code> file should be set to <code>true</code>.</p>"},{"location":"development-patterns/identity/#add-managed-identity-values-to-azure-app-configuration","title":"Add Managed Identity values to Azure App Configuration","text":"<p>In order for For your newly created Managed Identity to be correctly injected, the appropriate configuration needs to be passed to the Helm chart at deployment time.</p> <p><code>azureIdentity.clientId</code> and <code>azureIdentity.resourceId</code>.</p> <p>For Sandpit, these values can be obtained from the created Managed Identity and must be added to Azure App Configuration manually following the configuration guidance.  </p> <p>For other environments, these values are added the the Platform repository.  However, rather than adding the actual values, tokenised values can be added following the format <code>&lt;managed-identity-name&gt;-clientId</code> and <code>&lt;managed-identity-name&gt;-resourceId</code>.</p>"},{"location":"development-patterns/shared-assets/","title":"Shared assets","text":"<p>To better enable developer agility, the FCP provide several shared assets to reduce the effort needed to deliver a microservice ecosystem within.</p> <p>Teams are expected to use these shared assets.</p>"},{"location":"development-patterns/shared-assets/#development-guide","title":"Development guide","text":"<p>The development guide is a library of FFC standards and guides useful to support developers within the FFC programme.</p> <ul> <li>Development guide</li> </ul>"},{"location":"development-patterns/shared-assets/#docker-parent-images","title":"Docker parent images","text":"<p>Docker parent images have been created for Node.js and .NET.  These parent images prevent duplication in each microservice by abstracting common layers away from each microservice.</p>"},{"location":"development-patterns/shared-assets/#dockerhub","title":"DockerHub","text":"<ul> <li>Node.js</li> <li>Node.js development</li> <li>.NET</li> <li>.NET development</li> </ul>"},{"location":"development-patterns/shared-assets/#github","title":"GitHub","text":"<ul> <li>Node.js</li> <li>.NET</li> </ul>"},{"location":"development-patterns/shared-assets/#jenkins-library","title":"Jenkins library","text":"<p>The Jenkins shared library abstracts common CI stages away from each microservice repository and ensures that all adequate assurance steps are completed.  The library supports the addition of custom steps at key points in the pipeline.</p>"},{"location":"development-patterns/shared-assets/#github_1","title":"GitHub","text":"<ul> <li>Jenkins library</li> </ul>"},{"location":"development-patterns/shared-assets/#helm-chart-library","title":"Helm chart library","text":"<p>The Helm chart library keeps microservice Helm charts dry by abstracting common resource definitions away from each microservice repository.  The packaged chart is hosted in a GitHub repository.</p>"},{"location":"development-patterns/shared-assets/#github_2","title":"GitHub","text":"<ul> <li>Helm chart library</li> <li>Helm chart repository</li> </ul>"},{"location":"development-patterns/shared-assets/#microservice-template","title":"Microservice template","text":"<p>A GitHub template repository has been created to allow teams to quickly get started with new microservice.  The template includes all common setup steps such as Docker, Helm charts and Jenkins.</p>"},{"location":"development-patterns/shared-assets/#github_3","title":"GitHub","text":"<ul> <li>Node.js</li> </ul>"},{"location":"development-patterns/shared-assets/#net-sonarcloud-scanning","title":".NET SonarCloud scanning","text":"<p>Analysing containerised .NET microservices with SonarCloud in CI is challenging.  To simplify this process a Docker image has been created to abstract this complexity away from both microservices and the Jenkins library.</p>"},{"location":"development-patterns/shared-assets/#dockerhub_1","title":"DockerHub","text":"<ul> <li>.NET SonarCloud Analysis</li> </ul>"},{"location":"development-patterns/shared-assets/#github_4","title":"GitHub","text":"<ul> <li>.NET SonarCloud Analysis</li> </ul>"},{"location":"development-patterns/shared-assets/#secret-scanning","title":"Secret scanning","text":"<p>All FCP repositories are regularly scanned for committed secrets.  A utility repository has been created in support of that.</p>"},{"location":"development-patterns/shared-assets/#github_5","title":"GitHub","text":"<ul> <li>Git secret scanning</li> <li>Secret scanning</li> </ul>"},{"location":"development-patterns/shared-assets/#npm-publishing","title":"npm publishing","text":"<p>Simplify publishing packages to npm, including switching between pre-release and full releases.</p>"},{"location":"development-patterns/shared-assets/#github_6","title":"GitHub","text":"<ul> <li>npm publish</li> </ul>"},{"location":"development-patterns/shared-assets/#dockerhub_2","title":"DockerHub","text":"<ul> <li>npm publish</li> </ul>"},{"location":"development-patterns/shared-assets/#kafka-admin-client","title":"Kafka admin client","text":"<p>Simplify viewing and deleting Kafka consumer groups</p>"},{"location":"development-patterns/shared-assets/#github_7","title":"GitHub","text":"<ul> <li>Kafka admin client</li> </ul>"},{"location":"development-patterns/shared-assets/#messaging-npm-package","title":"Messaging npm package","text":"<p>npm package to simplify Azure Service Bus sending and consuming in line with FFC standards</p>"},{"location":"development-patterns/shared-assets/#github_8","title":"GitHub","text":"<ul> <li>ffc-messaging</li> </ul>"},{"location":"development-patterns/shared-assets/#npm","title":"npm","text":"<ul> <li>ffc-messaging</li> </ul>"},{"location":"development-patterns/shared-assets/#events-npm-package","title":"Events npm package","text":"<p>npm package to simplify Azure Event Hubs sending and consuming in line with FFC standards</p>"},{"location":"development-patterns/shared-assets/#github_9","title":"GitHub","text":"<ul> <li>ffc-events</li> </ul>"},{"location":"development-patterns/shared-assets/#npm_1","title":"npm","text":"<ul> <li>ffc-events</li> </ul>"},{"location":"getting-started/arrange-access/","title":"Arrange access","text":"<p>Developers will need access to several resources and communication channels in order to be productive.</p> <p>Access to some resources can take time to arrange, so it's important to start this process as soon as possible.</p>"},{"location":"getting-started/arrange-access/#azure-ad-administrative-account","title":"Azure AD administrative account","text":"<p>FCP environments are hosted on Azure.  Access to the Azure Portal is restricted to those with an Azure AD administrative account.</p> <p>These accounts follow the naming convention <code>a-&lt;initials&gt;.&lt;surname&gt;@defra.onmicrosoft.com</code> or <code>&lt;initials&gt;.&lt;surname&gt;@defra.onmicrosoft.com</code>.</p> <p>The must be requested from ServiceNow under the catalogue item <code>O365/AzureAD Platform Admin</code>.</p> <p>The comments should make clear that the request is for a <code>a-</code> Microsoft administrative account for Azure Portal.</p>"},{"location":"getting-started/arrange-access/#access-to-the-azure-portal","title":"Access to the Azure Portal","text":"<p>Once the account has been created, the user will be able to access the Azure Portal.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to the FCP Azure subscriptions.</p> <p>FCP environments are split across three Azure tenants, Development, PreProduction and Production.</p> <p>If a developer has Security Clearance, they can request access to the PreProduction and Production tenants.  Otherwise they will only be able to access the Development tenant.  The request should make clear which environments the developer requires access to.  </p> <p>Evidence of Security Clearance will be requested by CCoE prior to completing the request</p>"},{"location":"getting-started/arrange-access/#access-to-azure-kubernetes-service-aks","title":"Access to Azure Kubernetes Service (AKS)","text":"<p>As AKS is the primary compute hosting platform for FCP, developers will need to be added to the appropriate Kubernetes Cluster role.  This will allow access to Kubernetes using client tools such as <code>kubectl</code> or <code>Lens</code>.</p> <p>As with the Azure Portal, developers can only be added to the PreProduction and Production clusters if they have Security Clearance.</p> <p>A read only role, is all that is permitted for developers in PreProduction and Production.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to the FCP AKS clusters.</p>"},{"location":"getting-started/arrange-access/#jenkins","title":"Jenkins","text":"<p>Jenkins is used for Continuous Integration pipelines in FCP.  Developers will need a <code>@dtz.local</code> account to access Jenkins.</p> <p>Jenkins is hosted on an Azure Virtual Machine and can be accessed on the Defra network or via OpenVPN.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to Jenkins.</p>"},{"location":"getting-started/arrange-access/#synk","title":"Synk","text":"<p>Synk is used for security scanning of code dependencies.  </p> <p>An FCP organisation has been created in Synk and developers will need to be added to it.</p> <p>Contact one of Defra's Principal Developers to arrange access.</p>"},{"location":"getting-started/arrange-access/#github","title":"GitHub","text":"<p>FCP uses GitHub for source control within the Defra organisation.</p> <p>As per GDS standards, code is open source by default.</p> <p>Developers will need to be added to the <code>FFC</code> GitHub team.</p> <p>Two Factor Authentication (2FA) and commit signing are mandatory for all GitHub accounts.</p> <p>Contact one of Defra's Principal Developers to arrange access.</p>"},{"location":"getting-started/arrange-access/#sonarcloud","title":"SonarCloud","text":"<p>SonarCloud is used for static code analysis during Continuous Integration pipelines in FCP.</p> <p>A Defra organisation has been created and developers will need to be added to it using their GitHub account.</p> <p>Contact one of Defra's Principal Developers to arrange access.</p>"},{"location":"getting-started/arrange-access/#azure-devops-ado","title":"Azure DevOps (ADO)","text":"<p>Azure DevOps is used for Continuous Delivery pipelines in FCP.  ADO is also used for work item tracking and Wiki creation.</p> <p>Developers will need to be added to the DEFRA-FFC project.</p> <p>A request should be raised in ServiceNow for CCoE to provide access to ADO.</p>"},{"location":"getting-started/arrange-access/#microsoft-teams","title":"Microsoft Teams","text":"<p>Microsoft Teams is the primary communication tool used across Defra.  </p> <p>Teams supports access from third party teams tenants to allow collaboration across corporate boundaries.</p> <p>Developers should be added to the following Teams chat channels:</p>"},{"location":"getting-started/arrange-access/#defra-fcp-platform-support-teams-chat","title":"<code>DEFRA FCP - Platform Support - Teams Chat</code>","text":"<p>This chat is used for general support and collaboration across all services using the FCP Platform.</p>"},{"location":"getting-started/arrange-access/#fcp-payments-documents-demo-and-progressive-reductions-releases","title":"<code>FCP Payments, Documents, Demo and Progressive Reductions releases</code>","text":"<p>This chat is used to arrange and coordinate releases across the majority of FCP services.  As other services onboard to the FCP Platform, they can also use this chat.</p> <p>Contact one of Defra's Principal Developers to arrange access.</p>"},{"location":"getting-started/arrange-access/#slack","title":"Slack","text":""},{"location":"getting-started/arrange-access/#ffc-notificationsslackcom","title":"<code>ffc-notifications.slack.com</code>","text":"<p>Jenkins will publish alerts to Slack in the <code>ffc-notifications.slack.com</code> workspace.  Developers will need to be added to the following channels:</p>"},{"location":"getting-started/arrange-access/#masterbuildfailures","title":"<code>#masterbuildfailures</code>","text":"<p>Notifications of failures relating to main branches.</p>"},{"location":"getting-started/arrange-access/#generalbuildfailures","title":"<code>#generalbuildfailures</code>","text":"<p>Notifications of failures relating to feature branches.  </p>"},{"location":"getting-started/arrange-access/#secretdetection","title":"<code>#secretdetection</code>","text":"<p>Notifications of detected secrets found in public GitHub repositories for review.</p> <p>Contact one of Defra's Principal Developers to arrange access.</p>"},{"location":"getting-started/arrange-access/#defra-digitalslackcom","title":"<code>defra-digital.slack.com</code>","text":"<p>The wider Defra Digital Slack workspace is used for collaboration and support across all of Defra's digital services.</p> <p>Whilst not essential to be part of this workspace, it may be beneficial for cross Defra collaboration.</p> <p>Developers are recommended to join the following channel:</p>"},{"location":"getting-started/arrange-access/#development","title":"<code>#development</code>","text":"<p>This channel is used for general development discussion and support across all of Defra's digital services.</p> <p>For FCP Platform or service specific support, the above Teams chats should be used instead.</p> <p>Anyone with a <code>defra.gov.uk</code> or an ALB email address can join the Defra Digital Slack workspace automatically.</p> <p>For those with other email addresses, a request should be raised in the <code>#slack-support</code> channel by someone with access.</p>"},{"location":"getting-started/arrange-access/#sharepoint","title":"SharePoint","text":"<p>SharePoint is Defra's primary document library.</p> <p>An FCP SharePoint site has been created.</p> <p>Not all developers will need access to SharePoint and it may not be possible for external suppliers to be provided with access.</p> <p>Should access be required, it should be request to an FCP Programme Delivery Manager.</p>"},{"location":"getting-started/arrange-access/#confluence","title":"Confluence","text":"<p>Whilst ADO Wiki's are preferred, much historic content exists in Defra Confluence.</p> <p>New licenses are often challenging to obtain, but should be requested through the <code>defra-digital.slack.com</code> Slack workspace in the <code>#jira-support</code> channel.</p>"},{"location":"getting-started/cicd/","title":"Continuous Integration and Continuous Delivery (CI/CD)","text":"<p>For the FCP Platform, Jenkins is used for CI pipelines, whilst Azure DevOps is used for CD.</p> <p>Originally the FCP Platform was primarily hosted on AWS where Jenkins was prescribed as the CI/CD tool.  However, as the platform has migrated to Azure, Azure DevOps was prescribed as the CD tool.  Jenkins is still used for CI pipelines as it is a well established tool within the programme and redevelopment of the pipelines in Azure DevOps would be a significant undertaking.</p>"},{"location":"getting-started/cicd/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>FCP adopt a \"shift left\" approach to security and quality, meaning that security and quality checks are performed as early as possible in the development lifecycle.  The CI pipeline is designed around this principle and looks to provide fast feedback to developers and testers prior to merging code to the main branch.</p> <p>Feature branch development allows for small changes to be made and tested in isolation.  This is supported by the CI pipeline which will run a subset of tests and checks on a feature branch build.  The pipeline will also provision a dedicated environment for the feature branch build to be deployed to, allowing for a more comprehensive set of tests to be run.</p> <p>In order to avoid duplication across all microservices, a common Jenkins pipeline has been created.</p> <p>Note that a feature branch must have an open Pull Request in order for the CI pipeline to run.</p>"},{"location":"getting-started/cicd/#supported-technologies","title":"Supported technologies","text":"<p>The CI pipeline supports the following technologies:</p> <ul> <li>Containerised Node.js applications targetting Azure Kubernetes Service (AKS)</li> <li>Containerised .NET applications targetting Azure Kubernetes Service (AKS)</li> <li>Helm charts</li> <li>Azure Functions</li> <li>Docker images</li> <li>npm packages</li> </ul>"},{"location":"getting-started/cicd/#pipeline-stages","title":"Pipeline stages","text":""},{"location":"getting-started/cicd/#build-steps","title":"Build steps","text":"<p>The steps for a feature or main branch build are more or less the same.  The most significant difference is there is no feature branch deployment for a main build.  Instead additional build assets are created ready to be promoted to higher environments.</p> <p>Below shows the pipeline steps for building a Node.js application targetting AKS.  The steps for other technologies are similar.  Anywhere <code>CUSTOM INJECTION</code> is shown custom steps can be injected.</p> <p>Some steps are optional depending on the content of the repository.</p> <ol> <li>Validate semantic version of package</li> <li>Lint Helm chart</li> <li>npm Audit vulnerability check</li> <li>Snyk vulnerability check</li> <li><code>CUSTOM INJECTION</code></li> <li>Build test Docker image</li> <li>Provision dynamic infrastructure for feature branch and to support integration tests</li> <li><code>CUSTOM INJECTION</code></li> <li>Run containerised tests (linting, unit, integration, contract)  </li> <li><code>CUSTOM INJECTION</code></li> <li>Publish contracts to Pact Broker</li> <li>SonarCloud static code analysis</li> <li>Build production Docker image</li> <li>Publish image to to Azure Container Registry</li> <li>Deploy image to dynamic AKS namespace (feature branch build only)</li> <li>AXE accessibility tests (optional)</li> <li>BrowserStack containerised UI and compatibility tests (optional)</li> <li>OWASP Zap security tests (optional)</li> <li>JMeter performance tests (optional)</li> <li>Publish Helm chart to AZure Container Registry (main branch build only)</li> <li><code>CUSTOM INJECTION</code></li> <li>Create GitHub release (main branch build only)</li> <li>Clean up build specific dynamic infrastructure</li> <li><code>CUSTOM INJECTION</code></li> </ol>"},{"location":"getting-started/cicd/#clean-up-steps","title":"Clean up steps","text":"<p>Once a feature branch is deleted, the clean up process will automatically run to remove any dynamic infrastructure.</p>"},{"location":"getting-started/cicd/#continuous-delivery-cd","title":"Continuous Delivery (CD)","text":"<p>Once a feature branch has been merged into the main branch, the CI pipeline will automatically run to package the assets and deploy the application to the Sandpit environment.  Assuming the Sandpit deployment was successful, Jenkins will then trigger Azure DevOps to deploy the application to the higher environments.</p> <p>ADO will deploy the application to the following environments once the previous environment has been successfully deployed to:</p> <ul> <li>Development (automatic)</li> <li>Test (requires approval from anyone in the team)</li> <li>PreProduction (requires approval from anyone in the team)</li> <li>Production (requires approval from CCoE and an Request for Change (RFC))</li> </ul> <p>Note that some of the more mature teams in FCP have approval to run their own changes to Production using a standard change.</p>"},{"location":"getting-started/platform/","title":"Platform","text":""},{"location":"getting-started/platform/#fcp-platform","title":"FCP Platform","text":"<p>To support rapid, highly assured delivery, the programme has delivered common Azure environments, PaaS components, delivery pipelines and supporting tools.</p> <p>These are collectively referred to as the <code>FCP Platform</code>.</p> <p>All FCP developed services are currently deployed to the FCP Platform.  </p> <p>Guidance for use of the FCP Platform is hosted within this repository.</p>"},{"location":"getting-started/platform/#environments","title":"Environments","text":""},{"location":"getting-started/platform/#sandpit","title":"Sandpit","text":"<p>The <code>Sandpit</code> environment is the first deployment environment for new services following merge to the <code>main</code> branch.  It also hosts dynamic feature branch deployments.</p> <p>It can also used for testing and experimentation.  Developers have a high level of access to the Sandpit environment to create and destroy resources as required.</p> <p>Unlike all subsequent environments, provisioning of Azure resources such as Managed Identities, PostgreSQL is not automated.  Teams must create their own resources in this environment using the guides in this repository.</p> <p>Although the majority of patterns between Sandpit are the same, a key difference is the use of Azure App Configuration for configuration management.</p> <p>The reason for this is that originally all environments used Azure App Configuration, however automation of configuration from an Azure repo has replaced this in all other environments.</p>"},{"location":"getting-started/platform/#development","title":"Development","text":"<p>The <code>Development</code> environment is the first automation only deployment environment.  Different teams utilise it for many purposes such as demonstrations and testing.</p> <p>Developers have full access to this environment, but cannot directly create or destroy resources.  Instead, they must use the automation provided to deploy their services.</p>"},{"location":"getting-started/platform/#test","title":"Test","text":"<p>The <code>Test</code> environment is used for more formal testing and assurance activities such as integration testing and user acceptance testing.</p> <p>Note: FCP principles are to apply a \"shift left\" approach to testing where as much testing as possible is done during development prior to merging to <code>main</code>, as well as a principle of deploying frequent small changes to <code>Production</code>. Therefore, it is not expected that changes build up in <code>Test</code> ahead of a large scale deployment to higher environments.  For incomplete features or those awaiting wider scale testing, feature toggles are used to disable new behaviour and not block deployment through to <code>Production</code>.</p> <p>Developers have full access to this environment, but cannot directly create or destroy resources.  Instead, they must use the automation provided to deploy their services.</p>"},{"location":"getting-started/platform/#pre-production","title":"Pre-Production","text":"<p>The <code>Pre-Production</code> environment is used as a staging area prior to deployment to <code>Production</code>.  Whilst some testing activities can be performed here, it is recommended to use <code>Test</code> for most testing activities as access to this environment is more restricted and requires Security Clearance.</p> <p>Developers with Security Clearance can request read only access to this environment.</p>"},{"location":"getting-started/platform/#production","title":"Production","text":"<p>The <code>Production</code> environment is the live environment for the service.  It is the only environment that is accessible to the public.</p> <p>Developers with Security Clearance can request read only access to this environment.</p>"},{"location":"getting-started/platform/#azure-development-platform-adp","title":"Azure Development Platform (ADP)","text":"<p>Following the success of the FCP Platform, Defra has created a Defra wide iteration known as the Azure Development Platform or ADP.</p> <p>The ADP is built on the same principles as the FCP Platform, but takes into account lessons learned throughout the lifetime of the FCP Platform to provide a better experience for teams utilising it.</p> <p>It is the long term intention of the programme to migrate all FCP Platform hosted services to the ADP once it becomes mature enough to provide feature parity.</p> <p>Guidance for use of ADP is hosted within the ADP Documentation.</p>"},{"location":"getting-started/platform/#choosing-a-platform","title":"Choosing a Platform","text":"<p>When starting a new project, the choice of platform should be made in consultation with the FCP Architecture team.  However, it is expected that all new projects will be hosted on the ADP unless there is a compelling reason not to.</p> <p>Early engagement with the ADP team is encouraged to ensure that the platform is suitable for the project's needs.</p>"},{"location":"local-development-setup/","title":"Setup local development environment","text":"<p>Developers are free to use any Defra approved device as long as they adhere to Defra's guidance on use of unmanaged devices. </p> <p>The most common environments are Windows with Windows Subsystem for Linux (WSL) Ubuntu distro and macOS.</p> <p>Visual Studio Code is the preferred code editor.  However, developers are free to use any compatible equivalent if it aides in their productivity.</p> <p>This guide will be targeted towards the above setups. For Windows, all mention of terminal commands should be run in WSL unless specified otherwise.</p>"},{"location":"local-development-setup/#microsoft-intune","title":"Microsoft InTune","text":"<p>Microsoft InTune allows an off network device access to Defra O365 resources such as Outlook, Teams and SharePoint.</p> <p>New InTune enrolments are currently banned which means if a user does not already have an InTune profile, they will not be able to access these resources from their device. There is no alternative at present.</p> <p>InTune requires that the user profile on a device is an organisation account.  So either this must be setup when first configuring the device or a new user profile will need to be created on the device.  It is not possible to convert an existing user profile to an InTune profile.</p>"},{"location":"local-development-setup/#setup-steps","title":"Setup steps","text":""},{"location":"local-development-setup/#windows","title":"Windows","text":"<ol> <li>Install Windows Subsystem for Linux (WSL)</li> <li>Mount Windows drives in WSL</li> <li>Add user to sudoers file (optional)</li> </ol>"},{"location":"local-development-setup/#macos","title":"macOS","text":"<ol> <li>Setup command line tools</li> </ol>"},{"location":"local-development-setup/#common","title":"Common","text":"<ol> <li>Install Docker Desktop</li> <li>Install Visual Studio Code</li> <li>Install StandardJs</li> <li>Install SonarLint</li> <li>Setup commit signing</li> <li>Install Docker Compose</li> <li>Install Detect Secrets</li> <li>Install Node Version Manager (NVM)</li> <li>Install .NET SDK</li> <li>Install kubectl</li> <li>Install Helm</li> <li>Install Azure CLI</li> <li>Install Snyk CLI</li> <li>Install GitHub CLI</li> <li>Install OpenVPN</li> </ol>"},{"location":"local-development-setup/install-azure-cli/","title":"Install Azure CLI","text":"<p>Azure CLI enables interaction with Azure services from the command line.</p>"},{"location":"local-development-setup/install-azure-cli/#installation","title":"Installation","text":"<p>Follow Microsoft's setup guide</p>"},{"location":"local-development-setup/install-azure-cli/#login-to-azure-tenant","title":"Login to Azure tenant","text":"<pre><code>az login --tenant TENANT.onmicrosoft.com\n</code></pre> <p>Follow the instructions displayed and sign in using your <code>a-</code> Microsoft administration account.</p>"},{"location":"local-development-setup/install-detect-secrets/","title":"Install Detect Secrets","text":"<p>Prevent committing passwords and other sensitive information to git repositories.</p> <p>The detect-secrets tool provides out-of-the-box support for scanning git commits for different types of credentials including keywords (e.g. 'password' or 'secret'), private SSH keys, and base64 high entropy string.</p>"},{"location":"local-development-setup/install-detect-secrets/#installation","title":"Installation","text":""},{"location":"local-development-setup/install-detect-secrets/#install-prerequisites","title":"Install prerequisites","text":"<p><code>detect-secrets</code> is written in Python and will require Python version 3 and <code>pip</code> (the package installer for Python) to be installed on your system.</p>"},{"location":"local-development-setup/install-detect-secrets/#python","title":"Python","text":"<p>Install <code>python</code> version 3, download the latest version for your operating system - Windows - Mac OS</p> <p>Once downloaded, run the .exe (windows) or .pkg (macOS) file. Follow the on screen prompts, after successful installation run the following commands to confirm Python was successfully installed.   - Windows - Open Powershell/Command Prompt and type <code>python --verison</code>, it should report the version   - Mac Os - Open a Terminal and type <code>python \u2013version</code>, it should report the version</p>"},{"location":"local-development-setup/install-detect-secrets/#wsl","title":"WSL","text":"<p>If using WSL2 with Docker Desktop then python should already be installed.  However, if needed it can be added with the following guide:   - WSL</p> <p>Once successfully installed, run the command <code>python3 \u2013-version</code>, it should report the version</p>"},{"location":"local-development-setup/install-detect-secrets/#pip","title":"pip","text":"<p>Install <code>pip</code> using the <code>get-pip.py</code> script following these instructions</p> <p>NOTE: pip is already installed if you are using Python 2 &gt;=2.7.9 or Python 3 &gt;=3.4 downloaded from python.org or if you are working in a Virtual Environment created by virtualenv or pyvenv. Just make sure to upgrade pip.</p>"},{"location":"local-development-setup/install-detect-secrets/#install-detect-secrets-and-pre-commit","title":"Install detect-secrets and pre-commit","text":"<p><code>detect-secrets</code> harnesses the <code>pre-commit</code> tool to set-up the git pre-commit hook that runs <code>detect-secrets</code> on the contents of the commit.</p> <ol> <li>Install <code>pre-commit</code> by following the instructions for your system</li> <li>Install <code>detect-secrets</code> by running:</li> </ol> <pre><code>pip install detect-secrets\n</code></pre>"},{"location":"local-development-setup/install-detect-secrets/#configuration","title":"Configuration","text":"<p>A <code>pre-commit</code> configuration file should exist in every FFC git repository that contains the necessary information to run <code>detect-secrets</code>. See the guide for creating FFC git repositories.</p> <p>Your system will need to be configured to set up the git hooks for both currently cloned, and future cloned, FFC repositories.</p>"},{"location":"local-development-setup/install-detect-secrets/#currently-cloned-ffc-git-repositories","title":"Currently cloned FFC git repositories","text":"<p>Set up the <code>pre-commit</code> git hook to run <code>detect-secrets</code> by running the following command in every FFC repository you have cloned on your system:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"local-development-setup/install-detect-secrets/#all-future-cloned-ffc-git-repositories","title":"All future cloned FFC git repositories","text":"<p>To automatically set up the <code>pre-commit</code> git hook to run <code>detect-secrets</code> for newly cloned repositories, set up a global template:</p> <pre><code>git config --global init.templateDir ~/.git-template\npre-commit init-templatedir ~/.git-template\n</code></pre>"},{"location":"local-development-setup/install-detect-secrets/#using-with-other-git-hooks-managers","title":"Using with other git hooks managers","text":"<p><code>pre-commit</code> installs a single git hook to <code>.git/hooks/pre-commit</code></p> <p>By default git only allows a single script to be run for each hook.</p> <p>If a repository is using a git hooks manager such as husky, additional configuration will be required in order to run git hooks created by husky and git hooks created by git-secrets.</p> <p>The solution for running multiple scripts from a single hook is out of scope of this document. However, husky provides options and this Stack Overflow post discusses another.</p>"},{"location":"local-development-setup/install-detect-secrets/#usage","title":"Usage","text":"<p>Refer to the secrets management guide for details on dealing with <code>detect-secrets</code> false positives and excludes.</p>"},{"location":"local-development-setup/install-docker-compose/","title":"Install Docker Compose","text":"<p>Docker Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Then, with a single command, you create and start all the services from your configuration.</p> <p>When installing Docker Desktop, Compose is automatically added to WSL.  Compose provides all the capability of Docker Compose and is intended to be the long term replacement, avoiding the need for the below installation as most <code>docker-compose</code> commands are included.</p> <p>However, it is to be noted that <code>Compose</code> has some differences detailed in this document</p> <p>It is also to be noted that <code>Compose</code> will not work with any volume mount ending with a trailing <code>/</code>.</p> <p>For example, this will work:</p> <pre><code>- volumes:\n  - ./app:/home/node/app\n</code></pre> <p>This will not:</p> <pre><code>- volumes:\n  - ./app/:/home/node/app/\n</code></pre>"},{"location":"local-development-setup/install-docker-compose/#installation","title":"Installation","text":"<p>Only follow this if your current installation of Docker Desktop does not include Compose.</p> <p>Follow the installation guide provided by Docker for your OS.</p>"},{"location":"local-development-setup/install-docker-desktop/","title":"Install Docker Desktop","text":"<p>Docker Desktop is a tool for MacOS and Windows machines for the building and sharing of containerized applications and microservices. It is the fastest and easiest way to get started with Docker on your local machine.</p>"},{"location":"local-development-setup/install-docker-desktop/#installation","title":"Installation","text":"<p>If not already installed as part of the Windows Subsystem for Linux (WSL) setup, follow the below steps to install Docker Desktop.</p> <ol> <li>Download the Docker Desktop installer for your operating system.</li> <li>Run the installer and follow the prompts to install Docker Desktop.</li> </ol>"},{"location":"local-development-setup/install-docker-desktop/#wsl","title":"WSL","text":"<ol> <li>Ensure that Docker Desktop is set to use the WSL2 backend.  This can be done by right clicking the Docker Desktop icon in the system tray and selecting <code>Settings</code>.  From there, select <code>General</code> and ensure <code>Use the WSL 2 based engine</code> is checked.</li> <li>Ensure that Docker Desktop is available to your WSL distro.  This can be done by right clicking the Docker Desktop icon in the system tray and selecting <code>Settings</code>.  From there, select <code>Resources</code>, <code>WSL Integration</code> and ensure that the distro you are using is checked.</li> </ol>"},{"location":"local-development-setup/install-dotnet-sdk/","title":"Install .NET SDK","text":"<p>The .NET SDK is a set of libraries and tools that allow developers to create .NET applications and libraries.</p>"},{"location":"local-development-setup/install-dotnet-sdk/#choose-version-or-use-lts","title":"Choose version (or use LTS)","text":"<p>Select the version required then either a package manager or binary for your OS from Microsoft</p> <p>Note On macOS, creation of symbolic links is missing from the installer after installation. To create them manually run</p> <pre><code>ln -s /usr/local/share/dotnet/dotnet /usr/local/bin/\n</code></pre>"},{"location":"local-development-setup/install-dotnet-sdk/#verify-installation","title":"Verify installation","text":"<p>Check your installation was successful with</p> <pre><code>dotnet --version\n</code></pre>"},{"location":"local-development-setup/install-dotnet-sdk/#install-net-tools","title":"Install .NET tools","text":"<p>As per Microsoft's setup guide the Entity Framework tools allow the creation and application of code first migrations can be installed with the command</p> <pre><code>dotnet tool install --global dotnet-ef\n</code></pre>"},{"location":"local-development-setup/install-github/","title":"Install GitHub CLI","text":"<p>GitHub allows interaction with GitHub via the command line.  Simplifying creation of remote branches and associated Pull Requests.</p> <p>For example, a new local branch can automatically be pushed to GitHub with a draft Pull Request with:</p> <pre><code>gh pr create -d\n</code></pre>"},{"location":"local-development-setup/install-github/#installation","title":"Installation","text":"<p>Follow the appropriate guide for your OS:</p> <ul> <li>Install guide</li> <li>Linux (including WSL) guide</li> </ul>"},{"location":"local-development-setup/install-helm/","title":"Install Helm","text":"<p>Helm is a package manager for Kubernetes. Helm Charts are the package definitions which help you install and upgrade Kubernetes applications.</p> <p>Helm 3 was released in November 2019.</p> <p>Until then, Helm 2 had been the default version of Helm. There are significant differences between the two versions, one of the most substantial being the removal of Tiller. This has resulted in a much easier installation process along with improving the management experience.</p> <p>All charts created by FCP are done so using Helm 3.</p>"},{"location":"local-development-setup/install-helm/#installation","title":"Installation","text":"<p>Installation instructions vary based on OS. Refer to the official docs for details.</p>"},{"location":"local-development-setup/install-kubectl/","title":"Install kubectl","text":"<p>kubectl is a command line tool for interacting with Kubernetes clusters.</p>"},{"location":"local-development-setup/install-kubectl/#installation","title":"Installation","text":"<p>Follow the Kubernetes documentation</p> <p>Note For WSL use the 'Install on Linux' instructions</p>"},{"location":"local-development-setup/install-kubectl/#setup-shell-autocompletion-optional","title":"Setup shell autocompletion (optional)","text":"<pre><code>sudo sh -c 'kubectl completion bash &gt; /etc/bash_completion.d/kubectl'\n</code></pre> <p>Notes: 1. assumes you are using <code>bash</code> shell 1. you will need to reload your shell for the change to be picked up</p> <p>Reference</p> <ul> <li>https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion</li> </ul>"},{"location":"local-development-setup/install-kubectl/#add-command-alias-for-kubectl-optional","title":"Add command alias for kubectl (optional)","text":"<p>To add the frequently used alias <code>k</code> for <code>kubectl</code> add the following lines to your <code>.bashrc</code> file (the 2nd line adds autocomplete for the alias):</p> <pre><code>alias k=kubectl\ncomplete -o default -F __start_kubectl k\n</code></pre>"},{"location":"local-development-setup/install-kubectl/#add-further-aliases-for-kubectl-optional","title":"Add further aliases for Kubectl (optional)","text":"<p>Download the following file and save to your home directory.</p> <p>https://github.com/ahmetb/kubectl-aliases/blob/0533366d8e3e3b3987cc1b7b07a7e8fcfb69f93c/.kubectl_aliases</p> <p>Update your <code>.bashrc</code> file with the below to enable autocomplete on all aliases in the file.</p> <pre><code># Kubectl\n[ -f ~/.kubectl_aliases ] &amp;&amp; source ~/.kubectl_aliases\nsource &lt;(kubectl completion bash)\n\nfor a in $(sed '/^alias /!d;s/^alias //;s/=.*$//' ~/.kubectl_aliases); do\n  complete -F _complete_alias \"$a\"\ndone\n</code></pre> <p>For quick switching of Kubernetes contexts and namespaces, it may be beneficial to append the following lines to the <code>kubectl_aliases</code> file.</p> <pre><code>alias kns='kubectl config set-context --current --namespace'\nalias kc='kubectl config use-context'\n</code></pre> <p>Full details are available in this blog post</p>"},{"location":"local-development-setup/install-node-version-manager/","title":"Install Node Version Manager","text":"<p>Node Version Manager allows a developer to easily install and switch between multiple Node versions on a single operating system.</p>"},{"location":"local-development-setup/install-node-version-manager/#installation","title":"Installation","text":"<p>Follow the installation guide</p>"},{"location":"local-development-setup/install-openvpn/","title":"Install OpenVPN","text":"<p>Access to some cloud resources is restricted to VPN access only. </p>"},{"location":"local-development-setup/install-openvpn/#install-client","title":"Install client","text":"<p>Install the client following the OpenVPN documentation</p>"},{"location":"local-development-setup/install-openvpn/#configure-client","title":"Configure client","text":"<p>Cloud Centre of Excellence (CCoE) will provide full client configuration instructions when setting up access for a new FCP developer.</p>"},{"location":"local-development-setup/install-snyk/","title":"Install Snyk CLI","text":""},{"location":"local-development-setup/install-snyk/#installation","title":"Installation","text":"<p>Follow the setup guide in the official Snyk documentation.</p>"},{"location":"local-development-setup/install-sonarlint/","title":"Install SonarLint","text":"<p>SonarLint is an IDE extension that identifies code quality issues as you code.</p> <p>It can be configured to run in <code>connected mode</code> which allows rules configured in a SonarCloud instance to be applied to a local workspace, flagging up issues as the developer writes code.</p> <p>This guide will demonstrate how to install the SonarLint extension in VS Code.  For other IDEs, refer to the SonarLint documentation.</p>"},{"location":"local-development-setup/install-sonarlint/#dependencies","title":"Dependencies","text":"<ul> <li>Java Runtime Environment v17+</li> </ul> <p>With Ubuntu, the open source version of the Java Runtime Environment (JRE) can be installed using the following command.</p> <pre><code>sudo apt-get install openjdk-17-jre\n</code></pre>"},{"location":"local-development-setup/install-sonarlint/#sonarlint-installation-vs-code","title":"SonarLint Installation (VS Code)","text":"<ol> <li> <p>install SonarLint extension</p> </li> <li> <p>set location of JRE in VS Code settings.  The below example is the install location of the above command    <code>json    \"sonarlint.ls.javaHome\": \"/usr/lib/jvm/java-11-openjdk-amd64\"</code> Note to WSL users ensure that you update the remote <code>settings.json</code> file if you wish to use SonarLint in your linux environment.</p> </li> </ol> <p>This will give you Sonar code analysis using default quality gates for languages supported by SonarLint.  If you wish to use <code>connected mode</code> to sync your local workspace with a SonarCloud project, follow the below steps.</p> <ol> <li> <p>within the SonarCloud UI, navigate to your account's security settings</p> </li> <li> <p>enter a token name and generate a token, noting the token value securely</p> </li> <li> <p>within VS Code, add a SonarCloud server connection to <code>settings.json</code> to enable <code>connected mode</code> <code>json    \"sonarlint.connectedMode.connections.sonarcloud\": [{     \"organizationKey\": \"defra\",     \"token\": \"MY_TOKEN\"    }]</code></p> </li> <li> <p>within each project workspace, create or edit your workspace <code>settings.json</code> file to include a project link    <code>json    \"sonarlint.connectedMode.project\": {     \"projectKey\": \"ffc-demo-payment-web\"    }</code> Note if you have multiple SonarCloud instances then a <code>connectionId</code> property can be added to the two code snippets above to correctly bind a project to the correct SonarCloud instance.</p> </li> <li> <p>update project bindings by selecting <code>Update all project bindings to SonarCloud</code> from the VS Code <code>Command Palette</code>.  (<code>ctrl + shift + p</code> to open)</p> </li> </ol> <p>Rules, quality gates, exclusions etc set in the SonarCloud project will now be applied to local workspace.</p>"},{"location":"local-development-setup/install-sonarlint/#configure-sonar-for-c","title":"Configure Sonar for C","text":"<p>The SonarLint extension for VS Code does not currently support C#, but we can still get Sonar linting rules set up in VS Code by adding the Sonar analyser to Omnisharp.</p> <ol> <li>Add the Sonar Analyzer NuGet package if not already present in the <code>csproj</code> file:</li> </ol> <pre><code>dotnet add &lt;PROJECT_NAME&gt; package SonarAnalyzer.CSharp\n</code></pre> <ol> <li>Enable analysers in VS Code: Search for Omnisharp in settings and select <code>Enable Roslyn Analysers</code>. Alternative configuration is available outside of VS Code.</li> </ol> <p>When VS Code analyses C# code, it will now check against the standard set of Sonar rules for C#. The Sonar rule IDs are prefixed with <code>S</code> when displayed in VS Code. For more details on the rules you can consult the Sonar rules documentation.</p>"},{"location":"local-development-setup/install-standard-js/","title":"Install StandardJs","text":"<p>In line with Defra's digital standards for JavaScript, StandardJS coding standards should be used. </p>"},{"location":"local-development-setup/install-standard-js/#installation","title":"Installation","text":"<p>Run the following command to install StandardJS globally:</p> <pre><code>npm install -g standard\n</code></pre>"},{"location":"local-development-setup/install-vs-code/","title":"Install IDE","text":"<p>Developers are free to use their own IDE/text editor of choice.  However, this guide will assume Visual Studio Code will be used as that is the most commonly used tool in FCP and Defra.</p> <p>If not using VS Code, this guide should still be read to ensure that the equivalent installation steps are applied.</p> <p>Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and macOS. It includes support for debugging, embedded Git control and GitHub, syntax highlighting, intelligent code completion, snippets, and code refactoring.</p>"},{"location":"local-development-setup/install-vs-code/#installation-vs-code","title":"Installation VS Code","text":"<p>Install for your operating system following Microsoft's guidance</p>"},{"location":"local-development-setup/install-vs-code/#vs-code-integration","title":"VS Code Integration","text":"<p>Visual Studio has an extension to validate syntax for StandardJS, Defra's linting standard.</p> <ol> <li>Install StandardJS extension</li> <li>Disable VS Code JavaScript Validation by adding <code>\"javascript.validate.enable\": false</code> to settings.json</li> <li>Set auto fix on save by adding <code>\"standard.autoFixOnSave\": true</code> to settings.json</li> </ol>"},{"location":"local-development-setup/install-vs-code/#install-c-extension","title":"Install C# Extension","text":"<p>Add C# extension to enable .Net development from the Marketplace</p>"},{"location":"local-development-setup/install-vs-code/#configure-autosave","title":"Configure autosave","text":"<p>If you prefer your code to autosave, add the following to settings.json.</p> <pre><code>\"files.autoSave\": \"afterDelay\"\n\"files.autoSaveDelay\": 600\n</code></pre>"},{"location":"local-development-setup/install-vs-code/#indentation","title":"Indentation","text":"<p>VS Code supports multiple indentation settings for each language by updating the <code>settings.json</code> file.</p> <p>The below example sets a default indentation to two spaces, but uses four spaces for C# files as per Microsoft standards.</p> <pre><code>\"editor.tabSize\": 2,\n\"[csharp]\":{\n    \"editor.tabSize\": 4,\n  },\n</code></pre> <p>Note JS standard will automatically correct JavaScript files to two space indentation.</p>"},{"location":"local-development-setup/install-vs-code/#wsl-configuration","title":"WSL Configuration","text":""},{"location":"local-development-setup/install-vs-code/#install-remote-wsl-extension","title":"Install Remote WSL extension","text":"<p>The Remote - WSL allows VS Code installed in Windows to be used in WSL.</p> <p>This means when code is source controlled in WSL, VS Code can read, debug and interact with it without workarounds.</p> <p>In WSL2 it is recommended to always use the remote WSL approach.  A current directory can be opened in VS Code with <code>code .</code>.</p> <p>With WSL1, as source code will be in Windows, depending on the development activity it may be easier to use VS Code in Windows.</p>"},{"location":"local-development-setup/install-vs-code/#turn-on-signed-commits","title":"Turn on signed commits","text":"<p>To turn on signed commits (see section on signed commits for more information) in the UI make sure the <code>Enable commit signing with GPG or X.509</code> is selected. </p> <p>For the JSON version of the settings use <code>\"git.enableCommitSigning\": true</code></p>"},{"location":"local-development-setup/install-wsl/","title":"Install Windows Subsystem for Linux (WSL)","text":"<p>The Windows Subsystem for Linux (WSL) allows developers run a GNU/Linux environment -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a virtual machine.</p> <p>Official Documentation</p>"},{"location":"local-development-setup/install-wsl/#wsl1-or-wsl2","title":"WSL1 or WSL2","text":"<p>WSL2 is built using a full Linux kernal and is optimised for size and performance.  It also solves many of the networking and Docker integration challenges that were present in WSL1.</p> <p>WSL1 requires significantly more manual setup.  The remainder of this guide will assume WSL2 is being used.</p> <p>For further information on the differences between WSL1 and WSL2, see the Microsoft documentation.</p>"},{"location":"local-development-setup/install-wsl/#installation","title":"Installation","text":"<p>Follow the below Microsoft guide to install the distro of your choice. The Ubuntu distro is recommended.</p> <p>Installation Guide</p> <p>Within the guide there is also a recommended setup of a development environment using WSL.</p> <p>This guide should also be followed as it includes setup of Docker Desktop, Git, Windows Terminal and VS Code.</p>"},{"location":"local-development-setup/install-wsl/#quick-links","title":"Quick links","text":"<ul> <li>Setup Docker Desktop</li> <li>Setup Git</li> <li>Setup Visual Studio Code</li> <li>Setup Windows Terminal</li> </ul>"},{"location":"local-development-setup/mount-windows-drives-in-wsl/","title":"Mount Windows drives in WSL","text":"<p>WSL will automatically mount all Windows drives under the <code>/mnt/</code> directory followed by the drive letter.</p> <p>For example, the <code>C:</code> drive will be accessible on <code>/mnt/c/</code>.</p> <p>However, in order to allow WSL to change Windows file permissions, the drives need to be mounted with the <code>metadata</code> option.</p>"},{"location":"local-development-setup/mount-windows-drives-in-wsl/#setup","title":"Setup","text":"<p>1 Create or edit WSL config file by entering the below terminal command.</p> <pre><code>sudo nano /etc/wsl.conf\n</code></pre> <p>2 Add the following content to the file, then save and exit.</p> <pre><code>[automount]\noptions = \"metadata\"\n</code></pre> <p>3 Reboot machine for changes to take effect.</p>"},{"location":"local-development-setup/setup-macos-command-line-tools/","title":"Setup command line tools","text":"<p>Mac users who prefer to have a more traditional Unix toolkit accessible to them through the Terminal may wish to install the optional Command Line Tools subsection of the Xcode IDE. </p> <p>This is possible installing the entire Xcode package, no developer account is required either.</p>"},{"location":"local-development-setup/setup-macos-command-line-tools/#install-command-line-tools","title":"Install command line tools","text":"<pre><code>xcode-select --install\n</code></pre>"},{"location":"local-development-setup/setup-sudoers/","title":"Add user to sudoers file","text":"<p><code>sudo</code> is a command line programme that allows trusted users to execute commands as root user.</p> <p>Running a <code>sudo</code> command will prompt for a password.  For local development in WSL, developers may prefer to add their WSL user to the <code>sudoers</code> file to avoid the need for a password.</p>"},{"location":"local-development-setup/setup-sudoers/#update-sudoers-file","title":"Update <code>sudoers</code> file","text":"<ol> <li>Open sudoers file with root permission to edit with <code>sudo nano /etc/sudoers</code> </li> <li>Append <code>username  ALL=(ALL) NOPASSWD:ALL</code> to bottom of file replacing <code>username</code> with your WSL username</li> <li>Save and exit the file</li> </ol>"},{"location":"local-development-setup/sign-commits/","title":"Setup commit signing","text":"<p>You can sign commits and tags locally, so other people can verify that your work comes from a trusted source. If a commit or tag has a GPG or S/MIME signature that is cryptographically verifiable, GitHub marks the commit or tag as verified.</p> <p>If a commit or tag has a signature that cannot be verified, GitHub marks the commit or tag as unverified.</p> <p>You can check the verification status of your signed commits or tags on GitHub and view why your commit signatures might be unverified. </p>"},{"location":"local-development-setup/sign-commits/#setup","title":"Setup","text":"<p>Follow GitHub's instructions for setting up and configuring a GPG key.</p> <ol> <li>Creating a new GPG key</li> <li>Adding a new GPG key to your GitHub account</li> <li>Setup Git to use new GPG key</li> </ol>"},{"location":"local-development-setup/sign-commits/#signing-commits-via-vs-code","title":"Signing commits via VS code","text":"<p>To turn on signed commits in the VS Code UI, make sure the <code>Enable commit signing with GPG or X.509</code> is ticked. For the JSON version of the settings use <code>\"git.enableCommitSigning\": true</code></p>"},{"location":"testing/contract/","title":"Contract testing","text":"<p>Contract testing is a form of integration testing where a contract is established between a consumer and a provider. The contract is a collection of requests and responses that the consumer expects to send and receive from the provider. The contract is used to verify that the provider can meet the consumer's expectations.</p> <p>Contracts can be for synchronous or asynchronous communication, and can be used to verify the behaviour of both HTTP and message-based systems.</p>"},{"location":"testing/contract/#pact-broker","title":"Pact Broker","text":"<p>The Pact Broker is an application for sharing of consumer driven contracts and verification results. It is optimised for use with \"pacts\" (contracts created by the Pact framework), but can be used for any type of contract that can be serialized to JSON. </p>"},{"location":"testing/contract/#webhooks","title":"Webhooks","text":"<p>Documentation: https://docs.pact.io/pact_broker/webhooks</p> <p>Through the Pact-Broker CLI:</p> <p>Open a command window and navigate to the bin folder of the pact-cli and run the following command replacing the variables for your deployment. For more information: https://docs.pact.io/pact_broker/client_cli/readme</p> <pre><code>pact-broker create-webhook \"https://listeners.jenkins-sb.savagebeast.com/job/listeners-acceptance/job/graphql/job/DEVTOOLS-610-test-pact-broker-webhooks/buildWithParameters?os_authType=basic&amp;environment=shared&amp;graphqlHost=shared.graphql.docker.savagebeast.com\" --request=POST --broker-base-url=http://localhost:9292 --description='Test 3 Webhook CLI' --contract-content-changed --consumer=\"Example App\" --provider=\"Example API\" --user=username:password --header=\"accept:application/json\"\n</code></pre> <p>Through the Pact-Broker UI:</p> <ol> <li>Under the column called \"Webhook status\" click on \"Create\"</li> <li>Under the heading \"Links\" Locate \"pb:create\" in the column called \"NON-GET\" Click on the \"!\" symbol. </li> <li>A new window will open to allow the use of HTTP verbs POST, DELETE. </li> </ol> <p>To create the webhook use POST with following body:</p>"},{"location":"testing/contract/#jenkins-example","title":"Jenkins Example","text":"<pre><code>{\n  \"description\": \"Trigger ffc-demo-claim-service Acceptance Test Job on contract_content_changed event from ffc-demo-payment-service\",\n  \"events\": [{\n    \"name\": \"contract_content_changed\"\n  }],\n  \"request\": {\n    \"method\": \"POST\",\n    \"url\": \"\",\n    \"headers\": {\n      \"authorization\": \"Basic \"\n    }\n  }\n}\n\n</code></pre>"},{"location":"testing/contract/#slack-example","title":"Slack Example","text":"<pre><code>{\n  \"description\": \"Trigger to send Slack message on provider-verification-failed event from ffc-demo-payment-service\",\n  \"events\": [\n    {\n      \"name\": \"provider_verification_failed\"\n    }\n  ],\n  \"request\": {\n    \"method\": \"POST\",\n    \"url\": \"\",\n    \"headers\": {\n      \"Content-Type\": \"application/json\"\n    },\n    \"body\": {\n      \"channel\": \"#generalbuildfailures\",\n      \"username\": \"webhookbot\",\n      \"text\": \"Pact verification failed for ${pactbroker.consumerName}/${pactbroker.providerName}: ${pactbroker.pactUrl}\",\n      \"icon_emoji\": \":(\"\n    }\n  }\n}\n</code></pre> <p>To delete a Webhook use the DELETE Verb with an empty body.</p> <p>To test the webhook:</p> <ol> <li>Select the webhook using \"GET\"</li> <li>Under the heading \"Links\" Locate \"pb:execute\"</li> </ol>"},{"location":"testing/contract/#note","title":"Note","text":"<p>As of writing this guide, there is an issue with the default url used within the UI for executing the API calls. </p> <p>The current default url uses the internal IP address. This causes the following error: </p> <p><code>violates the following Content Security Policy directive: \"default-src 'self'\". Note that 'connect-src' was not explicitly set, so 'default-src' is used as a fallback.</code></p> <p>to resolve the issue, the IP address needs replacing with the DNS domain name.</p>"}]}